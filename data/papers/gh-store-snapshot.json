{
  "snapshot_time": "2025-02-22T18:18:14.513658+00:00",
  "repository": "dmarx/papers-feed",
  "objects": {
    "interactions:2310.16410": {
      "data": {
        "paper_id": "2310.16410",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-24T08:29:08.482Z",
            "data": {
              "session_id": "session_1737707335861_ub76jpd",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-24T08:28:55.861Z",
              "end_time": "2025-01-24T08:29:02.492Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:29:03+00:00",
        "updated_at": "2025-01-24T08:29:27+00:00",
        "version": 5
      }
    },
    "paper:2310.16410": {
      "data": {
        "arxivId": "2310.16410",
        "url": "https://arxiv.org/abs/2310.16410",
        "title": "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in\n  AlphaZero",
        "authors": "Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, Been Kim",
        "abstract": "Artificial Intelligence (AI) systems have made remarkable progress, attaining\nsuper-human performance across various domains. This presents us with an\nopportunity to further human knowledge and improve human expert performance by\nleveraging the hidden knowledge encoded within these highly performant AI\nsystems. Yet, this knowledge is often hard to extract, and may be hard to\nunderstand or learn from. Here, we show that this is possible by proposing a\nnew method that allows us to extract new chess concepts in AlphaZero, an AI\nsystem that mastered the game of chess via self-play without human supervision.\nOur analysis indicates that AlphaZero may encode knowledge that extends beyond\nthe existing human knowledge, but knowledge that is ultimately not beyond human\ngrasp, and can be successfully learned from. In a human study, we show that\nthese concepts are learnable by top human experts, as four top chess\ngrandmasters show improvements in solving the presented concept prototype\npositions. This marks an important first milestone in advancing the frontier of\nhuman knowledge by leveraging AI; a development that could bear profound\nimplications and help us shape how we interact with AI systems across many AI\napplications.",
        "timestamp": "2025-01-24T08:28:29.420Z",
        "rating": "novote",
        "published_date": "2023-10-25T06:49:26Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.HC",
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:28:29+00:00",
        "updated_at": "2025-01-24T08:28:32+00:00",
        "version": 2
      }
    },
    "interactions:2501.04697": {
      "data": {
        "paper_id": "2501.04697",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-24T08:25:00+00:00",
        "updated_at": "2025-01-24T08:25:03+00:00",
        "version": 2
      }
    },
    "paper:2501.04697": {
      "data": {
        "arxivId": "2501.04697",
        "url": "https://arxiv.org/abs/2501.04697",
        "title": "Grokking at the Edge of Numerical Stability",
        "authors": "Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal",
        "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
        "timestamp": "2025-01-24T08:24:55.685Z",
        "rating": "novote",
        "published_date": "2025-01-08T18:58:48Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T08:24:56+00:00",
        "updated_at": "2025-01-24T08:24:59+00:00",
        "version": 2
      }
    },
    "interactions:2002.09291": {
      "data": {
        "paper_id": "2002.09291",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-24T07:09:06.090Z",
            "data": {
              "session_id": "session_1737702535793_x55fw99",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-24T07:08:55.793Z",
              "end_time": "2025-01-24T07:09:05.184Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:29:19.260Z",
            "data": {
              "session_id": "session_1737782950954_orhggu3",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:29:10.954Z",
              "end_time": "2025-01-25T05:29:17.671Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:09:06+00:00",
        "updated_at": "2025-01-25T05:29:30+00:00",
        "version": 7
      }
    },
    "paper:2002.09291": {
      "data": {
        "arxivId": "2002.09291",
        "url": "https://arxiv.org/abs/2002.09291",
        "title": "Transformer Hawkes Process",
        "authors": "Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha",
        "abstract": "Modern data acquisition routinely produce massive amounts of event sequence\ndata in various domains, such as social media, healthcare, and financial\nmarkets. These data often exhibit complicated short-term and long-term temporal\ndependencies. However, most of the existing recurrent neural network based\npoint process models fail to capture such dependencies, and yield unreliable\nprediction performance. To address this issue, we propose a Transformer Hawkes\nProcess (THP) model, which leverages the self-attention mechanism to capture\nlong-term dependencies and meanwhile enjoys computational efficiency. Numerical\nexperiments on various datasets show that THP outperforms existing models in\nterms of both likelihood and event prediction accuracy by a notable margin.\nMoreover, THP is quite general and can incorporate additional structural\nknowledge. We provide a concrete example, where THP achieves improved\nprediction performance for learning multiple point processes when incorporating\ntheir relational information.",
        "timestamp": "2025-01-24T07:08:55.938Z",
        "rating": "novote",
        "published_date": "2020-02-21T13:48:13Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:08:56+00:00",
        "updated_at": "2025-01-24T07:08:59+00:00",
        "version": 2
      }
    },
    "paper:2002.08521": {
      "data": {
        "arxivId": "2002.08521",
        "url": "https://arxiv.org/pdf/2002.08521",
        "title": "Group Network Hawkes Process",
        "authors": "Guanhua Fang, Ganggang Xu, Haochen Xu, Xuening Zhu, Yongtao Guan",
        "abstract": "In this work, we study the event occurrences of individuals interacting in a\nnetwork. To characterize the dynamic interactions among the individuals, we\npropose a group network Hawkes process (GNHP) model whose network structure is\nobserved and fixed. In particular, we introduce a latent group structure among\nindividuals to account for the heterogeneous user-specific characteristics. A\nmaximum likelihood approach is proposed to simultaneously cluster individuals\nin the network and estimate model parameters. A fast EM algorithm is\nsubsequently developed by utilizing the branching representation of the\nproposed GNHP model. Theoretical properties of the resulting estimators of\ngroup memberships and model parameters are investigated under both settings\nwhen the number of latent groups $G$ is over-specified or correctly specified.\nA data-driven criterion that can consistently identify the true $G$ under mild\nconditions is derived. Extensive simulation studies and an application to a\ndata set collected from Sina Weibo are used to illustrate the effectiveness of\nthe proposed methodology.",
        "timestamp": "2025-01-24T07:07:58.196Z",
        "rating": "novote",
        "published_date": "2020-02-20T01:30:42Z",
        "arxiv_tags": [
          "stat.ME",
          "math.ST",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-01-24T07:07:58+00:00",
        "updated_at": "2025-01-24T07:08:01+00:00",
        "version": 2
      }
    },
    "interactions:2312.00752": {
      "data": {
        "paper_id": "2312.00752",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-23T07:26:06.418Z",
            "data": {
              "session_id": "session_1737617143583_9cgks93",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-23T07:25:43.583Z",
              "end_time": "2025-01-23T07:26:06.402Z",
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-23T07:25:37+00:00",
        "updated_at": "2025-01-23T21:30:38+00:00",
        "version": 4
      }
    },
    "paper:2312.00752": {
      "data": {
        "arxivId": "2312.00752",
        "url": "https://arxiv.org/abs/2312.00752",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "authors": "Albert Gu, Tri Dao",
        "abstract": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
        "timestamp": "2025-01-23T07:24:53.349Z",
        "rating": "novote",
        "published_date": "2023-12-01T18:01:34Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-23T07:24:53+00:00",
        "updated_at": "2025-01-23T07:24:56+00:00",
        "version": 2
      }
    },
    "interactions:2402.18012": {
      "data": {
        "paper_id": "2402.18012",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-23T02:00:14.220Z",
            "data": {
              "session_id": "session_1737597587001_4wifmlu",
              "duration_seconds": 26,
              "idle_seconds": 0,
              "start_time": "2025-01-23T01:59:47.001Z",
              "end_time": "2025-01-23T02:00:13.427Z",
              "total_elapsed_seconds": 26
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-23T02:00:15+00:00",
        "updated_at": "2025-01-24T08:14:42+00:00",
        "version": 3
      }
    },
    "paper:2402.18012": {
      "data": {
        "arxivId": "2402.18012",
        "url": "https://arxiv.org/abs/2402.18012",
        "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown\n  Constraints",
        "authors": "Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortoli, Dongxia Wu, Haorui Wang, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao Zhang",
        "abstract": "Addressing real-world optimization problems becomes particularly challenging\nwhen analytic objective functions or constraints are unavailable. While\nnumerous studies have addressed the issue of unknown objectives, limited\nresearch has focused on scenarios where feasibility constraints are not given\nexplicitly. Overlooking these constraints can lead to spurious solutions that\nare unrealistic in practice. To deal with such unknown constraints, we propose\nto perform optimization within the data manifold using diffusion models. To\nconstrain the optimization process to the data manifold, we reformulate the\noriginal optimization problem as a sampling problem from the product of the\nBoltzmann distribution defined by the objective function and the data\ndistribution learned by the diffusion model. Depending on the differentiability\nof the objective function, we propose two different sampling methods. For\ndifferentiable objectives, we propose a two-stage framework that begins with a\nguided diffusion process for warm-up, followed by a Langevin dynamics stage for\nfurther correction. For non-differentiable objectives, we propose an iterative\nimportance sampling strategy using the diffusion model as the proposal\ndistribution. Comprehensive experiments on a synthetic dataset, six real-world\nblack-box optimization datasets, and a multi-objective molecule optimization\ndataset show that our method achieves better or comparable performance with\nprevious state-of-the-art baselines.",
        "timestamp": "2025-01-23T01:59:47.272Z",
        "rating": "novote",
        "published_date": "2024-02-28T03:09:12Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-23T01:59:47+00:00",
        "updated_at": "2025-01-23T01:59:50+00:00",
        "version": 2
      }
    },
    "interactions:1503.02531": {
      "data": {
        "paper_id": "1503.02531",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T23:53:28.499Z",
            "data": {
              "session_id": "session_1737589998373_thnxbvn",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-01-22T23:53:18.373Z",
              "end_time": "2025-01-22T23:53:24.127Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:27:00.043Z",
            "data": {
              "session_id": "session_1737934014658_cxm1yfn",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:26:54.658Z",
              "end_time": "2025-01-26T23:26:59.676Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:56:16.420Z",
            "data": {
              "session_id": "session_1737935768259_ecxh3v6",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:56:08.260Z",
              "end_time": "2025-01-26T23:56:16.179Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:09:39.404Z",
            "data": {
              "session_id": "session_1737936570761_wc050fw",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:09:30.762Z",
              "end_time": "2025-01-27T00:09:37.896Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:53:28+00:00",
        "updated_at": "2025-01-27T00:10:59+00:00",
        "version": 7
      }
    },
    "paper:1503.02531": {
      "data": {
        "arxivId": "1503.02531",
        "url": "https://arxiv.org/abs/1503.02531",
        "title": "Distilling the Knowledge in a Neural Network",
        "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
        "abstract": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.",
        "timestamp": "2025-01-22T23:52:59.719Z",
        "rating": "novote",
        "published_date": "2015-03-09T15:44:49Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:53:00+00:00",
        "updated_at": "2025-01-22T23:53:02+00:00",
        "version": 2
      }
    },
    "interactions:2212.07677": {
      "data": {
        "paper_id": "2212.07677",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T23:51:48.964Z",
            "data": {
              "session_id": "session_1737589861712_6k0limt",
              "duration_seconds": 46,
              "idle_seconds": 0,
              "start_time": "2025-01-22T23:51:01.712Z",
              "end_time": "2025-01-22T23:51:48.063Z",
              "total_elapsed_seconds": 46
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:27:09.072Z",
            "data": {
              "session_id": "session_1737934024181_5e34nc6",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:27:04.181Z",
              "end_time": "2025-01-26T23:27:09.058Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T23:56:23.270Z",
            "data": {
              "session_id": "session_1737935779556_s7zdd89",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-26T23:56:19.557Z",
              "end_time": "2025-01-26T23:56:22.895Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:09:46.369Z",
            "data": {
              "session_id": "session_1737936582965_48u5lh2",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:09:42.965Z",
              "end_time": "2025-01-27T00:09:46.170Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:51:49+00:00",
        "updated_at": "2025-01-27T00:11:07+00:00",
        "version": 7
      }
    },
    "paper:2212.07677": {
      "data": {
        "arxivId": "2212.07677",
        "url": "https://arxiv.org/abs/2212.07677",
        "title": "Transformers learn in-context by gradient descent",
        "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov",
        "abstract": "At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
        "timestamp": "2025-01-22T23:51:01.971Z",
        "rating": "novote",
        "published_date": "2022-12-15T09:21:21Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:51:02+00:00",
        "updated_at": "2025-01-22T23:51:05+00:00",
        "version": 2
      }
    },
    "interactions:2501.12374": {
      "data": {
        "paper_id": "2501.12374",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:09:55.653Z",
            "data": {
              "session_id": "session_1737936591983_wpts0s9",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:09:51.983Z",
              "end_time": "2025-01-27T00:09:55.380Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:29:34+00:00",
        "updated_at": "2025-01-27T00:18:37+00:00",
        "version": 12
      }
    },
    "paper:2501.12374": {
      "data": {
        "arxivId": "2501.12374",
        "url": "https://arxiv.org/abs/2501.12374",
        "title": "Expertise elevates AI usage: experimental evidence comparing laypeople\n  and professional artists",
        "authors": "Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan",
        "abstract": "Novel capacities of generative AI to analyze and generate cultural artifacts\nraise inevitable questions about the nature and value of artistic education and\nhuman expertise. Has AI already leveled the playing field between professional\nartists and laypeople, or do trained artistic expressive capacity, curation\nskills and experience instead enhance the ability to use these new tools? In\nthis pre-registered study, we conduct experimental comparisons between 50\nactive artists and a demographically matched sample of laypeople. We designed\ntwo tasks to approximate artistic practice for testing their capabilities in\nboth faithful and creative image creation: replicating a reference image, and\nmoving as far away as possible from it. We developed a bespoke platform where\nparticipants used a modern text-to-image model to complete both tasks. We also\ncollected and compared participants' sentiments towards AI. On average, artists\nproduced more faithful and creative outputs than their lay counterparts,\nalthough only by a small margin. While AI may ease content creation,\nprofessional expertise is still valuable - even within the confined space of\ngenerative AI itself. Finally, we also explored how well an exemplary\nvision-capable large language model (GPT-4o) would complete the same tasks, if\ngiven the role of an image generation agent, and found it performed on par in\ncopying but outperformed even artists in the creative task. The very best\nresults were still produced by humans in both tasks. These outcomes highlight\nthe importance of integrating artistic skills with AI training to prepare\nartists and other visual professionals for a technologically evolving\nlandscape. We see a potential in collaborative synergy with generative AI,\nwhich could reshape creative industries and education in the arts.",
        "timestamp": "2025-01-22T23:29:08.663Z",
        "rating": "novote",
        "published_date": "2025-01-21T18:53:21Z",
        "arxiv_tags": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:29:09+00:00",
        "updated_at": "2025-01-22T23:29:12+00:00",
        "version": 2
      }
    },
    "interactions:2007.12927": {
      "data": {
        "paper_id": "2007.12927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T23:10:40.531Z",
            "data": {
              "duration_seconds": 606
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:10:06.264Z",
            "data": {
              "session_id": "session_1737936601397_fv5nag5",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:10:01.397Z",
              "end_time": "2025-01-27T00:10:04.498Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T00:20:27.592Z",
            "data": {
              "session_id": "session_1737937223431_7egykr9",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-27T00:20:23.431Z",
              "end_time": "2025-01-27T00:20:27.049Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T08:24:58.264Z",
            "data": {
              "session_id": "session_1739348691785_q4emcqh",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-12T08:24:51.786Z",
              "end_time": "2025-02-12T08:24:58.255Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T08:29:50.172Z",
            "data": {
              "session_id": "session_1739348982623_0wn429b",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-12T08:29:42.623Z",
              "end_time": "2025-02-12T08:29:47.615Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-12T20:28:11.055Z",
            "data": {
              "session_id": "session_1739392087155_4y79st1",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-12T20:28:07.155Z",
              "end_time": "2025-02-12T20:28:10.534Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T01:16:08.744Z",
            "data": {
              "session_id": "session_1739409364730_vvbgc10",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-13T01:16:04.730Z",
              "end_time": "2025-02-13T01:16:08.169Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:10:40+00:00",
        "updated_at": "2025-02-13T01:17:32+00:00",
        "version": 17
      }
    },
    "paper:2007.12927": {
      "data": {
        "arxivId": "2007.12927",
        "url": "https://arxiv.org/pdf/2007.12927",
        "title": "Neural networks with late-phase weights",
        "authors": "Johannes von Oswald, Seijin Kobayashi, Alexander Meulemans, Christian Henning, Benjamin F. Grewe, Jo\u00e3o Sacramento",
        "abstract": "The largely successful method of training neural networks is to learn their\nweights using some variant of stochastic gradient descent (SGD). Here, we show\nthat the solutions found by SGD can be further improved by ensembling a subset\nof the weights in late stages of learning. At the end of learning, we obtain\nback a single model by taking a spatial average in weight space. To avoid\nincurring increased computational costs, we investigate a family of\nlow-dimensional late-phase weight models which interact multiplicatively with\nthe remaining parameters. Our results show that augmenting standard models with\nlate-phase weights improves generalization in established benchmarks such as\nCIFAR-10/100, ImageNet and enwik8. These findings are complemented with a\ntheoretical analysis of a noisy quadratic problem which provides a simplified\npicture of the late phases of neural network learning.",
        "timestamp": "2025-01-22T23:00:32.558Z",
        "rating": "novote",
        "published_date": "2020-07-25T13:23:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T23:00:33+00:00",
        "updated_at": "2025-01-22T23:00:36+00:00",
        "version": 2
      }
    },
    "interactions:2212.13345": {
      "data": {
        "paper_id": "2212.13345",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T22:08:59.431Z",
            "data": {
              "duration_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T22:06:24+00:00",
        "updated_at": "2025-01-24T08:15:21+00:00",
        "version": 7
      }
    },
    "paper:2212.13345": {
      "data": {
        "arxivId": "2212.13345",
        "url": "https://arxiv.org/abs/2212.13345",
        "title": "The Forward-Forward Algorithm: Some Preliminary Investigations",
        "authors": "Geoffrey Hinton",
        "abstract": "The aim of this paper is to introduce a new learning procedure for neural\nnetworks and to demonstrate that it works well enough on a few small problems\nto be worth further investigation. The Forward-Forward algorithm replaces the\nforward and backward passes of backpropagation by two forward passes, one with\npositive (i.e. real) data and the other with negative data which could be\ngenerated by the network itself. Each layer has its own objective function\nwhich is simply to have high goodness for positive data and low goodness for\nnegative data. The sum of the squared activities in a layer can be used as the\ngoodness but there are many other possibilities, including minus the sum of the\nsquared activities. If the positive and negative passes could be separated in\ntime, the negative passes could be done offline, which would make the learning\nmuch simpler in the positive pass and allow video to be pipelined through the\nnetwork without ever storing activities or stopping to propagate derivatives.",
        "timestamp": "2025-01-22T22:06:19.707Z",
        "rating": "thumbsup",
        "published_date": "2022-12-27T02:54:46Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T22:06:20+00:00",
        "updated_at": "2025-01-24T08:15:32+00:00",
        "version": 3
      }
    },
    "interactions:2112.04215": {
      "data": {
        "paper_id": "2112.04215",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-22T07:09:51.236Z",
            "data": {
              "duration_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-22T07:08:39+00:00",
        "updated_at": "2025-01-24T08:15:39+00:00",
        "version": 8
      }
    },
    "paper:2112.04215": {
      "data": {
        "arxivId": "2112.04215",
        "url": "https://arxiv.org/abs/2112.04215",
        "title": "Self-Supervised Models are Continual Learners",
        "authors": "Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, Julien Mairal",
        "abstract": "Self-supervised models have been shown to produce comparable or better visual\nrepresentations than their supervised counterparts when trained offline on\nunlabeled data at scale. However, their efficacy is catastrophically reduced in\na Continual Learning (CL) scenario where data is presented to the model\nsequentially. In this paper, we show that self-supervised loss functions can be\nseamlessly converted into distillation mechanisms for CL by adding a predictor\nnetwork that maps the current state of the representations to their past state.\nThis enables us to devise a framework for Continual self-supervised visual\nrepresentation Learning that (i) significantly improves the quality of the\nlearned representations, (ii) is compatible with several state-of-the-art\nself-supervised objectives, and (iii) needs little to no hyperparameter tuning.\nWe demonstrate the effectiveness of our approach empirically by training six\npopular self-supervised models in various CL settings.",
        "timestamp": "2025-01-22T07:08:23.330Z",
        "rating": "novote",
        "published_date": "2021-12-08T10:39:13Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-22T07:08:23+00:00",
        "updated_at": "2025-01-22T07:08:26+00:00",
        "version": 2
      }
    },
    "interactions:2412.09315": {
      "data": {
        "paper_id": "2412.09315",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T20:16:58+00:00",
        "updated_at": "2025-01-21T20:17:01+00:00",
        "version": 2
      }
    },
    "paper:2412.09315": {
      "data": {
        "arxivId": "2412.09315",
        "url": "https://arxiv.org/abs/2412.09315",
        "title": "Beware of Metacognitive Laziness: Effects of Generative Artificial\n  Intelligence on Learning Motivation, Processes, and Performance",
        "authors": "Yizhou Fan, Luzhen Tang, Huixiao Le, Kejie Shen, Shufang Tan, Yueying Zhao, Yuan Shen, Xinyu Li, Dragan Ga\u0161evi\u0107",
        "abstract": "With the continuous development of technological and educational innovation,\nlearners nowadays can obtain a variety of support from agents such as teachers,\npeers, education technologies, and recently, generative artificial intelligence\nsuch as ChatGPT. The concept of hybrid intelligence is still at a nascent\nstage, and how learners can benefit from a symbiotic relationship with various\nagents such as AI, human experts and intelligent learning systems is still\nunknown. The emerging concept of hybrid intelligence also lacks deep insights\nand understanding of the mechanisms and consequences of hybrid human-AI\nlearning based on strong empirical research. In order to address this gap, we\nconducted a randomised experimental study and compared learners' motivations,\nself-regulated learning processes and learning performances on a writing task\namong different groups who had support from different agents (ChatGPT, human\nexpert, writing analytics tools, and no extra tool). A total of 117 university\nstudents were recruited, and their multi-channel learning, performance and\nmotivation data were collected and analysed. The results revealed that:\nlearners who received different learning support showed no difference in\npost-task intrinsic motivation; there were significant differences in the\nfrequency and sequences of the self-regulated learning processes among groups;\nChatGPT group outperformed in the essay score improvement but their knowledge\ngain and transfer were not significantly different. Our research found that in\nthe absence of differences in motivation, learners with different supports\nstill exhibited different self-regulated learning processes, ultimately leading\nto differentiated performance. What is particularly noteworthy is that AI\ntechnologies such as ChatGPT may promote learners' dependence on technology and\npotentially trigger metacognitive laziness.",
        "timestamp": "2025-01-21T20:05:49.177Z",
        "rating": "novote",
        "published_date": "2024-12-12T14:32:39Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.HC"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T20:05:49+00:00",
        "updated_at": "2025-01-21T20:05:53+00:00",
        "version": 2
      }
    },
    "interactions:2412.12095": {
      "data": {
        "paper_id": "2412.12095",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T16:27:01+00:00",
        "updated_at": "2025-01-21T16:27:05+00:00",
        "version": 2
      }
    },
    "paper:2412.12095": {
      "data": {
        "arxivId": "2412.12095",
        "url": "https://arxiv.org/abs/2412.12095",
        "title": "Causal Diffusion Transformers for Generative Modeling",
        "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan",
        "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.",
        "timestamp": "2025-01-21T16:26:49.971Z",
        "rating": "novote",
        "published_date": "2024-12-16T18:59:29Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T16:26:50+00:00",
        "updated_at": "2025-01-21T16:26:53+00:00",
        "version": 2
      }
    },
    "paper:2501.05441": {
      "data": {
        "arxivId": "2501.05441",
        "url": "https://arxiv.org/abs/2501.05441",
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "authors": "Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin",
        "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN\narchitectures in the literature are littered with empirical tricks. We provide\nevidence against this claim and build a modern GAN baseline in a more\nprincipled manner. First, we derive a well-behaved regularized relativistic GAN\nloss that addresses issues of mode dropping and non-convergence that were\npreviously tackled via a bag of ad-hoc tricks. We analyze our loss\nmathematically and prove that it admits local convergence guarantees, unlike\nmost existing relativistic losses. Second, our new loss allows us to discard\nall ad-hoc tricks and replace outdated backbones used in common GANs with\nmodern architectures. Using StyleGAN2 as an example, we present a roadmap of\nsimplification and modernization that results in a new minimalist baseline --\nR3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,\nImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against\nstate-of-the-art GANs and diffusion models.",
        "timestamp": "2025-01-21T16:24:13.360Z",
        "rating": "novote",
        "published_date": "2025-01-09T18:53:06Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-01-21T16:24:13+00:00",
        "updated_at": "2025-01-21T16:24:17+00:00",
        "version": 2
      }
    },
    "interactions:2501.05441": {
      "data": {
        "paper_id": "2501.05441",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-21T16:24:38+00:00",
        "updated_at": "2025-01-21T16:24:42+00:00",
        "version": 2
      }
    },
    "interactions:2407.05872": {
      "data": {
        "paper_id": "2407.05872",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-20T19:45:01+00:00",
        "updated_at": "2025-01-20T19:45:03+00:00",
        "version": 2
      }
    },
    "paper:2407.05872": {
      "data": {
        "arxivId": "2407.05872",
        "url": "https://arxiv.org/abs/2407.05872",
        "title": "Scaling Exponents Across Parameterizations and Optimizers",
        "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, Jeffrey Pennington",
        "abstract": "Robust and effective scaling of models from small to large width typically\nrequires the precise adjustment of many algorithmic and architectural details,\nsuch as parameterization and optimizer choices. In this work, we propose a new\nperspective on parameterization by investigating a key assumption in prior work\nabout the alignment between parameters and data and derive new theoretical\nresults under weaker assumptions and a broader set of optimizers. Our extensive\nempirical investigation includes tens of thousands of models trained with all\ncombinations of three optimizers, four parameterizations, several alignment\nassumptions, more than a dozen learning rates, and fourteen model sizes up to\n26.8B parameters. We find that the best learning rate scaling prescription\nwould often have been excluded by the assumptions in prior work. Our results\nshow that all parameterizations, not just maximal update parameterization\n(muP), can achieve hyperparameter transfer; moreover, our novel per-layer\nlearning rate prescription for standard parameterization outperforms muP.\nFinally, we demonstrate that an overlooked aspect of parameterization, the\nepsilon parameter in Adam, must be scaled correctly to avoid gradient underflow\nand propose Adam-atan2, a new numerically stable, scale-invariant version of\nAdam that eliminates the epsilon hyperparameter entirely.",
        "timestamp": "2025-01-20T19:44:42.999Z",
        "rating": "novote",
        "published_date": "2024-07-08T12:32:51Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:44:43+00:00",
        "updated_at": "2025-01-20T19:44:46+00:00",
        "version": 2
      }
    },
    "interactions:2402.06184": {
      "data": {
        "paper_id": "2402.06184",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T19:26:52.959Z",
            "data": {
              "duration_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:26:46+00:00",
        "updated_at": "2025-01-24T08:17:12+00:00",
        "version": 4
      }
    },
    "paper:2402.06184": {
      "data": {
        "arxivId": "2402.06184",
        "url": "https://arxiv.org/abs/2402.06184",
        "title": "The boundary of neural network trainability is fractal",
        "authors": "Jascha Sohl-Dickstein",
        "abstract": "Some fractals -- for instance those associated with the Mandelbrot and\nquadratic Julia sets -- are computed by iterating a function, and identifying\nthe boundary between hyperparameters for which the resulting series diverges or\nremains bounded. Neural network training similarly involves iterating an update\nfunction (e.g. repeated steps of gradient descent), can result in convergent or\ndivergent behavior, and can be extremely sensitive to small changes in\nhyperparameters. Motivated by these similarities, we experimentally examine the\nboundary between neural network hyperparameters that lead to stable and\ndivergent training. We find that this boundary is fractal over more than ten\ndecades of scale in all tested configurations.",
        "timestamp": "2025-01-20T19:26:32.219Z",
        "rating": "novote",
        "published_date": "2024-02-09T04:46:48Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.NE",
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T19:26:32+00:00",
        "updated_at": "2025-01-20T19:26:37+00:00",
        "version": 2
      }
    },
    "interactions:2410.05229": {
      "data": {
        "paper_id": "2410.05229",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T09:41:41.160Z",
            "data": {
              "duration_seconds": 108
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:41:36+00:00",
        "updated_at": "2025-01-24T08:17:19+00:00",
        "version": 4
      }
    },
    "interactions:2411.04872": {
      "data": {
        "paper_id": "2411.04872",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T09:39:40.379Z",
            "data": {
              "duration_seconds": 30
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:38:50+00:00",
        "updated_at": "2025-01-24T08:17:26+00:00",
        "version": 5
      }
    },
    "paper:2410.05229": {
      "data": {
        "arxivId": "2410.05229",
        "url": "https://arxiv.org/abs/2410.05229",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models",
        "authors": "Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar",
        "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.",
        "timestamp": "2025-01-20T09:39:37.110Z",
        "rating": "novote",
        "published_date": "2024-10-07T17:36:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:39:37+00:00",
        "updated_at": "2025-01-20T09:39:40+00:00",
        "version": 2
      }
    },
    "paper:2411.04872": {
      "data": {
        "arxivId": "2411.04872",
        "url": "https://arxiv.org/abs/2411.04872",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning\n  in AI",
        "authors": "Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli J\u00e4rviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, Mark Wildon",
        "abstract": "We introduce FrontierMath, a benchmark of hundreds of original, exceptionally\nchallenging mathematics problems crafted and vetted by expert mathematicians.\nThe questions cover most major branches of modern mathematics -- from\ncomputationally intensive problems in number theory and real analysis to\nabstract questions in algebraic geometry and category theory. Solving a typical\nproblem requires multiple hours of effort from a researcher in the relevant\nbranch of mathematics, and for the upper end questions, multiple days.\nFrontierMath uses new, unpublished problems and automated verification to\nreliably evaluate models while minimizing risk of data contamination. Current\nstate-of-the-art AI models solve under 2% of problems, revealing a vast gap\nbetween AI capabilities and the prowess of the mathematical community. As AI\nsystems advance toward expert-level mathematical abilities, FrontierMath offers\na rigorous testbed that quantifies their progress.",
        "timestamp": "2025-01-20T09:38:38.997Z",
        "rating": "novote",
        "published_date": "2024-11-07T17:07:35Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T09:38:39+00:00",
        "updated_at": "2025-01-20T09:38:42+00:00",
        "version": 2
      }
    },
    "paper:1910.02054": {
      "data": {
        "arxivId": "1910.02054",
        "url": "https://arxiv.org/abs/1910.02054",
        "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
        "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He",
        "abstract": "Large deep learning models offer significant accuracy gains, but training\nbillions to trillions of parameters is challenging. Existing solutions such as\ndata and model parallelisms exhibit fundamental limitations to fit these models\ninto limited device memory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero Redundancy Optimizer\n(ZeRO), to optimize memory, vastly improving training speed while increasing\nthe model size that can be efficiently trained. ZeRO eliminates memory\nredundancies in data- and model-parallel training while retaining low\ncommunication volume and high computational granularity, allowing us to scale\nthe model size proportional to the number of devices with sustained high\nefficiency. Our analysis on memory requirements and communication volume\ndemonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters\nusing today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter\nwith super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.\nThis represents an 8x increase in model size and 10x increase in achievable\nperformance over state-of-the-art. In terms of usability, ZeRO can train large\nmodels of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)\nwithout requiring model parallelism which is harder for scientists to apply.\nLast but not the least, researchers have used the system breakthroughs of ZeRO\nto create the world's largest language model (Turing-NLG, 17B parameters) with\nrecord breaking accuracy.",
        "timestamp": "2025-01-20T07:32:15.795Z",
        "rating": "novote",
        "published_date": "2019-10-04T17:29:39Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.DC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:32:16+00:00",
        "updated_at": "2025-01-20T07:32:19+00:00",
        "version": 2
      }
    },
    "interactions:2104.07857": {
      "data": {
        "paper_id": "2104.07857",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:31:48.486Z",
            "data": {
              "duration_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:31:40+00:00",
        "updated_at": "2025-01-24T08:17:35+00:00",
        "version": 4
      }
    },
    "paper:2104.07857": {
      "data": {
        "arxivId": "2104.07857",
        "url": "https://arxiv.org/abs/2104.07857",
        "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning",
        "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He",
        "abstract": "In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.",
        "timestamp": "2025-01-20T07:31:26.129Z",
        "rating": "novote",
        "published_date": "2021-04-16T02:22:12Z",
        "arxiv_tags": [
          "cs.DC",
          "cs.AI",
          "cs.LG",
          "cs.PF"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:31:26+00:00",
        "updated_at": "2025-01-20T07:31:29+00:00",
        "version": 2
      }
    },
    "interactions:2501.00663": {
      "data": {
        "paper_id": "2501.00663",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-21T15:10:34.138Z",
            "data": {
              "duration_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:45:44.726Z",
            "data": {
              "session_id": "session_1739659522005_qm655og",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:45:22.005Z",
              "end_time": "2025-02-15T22:45:44.081Z",
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:54:45.420Z",
            "data": {
              "session_id": "session_1739660077611_ts0pcur",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:54:37.611Z",
              "end_time": "2025-02-15T22:54:44.829Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:23:22+00:00",
        "updated_at": "2025-02-15T22:55:38+00:00",
        "version": 14
      }
    },
    "paper:2501.00663": {
      "data": {
        "arxivId": "2501.00663",
        "url": "https://arxiv.org/abs/2501.00663",
        "title": "Titans: Learning to Memorize at Test Time",
        "authors": "Ali Behrouz, Peilin Zhong, Vahab Mirrokni",
        "abstract": "Over more than a decade there has been an extensive research effort on how to\neffectively utilize recurrent models and attention. While recurrent models aim\nto compress the data into a fixed-size memory (called hidden state), attention\nallows attending to the entire context window, capturing the direct\ndependencies of all tokens. This more accurate modeling of dependencies,\nhowever, comes with a quadratic cost, limiting the model to a fixed-length\ncontext. We present a new neural long-term memory module that learns to\nmemorize historical context and helps attention to attend to the current\ncontext while utilizing long past information. We show that this neural memory\nhas the advantage of fast parallelizable training while maintaining a fast\ninference. From a memory perspective, we argue that attention due to its\nlimited context but accurate dependency modeling performs as a short-term\nmemory, while neural memory due to its ability to memorize the data, acts as a\nlong-term, more persistent, memory. Based on these two modules, we introduce a\nnew family of architectures, called Titans, and present three variants to\naddress how one can effectively incorporate memory into this architecture. Our\nexperimental results on language modeling, common-sense reasoning, genomics,\nand time series tasks show that Titans are more effective than Transformers and\nrecent modern linear recurrent models. They further can effectively scale to\nlarger than 2M context window size with higher accuracy in needle-in-haystack\ntasks compared to baselines.",
        "timestamp": "2025-01-20T07:22:12.333Z",
        "rating": "novote",
        "published_date": "2024-12-31T22:32:03Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:22:12+00:00",
        "updated_at": "2025-01-20T07:22:15+00:00",
        "version": 2
      }
    },
    "paper:2412.06769": {
      "data": {
        "arxivId": "2412.06769",
        "url": "https://arxiv.org/abs/2412.06769",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "authors": "Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian",
        "abstract": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
        "timestamp": "2025-01-20T07:20:45.082Z",
        "rating": "novote",
        "published_date": "2024-12-09T18:55:56Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:20:45+00:00",
        "updated_at": "2025-01-20T07:20:48+00:00",
        "version": 2
      }
    },
    "interactions:2412.06769": {
      "data": {
        "paper_id": "2412.06769",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:33:30.990Z",
            "data": {
              "session_id": "session_1739658786833_yfuyeda",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:33:06.833Z",
              "end_time": "2025-02-15T22:33:30.437Z",
              "total_elapsed_seconds": 24
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:53:42.044Z",
            "data": {
              "session_id": "session_1739660014301_kbf1c52",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:53:34.301Z",
              "end_time": "2025-02-15T22:53:41.405Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:55:32.213Z",
            "data": {
              "session_id": "session_1739660126339_1ooh9yv",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:55:26.339Z",
              "end_time": "2025-02-15T22:55:32.012Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:21:08+00:00",
        "updated_at": "2025-02-15T22:56:24+00:00",
        "version": 5
      }
    },
    "interactions:2501.09891": {
      "data": {
        "paper_id": "2501.09891",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:56:09.225Z",
            "data": {
              "session_id": "session_1739660159139_vp6mz1b",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:55:59.139Z",
              "end_time": "2025-02-15T22:56:08.755Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:18:47+00:00",
        "updated_at": "2025-02-15T22:56:57+00:00",
        "version": 3
      }
    },
    "paper:2501.09891": {
      "data": {
        "arxivId": "2501.09891",
        "url": "https://arxiv.org/abs/2501.09891",
        "title": "Evolving Deeper LLM Thinking",
        "authors": "Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen",
        "abstract": "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver.",
        "timestamp": "2025-01-20T07:17:04.098Z",
        "rating": "novote",
        "published_date": "2025-01-17T00:41:44Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:17:04+00:00",
        "updated_at": "2025-01-20T07:17:07+00:00",
        "version": 2
      }
    },
    "interactions:2403.09635": {
      "data": {
        "paper_id": "2403.09635",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T07:13:23.907Z",
            "data": {
              "duration_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:13:01+00:00",
        "updated_at": "2025-01-24T08:18:34+00:00",
        "version": 4
      }
    },
    "paper:2403.09635": {
      "data": {
        "arxivId": "2403.09635",
        "url": "https://arxiv.org/abs/2403.09635",
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models",
        "authors": "Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee",
        "abstract": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 1000\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across encoder-only, decoder-only and\nencoder-decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for Image Classification.",
        "timestamp": "2025-01-20T07:12:49.004Z",
        "rating": "novote",
        "published_date": "2024-03-14T17:59:14Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV",
          "cs.LG",
          "I.2.7; I.2.10"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T07:12:49+00:00",
        "updated_at": "2025-01-20T07:12:52+00:00",
        "version": 2
      }
    },
    "interactions:2006.08570": {
      "data": {
        "paper_id": "2006.08570",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-20T05:50:39.552Z",
            "data": {
              "duration_seconds": 32
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-20T05:50:00+00:00",
        "updated_at": "2025-01-24T08:18:48+00:00",
        "version": 8
      }
    },
    "paper:2006.08570": {
      "data": {
        "arxivId": "2006.08570",
        "url": "https://arxiv.org/abs/2006.08570",
        "title": "Cross-temporal forecast reconciliation: Optimal combination method and\n  heuristic alternatives",
        "authors": "Tommaso Di Fonzo, Daniele Girolimetto",
        "abstract": "Forecast reconciliation is a post-forecasting process aimed to improve the\nquality of the base forecasts for a system of hierarchical/grouped time series\n(Hyndman et al., 2011). Contemporaneous (cross-sectional) and temporal\nhierarchies have been considered in the literature, but - except for Kourentzes\nand Athanasopoulos (2019) - generally these two features have not been fully\nconsidered together. Adopting a notation able to simultaneously deal with both\nforecast reconciliation dimensions, the paper shows two new results: (i) an\niterative cross-temporal forecast reconciliation procedure which extends, and\novercomes some weaknesses of, the two-step procedure by Kourentzes and\nAthanasopoulos (2019), and (ii) the closed-form expression of the optimal (in\nleast squares sense) point forecasts which fulfill both contemporaneous and\ntemporal constraints. The feasibility of the proposed procedures, along with\nfirst evaluations of their performance as compared to the most performing\n`single dimension' (either cross-sectional or temporal) forecast reconciliation\nprocedures, is studied through a forecasting experiment on the 95 quarterly\ntime series of the Australian GDP from Income and Expenditure sides considered\nby Athanasopoulos et al. (2019).",
        "timestamp": "2025-01-20T05:49:19.264Z",
        "rating": "novote",
        "published_date": "2020-06-15T17:34:05Z",
        "arxiv_tags": [
          "stat.ME"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T05:49:20+00:00",
        "updated_at": "2025-01-20T05:49:23+00:00",
        "version": 2
      }
    },
    "interactions:2006.02043": {
      "data": {
        "paper_id": "2006.02043",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-20T00:20:33+00:00",
        "updated_at": "2025-01-20T00:20:36+00:00",
        "version": 2
      }
    },
    "paper:2006.02043": {
      "data": {
        "arxivId": "2006.02043",
        "url": "https://arxiv.org/pdf/2006.02043",
        "title": "Hierarchical forecast reconciliation with machine learning",
        "authors": "Evangelos Spiliotis, Mahdi Abolghasemi, Rob J Hyndman, Fotios Petropoulos, Vassilios Assimakopoulos",
        "abstract": "Hierarchical forecasting methods have been widely used to support aligned\ndecision-making by providing coherent forecasts at different aggregation\nlevels. Traditional hierarchical forecasting approaches, such as the bottom-up\nand top-down methods, focus on a particular aggregation level to anchor the\nforecasts. During the past decades, these have been replaced by a variety of\nlinear combination approaches that exploit information from the complete\nhierarchy to produce more accurate forecasts. However, the performance of these\ncombination methods depends on the particularities of the examined series and\ntheir relationships. This paper proposes a novel hierarchical forecasting\napproach based on machine learning that deals with these limitations in three\nimportant ways. First, the proposed method allows for a non-linear combination\nof the base forecasts, thus being more general than the linear approaches.\nSecond, it structurally combines the objectives of improved post-sample\nempirical forecasting accuracy and coherence. Finally, due to its non-linear\nnature, our approach selectively combines the base forecasts in a direct and\nautomated way without requiring that the complete information must be used for\nproducing reconciled forecasts for each series and level. The proposed method\nis evaluated both in terms of accuracy and bias using two different data sets\ncoming from the tourism and retail industries. Our results suggest that the\nproposed method gives superior point forecasts than existing approaches,\nespecially when the series comprising the hierarchy are not characterized by\nthe same patterns.",
        "timestamp": "2025-01-20T00:20:16.561Z",
        "rating": "novote",
        "published_date": "2020-06-03T04:49:39Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.CO",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-20T00:20:17+00:00",
        "updated_at": "2025-01-20T00:20:20+00:00",
        "version": 2
      }
    },
    "paper:1801.02042": {
      "data": {
        "arxivId": "1801.02042",
        "url": "https://arxiv.org/pdf/1801.02042",
        "title": "Learning from Neighbors about a Changing State",
        "authors": "Krishna Dasaratha, Benjamin Golub, Nir Hak",
        "abstract": "Agents learn about a changing state using private signals and their\nneighbors' past estimates of the state. We present a model in which Bayesian\nagents in equilibrium use neighbors' estimates simply by taking weighted sums\nwith time-invariant weights. The dynamics thus parallel those of the tractable\nDeGroot model of learning in networks, but arise as an equilibrium outcome\nrather than a behavioral assumption. We examine whether information aggregation\nis nearly optimal as neighborhoods grow large. A key condition for this is\nsignal diversity: each individual's neighbors have private signals that not\nonly contain independent information, but also have sufficiently different\ndistributions. Without signal diversity $\\unicode{x2013}$ e.g., if private\nsignals are i.i.d. $\\unicode{x2013}$ learning is suboptimal in all networks and\nhighly inefficient in some. Turning to social influence, we find it is much\nmore sensitive to one's signal quality than to one's number of neighbors, in\ncontrast to standard models with exogenous updating rules.",
        "timestamp": "2025-01-19T20:59:30.013Z",
        "rating": "novote",
        "published_date": "2018-01-06T16:14:47Z",
        "arxiv_tags": [
          "econ.TH",
          "cs.GT",
          "cs.SI"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T20:59:30+00:00",
        "updated_at": "2025-01-19T20:59:33+00:00",
        "version": 2
      }
    },
    "interactions:1710.06026": {
      "data": {
        "paper_id": "1710.06026",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-01-19T20:57:30+00:00",
        "updated_at": "2025-01-19T20:57:33+00:00",
        "version": 2
      }
    },
    "paper:1710.06026": {
      "data": {
        "arxivId": "1710.06026",
        "url": "https://arxiv.org/abs/1710.06026",
        "title": "Targeting Interventions in Networks",
        "authors": "Andrea Galeotti, Benjamin Golub, Sanjeev Goyal",
        "abstract": "We study games in which a network mediates strategic spillovers and\nexternalities among the players. How does a planner optimally target\ninterventions that change individuals' private returns to investment? We\nanalyze this question by decomposing any intervention into orthogonal principal\ncomponents, which are determined by the network and are ordered according to\ntheir associated eigenvalues. There is a close connection between the nature of\nspillovers and the representation of various principal components in the\noptimal intervention. In games of strategic complements (substitutes),\ninterventions place more weight on the top (bottom) principal components, which\nreflect more global (local) network structure. For large budgets, optimal\ninterventions are simple -- they involve a single principal component.",
        "timestamp": "2025-01-19T20:56:52.926Z",
        "rating": "novote",
        "published_date": "2017-10-16T23:18:55Z",
        "arxiv_tags": [
          "cs.GT"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T20:56:53+00:00",
        "updated_at": "2025-01-19T20:56:56+00:00",
        "version": 2
      }
    },
    "interactions:2307.13912": {
      "data": {
        "paper_id": "2307.13912",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T15:26:37.045Z",
            "data": {
              "duration_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T15:26:37+00:00",
        "updated_at": "2025-01-24T08:19:20+00:00",
        "version": 3
      }
    },
    "paper:2307.13912": {
      "data": {
        "arxivId": "2307.13912",
        "url": "https://arxiv.org/abs/2307.13912",
        "title": "Embedding Democratic Values into Social Media AIs via Societal Objective\n  Functions",
        "authors": "Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, Michael S. Bernstein",
        "abstract": "Can we design artificial intelligence (AI) systems that rank our social media\nfeeds to consider democratic values such as mitigating partisan animosity as\npart of their objective functions? We introduce a method for translating\nestablished, vetted social scientific constructs into AI objective functions,\nwhich we term societal objective functions, and demonstrate the method with\napplication to the political science construct of anti-democratic attitudes.\nTraditionally, we have lacked observable outcomes to use to train such models,\nhowever, the social sciences have developed survey instruments and qualitative\ncodebooks for these constructs, and their precision facilitates translation\ninto detailed prompts for large language models. We apply this method to create\na democratic attitude model that estimates the extent to which a social media\npost promotes anti-democratic attitudes, and test this democratic attitude\nmodel across three studies. In Study 1, we first test the attitudinal and\nbehavioral effectiveness of the intervention among US partisans (N=1,380) by\nmanually annotating (alpha=.895) social media posts with anti-democratic\nattitude scores and testing several feed ranking conditions based on these\nscores. Removal (d=.20) and downranking feeds (d=.25) reduced participants'\npartisan animosity without compromising their experience and engagement. In\nStudy 2, we scale up the manual labels by creating the democratic attitude\nmodel, finding strong agreement with manual labels (rho=.75). Finally, in Study\n3, we replicate Study 1 using the democratic attitude model instead of manual\nlabels to test its attitudinal and behavioral impact (N=558), and again find\nthat the feed downranking using the societal objective function reduced\npartisan animosity (d=.25). This method presents a novel strategy to draw on\nsocial science theory and methods to mitigate societal harms in social media\nAIs.",
        "timestamp": "2025-01-19T15:26:24.313Z",
        "rating": "novote",
        "published_date": "2023-07-26T02:27:24Z",
        "arxiv_tags": [
          "cs.HC",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T15:26:24+00:00",
        "updated_at": "2025-01-19T15:26:27+00:00",
        "version": 2
      }
    },
    "interactions:2410.05437": {
      "data": {
        "paper_id": "2410.05437",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T09:46:23.440Z",
            "data": {
              "duration_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T09:46:19+00:00",
        "updated_at": "2025-01-24T08:19:25+00:00",
        "version": 4
      }
    },
    "paper:2410.05437": {
      "data": {
        "arxivId": "2410.05437",
        "url": "https://arxiv.org/abs/2410.05437",
        "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
        "authors": "Charbel Sakr, Brucek Khailany",
        "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality\nreduction of activations. Unlike prior works on weight-centric tensor\ndecomposition, ESPACE projects activations onto a pre-calibrated set of\nprincipal components. The activation-centrality of the approach enables\nretraining LLMs with no loss of expressivity; while at inference, weight\ndecomposition is obtained as a byproduct of matrix multiplication\nassociativity. Theoretical results on the construction of projection matrices\nwith optimal computational accuracy are provided. Experimentally, we find\nESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small\naccuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At\nlower compression rates of 20% to 40%, ESPACE drives GPT3 models to\noutperforming their baseline, by up to a 0.38 decrease in perplexity for\nGPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency\non existing hardware. Comparison with related works on compressing Llama2-7B\nvia matrix factorization shows that ESPACE is a first step in advancing the\nstate-of-the-art in tensor decomposition compression of LLMs.",
        "timestamp": "2025-01-19T09:45:58.441Z",
        "rating": "novote",
        "published_date": "2024-10-07T18:59:22Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T09:45:58+00:00",
        "updated_at": "2025-01-19T09:46:01+00:00",
        "version": 2
      }
    },
    "interactions:2406.07522": {
      "data": {
        "paper_id": "2406.07522",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T04:15:52.932Z",
            "data": {
              "duration_seconds": 3,
              "session_config": {
                "idle_threshold_seconds": 300,
                "min_duration_seconds": 3,
                "continuous_activity_required": true,
                "partial_sessions_logged": false
              }
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:56:40.440Z",
            "data": {
              "session_id": "session_1739660195320_9xm6s3i",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:56:35.320Z",
              "end_time": "2025-02-15T22:56:40.074Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T03:47:59+00:00",
        "updated_at": "2025-02-15T22:57:23+00:00",
        "version": 5
      }
    },
    "paper:2406.07522": {
      "data": {
        "arxivId": "2406.07522",
        "url": "https://arxiv.org/abs/2406.07522",
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
        "authors": "Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen",
        "abstract": "Efficiently modeling sequences with infinite context length has long been a\nchallenging problem. Previous approaches have either suffered from quadratic\ncomputational complexity or limited extrapolation ability in length\ngeneralization. In this work, we present Samba, a simple hybrid architecture\nthat layer-wise combines Mamba, a selective State Space Model (SSM), with\nSliding Window Attention (SWA). Samba selectively compresses a given sequence\ninto recurrent hidden states while still maintaining the ability to precisely\nrecall recent memories with the attention mechanism. We scale Samba up to 3.8B\nparameters with 3.2T training tokens and demonstrate that it significantly\noutperforms state-of-the-art models across a variety of benchmarks. Pretrained\non sequences of 4K length, Samba shows improved perplexity in context lengths\nof up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba\nefficiently extrapolates to a 256K context length with perfect memory recall on\nthe Passkey Retrieval task, and exhibits superior retrieval extrapolation on\nthe challenging Phonebook task compared to full-attention models. As a\nlinear-time sequence model, Samba achieves a 3.73x higher throughput compared\nto Transformers with grouped-query attention for user prompts of 128K length,\nand a 3.64x speedup when generating 64K tokens with unlimited streaming. Our\ncode for training on open source data is publicly available at\nhttps://github.com/microsoft/Samba.",
        "timestamp": "2025-01-19T03:47:40.608Z",
        "rating": "novote",
        "published_date": "2024-06-11T17:50:51Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T03:47:41+00:00",
        "updated_at": "2025-01-19T03:47:43+00:00",
        "version": 2
      }
    },
    "interactions:1503.03585": {
      "data": {
        "paper_id": "1503.03585",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-19T02:52:38.492Z",
            "data": {
              "duration_seconds": 77,
              "session_config": {
                "idle_threshold_seconds": 300,
                "min_duration_seconds": 3,
                "continuous_activity_required": true,
                "partial_sessions_logged": false
              }
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:51:09.439Z",
            "data": {
              "session_id": "session_1740153060651_kntieop",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:51:00.651Z",
              "end_time": "2025-02-21T15:51:09.049Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-19T02:52:39+00:00",
        "updated_at": "2025-02-21T15:52:56+00:00",
        "version": 4
      }
    },
    "paper:1503.03585": {
      "data": {
        "arxivId": "1503.03585",
        "url": "https://arxiv.org/abs/1503.03585",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.",
        "timestamp": "2025-01-19T02:31:15.039Z",
        "rating": "novote",
        "published_date": "2015-03-12T04:51:37Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "q-bio.NC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-19T02:31:15+00:00",
        "updated_at": "2025-01-19T02:31:18+00:00",
        "version": 2
      }
    },
    "interactions:2405.07987": {
      "data": {
        "paper_id": "2405.07987",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:33:43.312Z",
            "data": {
              "session_id": "session_1737775995128_823e123",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:33:15.128Z",
              "end_time": "2025-01-25T03:33:37.681Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:34:26.731Z",
            "data": {
              "session_id": "session_1737776038591_jr5426l",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:33:58.591Z",
              "end_time": "2025-01-25T03:34:26.720Z",
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T03:34:53.954Z",
            "data": {
              "session_id": "session_1737776069173_63927kh",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-01-25T03:34:29.173Z",
              "end_time": "2025-01-25T03:34:51.012Z",
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:07:48.512Z",
            "data": {
              "session_id": "session_1737781656680_dfgmzhc",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:07:36.680Z",
              "end_time": "2025-01-25T05:07:48.504Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:20:22.857Z",
            "data": {
              "session_id": "session_1737782408397_970dbde",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:20:08.397Z",
              "end_time": "2025-01-25T05:20:22.245Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T05:28:39.568Z",
            "data": {
              "session_id": "session_1737782910571_c1pwhng",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-25T05:28:30.571Z",
              "end_time": "2025-01-25T05:28:38.176Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-25T06:22:48.568Z",
            "data": {
              "session_id": "session_1737786153474_cyz0a5e",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-01-25T06:22:33.474Z",
              "end_time": "2025-01-25T06:22:48.369Z",
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:30:34.786Z",
            "data": {
              "session_id": "session_1737912621845_hrkv0t1",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:30:21.846Z",
              "end_time": "2025-01-26T17:30:34.593Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:31:52.770Z",
            "data": {
              "session_id": "session_1737912693672_92zw2fd",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:31:33.672Z",
              "end_time": "2025-01-26T17:31:52.108Z",
              "total_elapsed_seconds": 18
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T17:44:24.170Z",
            "data": {
              "session_id": "session_1737913455635_be9rxrm",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-26T17:44:15.635Z",
              "end_time": "2025-01-26T17:44:24.158Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:00:14.764Z",
            "data": {
              "session_id": "session_1737914406476_buksn0u",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:00:06.476Z",
              "end_time": "2025-01-26T18:00:14.756Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:04:30.486Z",
            "data": {
              "session_id": "session_1737914665494_dzmw70a",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:04:25.494Z",
              "end_time": "2025-01-26T18:04:30.295Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:08:42.226Z",
            "data": {
              "session_id": "session_1737914915235_xmyy3nu",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:08:35.235Z",
              "end_time": "2025-01-26T18:08:39.094Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T18:46:46.046Z",
            "data": {
              "session_id": "session_1737917194590_uxzn9qg",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-26T18:46:34.590Z",
              "end_time": "2025-01-26T18:46:46.035Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T19:04:15.701Z",
            "data": {
              "session_id": "session_1737918242839_149myfe",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-01-26T19:04:02.839Z",
              "end_time": "2025-01-26T19:04:15.690Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-25T03:33:39+00:00",
        "updated_at": "2025-01-27T00:19:06+00:00",
        "version": 34
      }
    },
    "paper:2405.07987": {
      "data": {
        "arxivId": "2405.07987",
        "url": "https://arxiv.org/abs/2405.07987",
        "title": "The Platonic Representation Hypothesis",
        "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
        "abstract": "We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.",
        "timestamp": "2025-01-25T03:33:15.181Z",
        "rating": "thumbsup",
        "published_date": "2024-05-13T17:58:30Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CV",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-01-25T03:33:15+00:00",
        "updated_at": "2025-01-25T05:08:15+00:00",
        "version": 4
      }
    },
    "interactions:2501.13928": {
      "data": {
        "paper_id": "2501.13928",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:21:43.630Z",
            "data": {
              "session_id": "session_1737872496276_hp25cq5",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:21:36.276Z",
              "end_time": "2025-01-26T06:21:42.903Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:30:24.892Z",
            "data": {
              "session_id": "session_1737873020744_dpvbin3",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:30:20.744Z",
              "end_time": "2025-01-26T06:30:23.772Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-01-26T06:32:00.574Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T06:33:09.937Z",
            "data": {
              "session_id": "session_1737873178285_xirs92q",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-26T06:32:58.285Z",
              "end_time": "2025-01-26T06:33:09.185Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T21:13:27.703Z",
            "data": {
              "session_id": "session_1737926001227_xo3ybup",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-01-26T21:13:21.227Z",
              "end_time": "2025-01-26T21:13:27.691Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-26T06:21:44+00:00",
        "updated_at": "2025-01-26T21:14:33+00:00",
        "version": 8
      }
    },
    "paper:2501.13928": {
      "data": {
        "arxivId": "2501.13928",
        "url": "https://arxiv.org/abs/2501.13928",
        "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
        "authors": "Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli",
        "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.",
        "timestamp": "2025-01-26T06:21:36.654Z",
        "rating": "thumbsup",
        "published_date": "2025-01-23T18:59:55Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.AI",
          "cs.GR",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-01-26T06:21:37+00:00",
        "updated_at": "2025-01-27T00:19:13+00:00",
        "version": 3
      }
    },
    "interactions:2501.12005": {
      "data": {
        "paper_id": "2501.12005",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T16:13:49.497Z",
            "data": {
              "session_id": "session_1737908018868_9fm5m83",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-26T16:13:38.868Z",
              "end_time": "2025-01-26T16:13:48.756Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-26T21:13:11.337Z",
            "data": {
              "session_id": "session_1737925981150_qdtg11j",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-26T21:13:01.150Z",
              "end_time": "2025-01-26T21:13:11.328Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-26T16:13:50+00:00",
        "updated_at": "2025-01-26T21:14:16+00:00",
        "version": 5
      }
    },
    "paper:2501.12005": {
      "data": {
        "arxivId": "2501.12005",
        "url": "https://arxiv.org/abs/2501.12005",
        "title": "A note on the relations between mixture models, maximum-likelihood and\n  entropic optimal transport",
        "authors": "Titouan Vayer, Etienne Lasalle",
        "abstract": "This note aims to demonstrate that performing maximum-likelihood estimation\nfor a mixture model is equivalent to minimizing over the parameters an optimal\ntransport problem with entropic regularization. The objective is pedagogical:\nwe seek to present this already known result in a concise and hopefully simple\nmanner. We give an illustration with Gaussian mixture models by showing that\nthe standard EM algorithm is a specific block-coordinate descent on an optimal\ntransport loss.",
        "timestamp": "2025-01-26T16:13:03.441Z",
        "rating": "novote",
        "published_date": "2025-01-21T09:55:21Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-26T16:13:03+00:00",
        "updated_at": "2025-01-26T16:13:06+00:00",
        "version": 2
      }
    },
    "interactions:1805.03929": {
      "data": {
        "paper_id": "1805.03929",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T02:01:33.953Z",
            "data": {
              "session_id": "session_1737943279095_i01t1lh",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-27T02:01:19.095Z",
              "end_time": "2025-01-27T02:01:33.412Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T02:00:30+00:00",
        "updated_at": "2025-01-27T02:01:49+00:00",
        "version": 5
      }
    },
    "paper:1805.03929": {
      "data": {
        "arxivId": "1805.03929",
        "url": "https://arxiv.org/abs/1805.03929",
        "title": "Resource-Bounded Kolmogorov Complexity Provides an Obstacle to Soficness\n  of Multidimensional Shifts",
        "authors": "Julien Destombes, Andrei Romashchenko",
        "abstract": "We suggest necessary conditions of soficness of multidimensional shifts\nformulated in termsof resource-bounded Kolmogorov complexity. Using this\ntechnique we provide examples ofeffective and non-sofic shifts on\n$\\mathbb{Z}^2$ with very low block complexity: the number of globallyadmissible\npatterns of size $n\\times n$ grows only as a polynomial in $n$. We also show\nthat moreconventional proofs of non-soficness for multi-dimensional effective\nshifts can be expressed interms of Kolmogorov complexity with unbounded\ncomputational resources.",
        "timestamp": "2025-01-27T02:00:10.459Z",
        "rating": "novote",
        "published_date": "2018-05-10T11:47:47Z",
        "arxiv_tags": [
          "cs.DM",
          "cs.CC"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T02:00:10+00:00",
        "updated_at": "2025-01-27T02:00:14+00:00",
        "version": 2
      }
    },
    "interactions:2003.13176": {
      "data": {
        "paper_id": "2003.13176",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T06:08:42.368Z",
            "data": {
              "session_id": "session_1737958099679_ruudfyj",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-27T06:08:19.679Z",
              "end_time": "2025-01-27T06:08:42.363Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T06:13:24.982Z",
            "data": {
              "session_id": "session_1737958394369_83iesup",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-01-27T06:13:14.370Z",
              "end_time": "2025-01-27T06:13:24.369Z",
              "total_elapsed_seconds": 10
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:52:51.632Z",
            "data": {
              "session_id": "session_1738057952567_x4bib0s",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:52:32.567Z",
              "end_time": "2025-01-28T09:52:51.307Z",
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:55:42.816Z",
            "data": {
              "session_id": "session_1738058117330_7k7dn7k",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:55:17.330Z",
              "end_time": "2025-01-28T09:55:42.601Z",
              "total_elapsed_seconds": 25
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T06:08:28+00:00",
        "updated_at": "2025-01-28T09:57:03+00:00",
        "version": 7
      }
    },
    "paper:2003.13176": {
      "data": {
        "arxivId": "2003.13176",
        "url": "https://arxiv.org/pdf/2003.13176",
        "title": "Non-reciprocal phase transitions",
        "authors": "Michel Fruchart, Ryo Hanai, Peter B. Littlewood, Vincenzo Vitelli",
        "abstract": "Out of equilibrium, the lack of reciprocity is the rule rather than the\nexception. Non-reciprocal interactions occur, for instance, in networks of\nneurons, directional growth of interfaces, and synthetic active materials.\nWhile wave propagation in non-reciprocal media has recently been under intense\nstudy, less is known about the consequences of non-reciprocity on the\ncollective behavior of many-body systems. Here, we show that non-reciprocity\nleads to time-dependent phases where spontaneously broken symmetries are\ndynamically restored. The resulting phase transitions are controlled by\nspectral singularities called exceptional points. We describe the emergence of\nthese phases using insights from bifurcation theory and non-Hermitian quantum\nmechanics. Our approach captures non-reciprocal generalizations of three\narchetypal classes of self-organization out of equilibrium: synchronization,\nflocking and pattern formation. Collective phenomena in these non-reciprocal\nsystems range from active time-(quasi)crystals to exceptional-point enforced\npattern-formation and hysteresis. Our work paves the way towards a general\ntheory of critical phenomena in non-reciprocal matter.",
        "timestamp": "2025-01-27T06:07:05.033Z",
        "rating": "thumbsup",
        "published_date": "2020-03-30T01:09:20Z",
        "arxiv_tags": [
          "cond-mat.soft",
          "cond-mat.stat-mech"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T06:07:05+00:00",
        "updated_at": "2025-01-27T06:09:42+00:00",
        "version": 3
      }
    },
    "interactions:1309.6605": {
      "data": {
        "paper_id": "1309.6605",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-27T07:09:12.829Z",
            "data": {
              "session_id": "session_1737961749044_dapnb4p",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-01-27T07:09:09.044Z",
              "end_time": "2025-01-27T07:09:12.063Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:09:13+00:00",
        "updated_at": "2025-01-27T07:10:31+00:00",
        "version": 3
      }
    },
    "paper:1309.6605": {
      "data": {
        "arxivId": "1309.6605",
        "url": "https://arxiv.org/abs/1309.6605",
        "title": "Symmetries, Cluster Synchronization, and Isolated Desynchronization in\n  Complex Networks",
        "authors": "Louis M. Pecora, Francesco Sorrentino, Aaron M. Hagerstrom, Thomas E. Murphy, Rajarshi Roy",
        "abstract": "Synchronization is of central importance in power distribution,\ntelecommunication, neuronal, and biological networks. Many networks are\nobserved to produce patterns of synchronized clusters, but it has been\ndifficult to predict these clusters or understand the conditions under which\nthey form, except for in the simplest of networks. In this article, we shed\nlight on the intimate connection between network symmetry and cluster\nsynchronization. We introduce general techniques that use network symmetries to\nreveal the patterns of synchronized clusters and determine the conditions under\nwhich they persist. The connection between symmetry and cluster synchronization\nis experimentally explored using an electro-optic network. We experimentally\nobserve and theoretically predict a surprising phenomenon in which some\nclusters lose synchrony while leaving others synchronized. The results could\nguide the design of new power grid systems or lead to new understanding of the\ndynamical behavior of networks ranging from neural to social.",
        "timestamp": "2025-01-27T07:09:09.367Z",
        "rating": "novote",
        "published_date": "2013-09-25T18:42:34Z",
        "arxiv_tags": [
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:09:09+00:00",
        "updated_at": "2025-01-27T07:09:12+00:00",
        "version": 2
      }
    },
    "paper:0806.0594": {
      "data": {
        "arxivId": "0806.0594",
        "url": "https://arxiv.org/pdf/0806.0594",
        "title": "Solvable model for chimera states of coupled oscillators",
        "authors": "Daniel M. Abrams, Renato E. Mirollo, Steven H. Strogatz, Daniel A. Wiley",
        "abstract": "Networks of identical, symmetrically coupled oscillators can spontaneously\nsplit into synchronized and desynchronized sub-populations. Such chimera states\nwere discovered in 2002, but are not well understood theoretically. Here we\nobtain the first exact results about the stability, dynamics, and bifurcations\nof chimera states by analyzing a minimal model consisting of two interacting\npopulations of oscillators. Along with a completely synchronous state, the\nsystem displays stable chimeras, breathing chimeras, and saddle-node, Hopf and\nhomoclinic bifurcations of chimeras.",
        "timestamp": "2025-01-27T07:05:40.255Z",
        "rating": "novote",
        "published_date": "2008-06-03T17:26:51Z",
        "arxiv_tags": [
          "nlin.CD",
          "math.DS",
          "nlin.PS"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:05:40+00:00",
        "updated_at": "2025-01-27T07:05:43+00:00",
        "version": 2
      }
    },
    "paper:1101.2899": {
      "data": {
        "arxivId": "1101.2899",
        "url": "https://arxiv.org/pdf/1101.2899",
        "title": "A mathematical framework for critical transitions: bifurcations,\n  fast-slow systems and stochastic dynamics",
        "authors": "Christian Kuehn",
        "abstract": "Bifurcations can cause dynamical systems with slowly varying parameters to\ntransition to far-away attractors. The terms ``critical transition'' or\n``tipping point'' have been used to describe this situation. Critical\ntransitions have been observed in an astonishingly diverse set of applications\nfrom ecosystems and climate change to medicine and finance. The main goal of\nthis paper is to give an overview which standard mathematical theories can be\napplied to critical transitions. We shall focus on early-warning signs that\nhave been suggested to predict critical transitions and point out what\nmathematical theory can provide in this context. Starting from classical\nbifurcation theory and incorporating multiple time scale dynamics one can give\na detailed analysis of local bifurcations that induce critical transitions. We\nsuggest that the mathematical theory of fast-slow systems provides a natural\ndefinition of critical transitions. Since noise often plays a crucial role near\ncritical transitions the next step is to consider stochastic fast-slow systems.\nThe interplay between sample path techniques, partial differential equations\nand random dynamical systems is highlighted. Each viewpoint provides potential\nearly-warning signs for critical transitions. Since increasing variance has\nbeen suggested as an early-warning sign we examine it in the context of normal\nforms analytically, numerically and geometrically; we also consider\nautocorrelation numerically. Hence we demonstrate the applicability of\nearly-warning signs for generic models. We end with suggestions for future\ndirections of the theory.",
        "timestamp": "2025-01-27T07:03:04.997Z",
        "rating": "novote",
        "published_date": "2011-01-14T21:00:57Z",
        "arxiv_tags": [
          "math.DS",
          "math.CA",
          "nlin.CD",
          "nlin.PS"
        ]
      },
      "meta": {
        "created_at": "2025-01-27T07:03:05+00:00",
        "updated_at": "2025-01-27T07:03:08+00:00",
        "version": 2
      }
    },
    "interactions:2406.10165": {
      "data": {
        "paper_id": "2406.10165",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T06:36:47.999Z",
            "data": {
              "session_id": "session_1738046188859_n2yylp7",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-01-28T06:36:28.859Z",
              "end_time": "2025-01-28T06:36:37.649Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-28T06:36:42+00:00",
        "updated_at": "2025-01-28T06:38:02+00:00",
        "version": 4
      }
    },
    "paper:2406.10165": {
      "data": {
        "arxivId": "2406.10165",
        "url": "https://arxiv.org/abs/2406.10165",
        "title": "CarLLaVA: Vision language models for camera-only closed-loop driving",
        "authors": "Katrin Renz, Long Chen, Ana-Maria Marcu, Jan H\u00fcnermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, Oleg Sinavski",
        "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM)\nfor autonomous driving, developed for the CARLA Autonomous Driving Challenge\n2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA\narchitecture as backbone, achieving state-of-the-art closed-loop driving\nperformance with only camera input and without the need for complex or\nexpensive labels. Additionally, we show preliminary results on predicting\nlanguage commentary alongside the driving output. CarLLaVA uses a\nsemi-disentangled output representation of both path predictions and waypoints,\ngetting the advantages of the path for better lateral control and the waypoints\nfor better longitudinal control. We propose an efficient training recipe to\ntrain on large driving datasets without wasting compute on easy, trivial data.\nCarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving\nChallenge 2.0 outperforming the previous state of the art by 458% and the best\nconcurrent submission by 32.6%.",
        "timestamp": "2025-01-28T06:35:56.917Z",
        "rating": "novote",
        "published_date": "2024-06-14T16:35:47Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-01-28T06:35:57+00:00",
        "updated_at": "2025-01-28T06:36:00+00:00",
        "version": 2
      }
    },
    "interactions:2405.11932": {
      "data": {
        "paper_id": "2405.11932",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-28T09:53:49.265Z",
            "data": {
              "session_id": "session_1738058018689_749s57d",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-01-28T09:53:38.689Z",
              "end_time": "2025-01-28T09:53:49.260Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-28T09:53:30+00:00",
        "updated_at": "2025-01-28T09:54:51+00:00",
        "version": 4
      }
    },
    "paper:2405.11932": {
      "data": {
        "arxivId": "2405.11932",
        "url": "https://arxiv.org/abs/2405.11932v3",
        "title": "Nonequilbrium physics of generative diffusion models",
        "authors": "Zhendong Yu, Haiping Huang",
        "abstract": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.",
        "timestamp": "2025-01-28T09:53:25.159Z",
        "rating": "novote",
        "published_date": "2024-05-20T10:16:26Z",
        "arxiv_tags": [
          "cond-mat.stat-mech",
          "cond-mat.dis-nn",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-28T09:53:25+00:00",
        "updated_at": "2025-01-28T09:53:29+00:00",
        "version": 2
      }
    },
    "paper:2501.17161": {
      "data": {
        "arxivId": "2501.17161",
        "url": "https://arxiv.org/abs/2501.17161",
        "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model\n  Post-training",
        "authors": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma",
        "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used\npost-training techniques for foundation models. However, their roles in\nenhancing model generalization capabilities remain unclear. This paper studies\nthe difference between SFT and RL on generalization and memorization, focusing\non text-based rule variants and visual variants. We introduce GeneralPoints, an\narithmetic reasoning card game, and adopt V-IRL, a real-world navigation\nenvironment, to assess how models trained with SFT and RL generalize to unseen\nvariants in both textual and visual domains. We show that RL, especially when\ntrained with an outcome-based reward, generalizes across both rule-based\ntextual and visual variants. SFT, in contrast, tends to memorize training data\nand struggles to generalize out-of-distribution scenarios. Further analysis\nreveals that RL improves the model's underlying visual recognition\ncapabilities, contributing to its enhanced generalization in the visual domain.\nDespite RL's superior generalization, we show that SFT remains essential for\neffective RL training; SFT stabilizes the model's output format, enabling\nsubsequent RL to achieve its performance gains. These findings demonstrates the\ncapability of RL for acquiring generalizable knowledge in complex, multi-modal\ntasks.",
        "timestamp": "2025-01-29T07:56:12.258Z",
        "rating": "novote",
        "published_date": "2025-01-28T18:59:44Z",
        "arxiv_tags": [
          "cs.AI",
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-29T07:56:12+00:00",
        "updated_at": "2025-01-29T07:56:16+00:00",
        "version": 2
      }
    },
    "interactions:1408.3060": {
      "data": {
        "paper_id": "1408.3060",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T04:04:11.488Z",
            "data": {
              "session_id": "session_1738209830744_3gayufi",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-01-30T04:03:50.744Z",
              "end_time": "2025-01-30T04:04:04.457Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T04:04:06+00:00",
        "updated_at": "2025-01-30T04:05:50+00:00",
        "version": 4
      }
    },
    "paper:1408.3060": {
      "data": {
        "arxivId": "1408.3060",
        "url": "https://arxiv.org/abs/1408.3060",
        "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time",
        "authors": "Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola",
        "abstract": "Despite their successes, what makes kernel methods difficult to use in many\nlarge scale problems is the fact that storing and computing the decision\nfunction is typically expensive, especially at prediction time. In this paper,\nwe overcome this difficulty by proposing Fastfood, an approximation that\naccelerates such computation significantly. Key to Fastfood is the observation\nthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit\nproperties similar to dense Gaussian random matrices. Yet unlike the latter,\nHadamard and diagonal matrices are inexpensive to multiply and store. These two\nmatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks\nproposed by Rahimi and Recht (2009) and thereby speeding up the computation for\na large range of kernel functions. Specifically, Fastfood requires O(n log d)\ntime and O(n) storage to compute n non-linear basis functions in d dimensions,\na significant improvement from O(nd) computation and storage, without\nsacrificing accuracy.\n  Our method applies to any translation invariant and any dot-product kernel,\nsuch as the popular RBF kernels and polynomial kernels. We prove that the\napproximation is unbiased and has low variance. Experiments show that we\nachieve similar accuracy to full kernel expansions and Random Kitchen Sinks\nwhile being 100x faster and using 1000x less memory. These improvements,\nespecially in terms of memory usage, make kernel methods more practical for\napplications that have large training sets and/or require real-time prediction.",
        "timestamp": "2025-01-30T04:03:50.839Z",
        "rating": "novote",
        "published_date": "2014-08-13T17:37:43Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T04:03:51+00:00",
        "updated_at": "2025-01-30T04:03:54+00:00",
        "version": 2
      }
    },
    "interactions:2501.17161": {
      "data": {
        "paper_id": "2501.17161",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-29T07:56:19.027Z",
            "data": {
              "session_id": "session_1738137371913_r49ayoe",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-01-29T07:56:11.913Z",
              "end_time": "2025-01-29T07:56:18.049Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-29T07:56:20+00:00",
        "updated_at": "2025-01-31T07:13:32+00:00",
        "version": 3
      }
    },
    "interactions:2402.03300": {
      "data": {
        "paper_id": "2402.03300",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T07:03:45.367Z",
            "data": {
              "session_id": "session_1738220615286_qpb5gon",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-30T07:03:35.286Z",
              "end_time": "2025-01-30T07:03:43.522Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T07:03:15+00:00",
        "updated_at": "2025-01-30T07:04:33+00:00",
        "version": 6
      }
    },
    "paper:2402.03300": {
      "data": {
        "arxivId": "2402.03300",
        "url": "https://arxiv.org/abs/2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\n  Language Models",
        "authors": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo",
        "abstract": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "timestamp": "2025-01-30T07:03:06.031Z",
        "rating": "novote",
        "published_date": "2024-02-05T18:55:32Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T07:03:06+00:00",
        "updated_at": "2025-01-30T07:03:09+00:00",
        "version": 2
      }
    },
    "interactions:2501.16975": {
      "data": {
        "paper_id": "2501.16975",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-30T15:22:39.770Z",
            "data": {
              "session_id": "session_1738250547292_s1ro2p3",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-01-30T15:22:27.292Z",
              "end_time": "2025-01-30T15:22:38.952Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T08:59:36.097Z",
            "data": {
              "session_id": "session_1738832350626_mwxgt4c",
              "duration_seconds": 24,
              "idle_seconds": 0,
              "start_time": "2025-02-06T08:59:10.626Z",
              "end_time": "2025-02-06T08:59:34.186Z",
              "total_elapsed_seconds": 24
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-30T15:22:40+00:00",
        "updated_at": "2025-02-06T09:00:52+00:00",
        "version": 9
      }
    },
    "paper:2501.16975": {
      "data": {
        "arxivId": "2501.16975",
        "url": "https://arxiv.org/pdf/2501.16975",
        "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
        "authors": "Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou",
        "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.",
        "timestamp": "2025-01-30T15:06:14.633Z",
        "rating": "thumbsup",
        "published_date": "2025-01-28T14:15:42Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-30T15:06:15+00:00",
        "updated_at": "2025-02-06T09:00:42+00:00",
        "version": 3
      }
    },
    "interactions:2306.16830": {
      "data": {
        "paper_id": "2306.16830",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T07:03:36.613Z",
            "data": {
              "session_id": "session_1738306999389_go48zjh",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-01-31T07:03:19.389Z",
              "end_time": "2025-01-31T07:03:35.907Z",
              "total_elapsed_seconds": 17
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T07:40:20.356Z",
            "data": {
              "session_id": "session_1738309214420_3w8tty1",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-01-31T07:40:14.420Z",
              "end_time": "2025-01-31T07:40:18.795Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T07:03:37+00:00",
        "updated_at": "2025-01-31T07:41:37+00:00",
        "version": 5
      }
    },
    "paper:2306.16830": {
      "data": {
        "arxivId": "2306.16830",
        "url": "https://arxiv.org/abs/2306.16830",
        "title": "Sampling weights of deep neural networks",
        "authors": "Erik Lien Bolager, Iryna Burak, Chinmay Datar, Qing Sun, Felix Dietrich",
        "abstract": "We introduce a probability distribution, combined with an efficient sampling\nalgorithm, for weights and biases of fully-connected neural networks. In a\nsupervised learning context, no iterative optimization or gradient computations\nof internal network parameters are needed to obtain a trained network. The\nsampling is based on the idea of random feature models. However, instead of a\ndata-agnostic distribution, e.g., a normal distribution, we use both the input\nand the output training data to sample shallow and deep networks. We prove that\nsampled networks are universal approximators. For Barron functions, we show\nthat the $L^2$-approximation error of sampled shallow networks decreases with\nthe square root of the number of neurons. Our sampling scheme is invariant to\nrigid body transformations and scaling of the input data, which implies many\npopular pre-processing techniques are not required. In numerical experiments,\nwe demonstrate that sampled networks achieve accuracy comparable to iteratively\ntrained ones, but can be constructed orders of magnitude faster. Our test cases\ninvolve a classification benchmark from OpenML, sampling of neural operators to\nrepresent maps in function spaces, and transfer learning using well-known\narchitectures.",
        "timestamp": "2025-01-31T07:03:19.679Z",
        "rating": "novote",
        "published_date": "2023-06-29T10:13:36Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.NA",
          "math.NA",
          "68T07",
          "G.1; G.3"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T07:03:20+00:00",
        "updated_at": "2025-01-31T07:03:22+00:00",
        "version": 2
      }
    },
    "interactions:1812.06162": {
      "data": {
        "paper_id": "1812.06162",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T08:42:52.205Z",
            "data": {
              "session_id": "session_1738312949259_7aaf3du",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-01-31T08:42:29.259Z",
              "end_time": "2025-01-31T08:42:51.841Z",
              "total_elapsed_seconds": 23
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T08:42:53+00:00",
        "updated_at": "2025-01-31T08:44:12+00:00",
        "version": 3
      }
    },
    "paper:1812.06162": {
      "data": {
        "arxivId": "1812.06162",
        "url": "https://arxiv.org/abs/1812.06162",
        "title": "An Empirical Model of Large-Batch Training",
        "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, OpenAI Dota Team",
        "abstract": "In an increasing number of domains it has been demonstrated that deep\nlearning models can be trained using relatively large batch sizes without\nsacrificing data efficiency. However the limits of this massive data\nparallelism seem to differ from domain to domain, ranging from batches of tens\nof thousands in ImageNet to batches of millions in RL agents that play the game\nDota 2. To our knowledge there is limited conceptual understanding of why these\nlimits to batch size differ or how we might choose the correct batch size in a\nnew domain. In this paper, we demonstrate that a simple and easy-to-measure\nstatistic called the gradient noise scale predicts the largest useful batch\nsize across many domains and applications, including a number of supervised\nlearning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),\nreinforcement learning domains (Atari and Dota), and even generative model\ntraining (autoencoders on SVHN). We find that the noise scale increases as the\nloss decreases over a training run and depends on the model size primarily\nthrough improved model performance. Our empirically-motivated theory also\ndescribes the tradeoff between compute-efficiency and time-efficiency, and\nprovides a rough model of the benefits of adaptive batch-size training.",
        "timestamp": "2025-01-31T08:42:27.593Z",
        "rating": "novote",
        "published_date": "2018-12-14T20:49:09Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T08:42:28+00:00",
        "updated_at": "2025-01-31T08:42:30+00:00",
        "version": 2
      }
    },
    "interactions:2501.06623": {
      "data": {
        "paper_id": "2501.06623",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T20:41:40.302Z",
            "data": {
              "session_id": "session_1738356091698_7mfriql",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-01-31T20:41:31.698Z",
              "end_time": "2025-01-31T20:41:39.585Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "rating",
            "timestamp": "2025-01-31T21:33:40.774Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T20:41:41+00:00",
        "updated_at": "2025-01-31T21:35:02+00:00",
        "version": 4
      }
    },
    "paper:2501.06623": {
      "data": {
        "arxivId": "2501.06623",
        "url": "https://arxiv.org/html/2501.06623v1#S1",
        "title": "Nuclear Explosions for Large Scale Carbon Sequestration",
        "authors": "Andrew Haverly",
        "abstract": "Confronting the escalating threat of climate change requires innovative and\nlarge-scale interventions. This paper presents a bold proposal to employ a\nburied nuclear explosion in a remote basaltic seabed for pulverizing basalt,\nthereby accelerating carbon sequestration through Enhanced Rock Weathering\n(ERW). By precisely locating the explosion beneath the seabed, we aim to\nconfine debris, radiation, and energy while ensuring rapid rock weathering at a\nscale substantial enough to make a meaningful dent in atmospheric carbon\nlevels. Our analysis outlines the parameters essential for efficient carbon\ncapture and minimal collateral effects, emphasizing that a yield on the order\nof gigatons is critical for global climate impact. Although this approach may\nappear radical, we illustrate its feasibility by examining safety factors,\npreservation of local ecosystems, political considerations, and financial\nviability. This work argues for reimagining nuclear technology not merely as a\ndestructive force but as a potential catalyst for decarbonization, thereby\ninviting further exploration of pioneering solutions in the fight against\nclimate change.",
        "timestamp": "2025-01-31T20:41:31.947Z",
        "rating": "thumbsdown",
        "published_date": "2025-01-11T19:18:00Z",
        "arxiv_tags": [
          "physics.soc-ph",
          "physics.ao-ph"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T20:41:32+00:00",
        "updated_at": "2025-01-31T21:34:55+00:00",
        "version": 3
      }
    },
    "paper:2403.01903": {
      "data": {
        "arxivId": "2403.01903",
        "url": "https://arxiv.org/abs/2403.01903",
        "title": "Online Locality Meets Distributed Quantum Computing",
        "authors": "Amirreza Akbari, Xavier Coiteux-Roy, Francesco d'Amore, Fran\u00e7ois Le Gall, Henrik Lievonen, Darya Melnyk, Augusto Modanese, Shreyas Pai, Marc-Olivier Renou, V\u00e1clav Rozho\u0148, Jukka Suomela",
        "abstract": "We connect three distinct lines of research that have recently explored\nextensions of the classical LOCAL model of distributed computing: A.\ndistributed quantum computing and non-signaling distributions [e.g. STOC 2024],\nB. finitely-dependent processes [e.g. Forum Math. Pi 2016], and C. locality in\nonline graph algorithms and dynamic graph algorithms [e.g. ICALP 2023].\n  We prove new results on the capabilities and limitations of all of these\nmodels of computing, for locally checkable labeling problems (LCLs). We show\nthat all these settings can be sandwiched between the classical LOCAL model and\nwhat we call the randomized online-LOCAL model. Our work implies limitations on\nthe quantum advantage in the distributed setting, and we also exhibit a new\nbarrier for proving tighter bounds. Our main technical results are these: 1.\nAll LCL problems solvable with locality $O(\\log^\\star n)$ in the classical\ndeterministic LOCAL model admit a finitely-dependent distribution with locality\n$O(1)$. This answers an open question by Holroyd [2024], and also presents a\nnew barrier for proving bounds on distributed quantum advantage using\ncausality-based arguments. 2. In rooted trees, if we can solve an LCL problem\nwith locality $o(\\log \\log \\log n)$ in the randomized online-LOCAL model (or\nany of the weaker models, such as quantum-LOCAL), we can solve it with locality\n$O(\\log^\\star n)$ in the classical deterministic LOCAL model. One of many\nimplications is that in rooted trees, $O(\\log^\\star n)$ locality in\nquantum-LOCAL is not stronger than $O(\\log^\\star n)$ locality in classical\nLOCAL.",
        "timestamp": "2025-01-31T23:33:29.641Z",
        "rating": "novote",
        "published_date": "2024-03-04T10:03:54Z",
        "arxiv_tags": [
          "cs.DC",
          "cs.CC",
          "math.PR",
          "quant-ph"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:33:30+00:00",
        "updated_at": "2025-01-31T23:33:32+00:00",
        "version": 2
      }
    },
    "interactions:2501.18388": {
      "data": {
        "paper_id": "2501.18388",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-01-31T23:32:08.310Z",
            "data": {
              "session_id": "session_1738366300545_194kvwn",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-01-31T23:31:40.545Z",
              "end_time": "2025-01-31T23:32:02.688Z",
              "total_elapsed_seconds": 22
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-01T16:07:53.437Z",
            "data": {
              "session_id": "session_1738426062214_dsnwkus",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-01T16:07:42.214Z",
              "end_time": "2025-02-01T16:07:53.069Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:32:04+00:00",
        "updated_at": "2025-02-01T16:09:10+00:00",
        "version": 6
      }
    },
    "paper:2501.18388": {
      "data": {
        "arxivId": "2501.18388",
        "url": "https://arxiv.org/abs/2501.18388",
        "title": "Improved Replicable Boosting with Majority-of-Majorities",
        "authors": "Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen",
        "abstract": "We introduce a new replicable boosting algorithm which significantly improves\nthe sample complexity compared to previous algorithms. The algorithm works by\ndoing two layers of majority voting, using an improved version of the\nreplicable boosting algorithm introduced by Impagliazzo et al. [2022] in the\nbottom layer.",
        "timestamp": "2025-01-31T23:31:40.811Z",
        "rating": "novote",
        "published_date": "2025-01-30T14:38:26Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-01-31T23:31:41+00:00",
        "updated_at": "2025-01-31T23:31:43+00:00",
        "version": 2
      }
    },
    "interactions:2403.01903": {
      "data": {
        "paper_id": "2403.01903",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-01T16:07:35.998Z",
            "data": {
              "session_id": "session_1738426050514_mh2t2xk",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-01T16:07:30.514Z",
              "end_time": "2025-02-01T16:07:35.205Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-01T16:07:36+00:00",
        "updated_at": "2025-02-01T16:09:10+00:00",
        "version": 3
      }
    },
    "paper:2409.02668": {
      "data": {
        "arxivId": "2409.02668",
        "url": "https://arxiv.org/pdf/2409.02668",
        "title": "Introduction to Machine Learning",
        "authors": "Laurent Younes",
        "abstract": "This book introduces the mathematical foundations and techniques that lead to\nthe development and analysis of many of the algorithms that are used in machine\nlearning. It starts with an introductory chapter that describes notation used\nthroughout the book and serve at a reminder of basic concepts in calculus,\nlinear algebra and probability and also introduces some measure theoretic\nterminology, which can be used as a reading guide for the sections that use\nthese tools. The introductory chapters also provide background material on\nmatrix analysis and optimization. The latter chapter provides theoretical\nsupport to many algorithms that are used in the book, including stochastic\ngradient descent, proximal methods, etc. After discussing basic concepts for\nstatistical prediction, the book includes an introduction to reproducing kernel\ntheory and Hilbert space techniques, which are used in many places, before\naddressing the description of various algorithms for supervised statistical\nlearning, including linear methods, support vector machines, decision trees,\nboosting, or neural networks. The subject then switches to generative methods,\nstarting with a chapter that presents sampling methods and an introduction to\nthe theory of Markov chains. The following chapter describe the theory of\ngraphical models, an introduction to variational methods for models with latent\nvariables, and to deep-learning based generative models. The next chapters\nfocus on unsupervised learning methods, for clustering, factor analysis and\nmanifold learning. The final chapter of the book is theory-oriented and\ndiscusses concentration inequalities and generalization bounds.",
        "timestamp": "2025-02-01T04:23:41.461Z",
        "rating": "novote",
        "published_date": "2024-09-04T12:51:41Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-01T04:23:41+00:00",
        "updated_at": "2025-02-01T04:23:44+00:00",
        "version": 2
      }
    },
    "interactions:2006.15191": {
      "data": {
        "paper_id": "2006.15191",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-04T20:04:11.468Z",
            "data": {
              "session_id": "session_1738699428367_anpe33x",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-04T20:03:48.367Z",
              "end_time": "2025-02-04T20:04:10.685Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-04T20:04:12+00:00",
        "updated_at": "2025-02-04T20:05:33+00:00",
        "version": 3
      }
    },
    "paper:2006.15191": {
      "data": {
        "arxivId": "2006.15191",
        "url": "https://arxiv.org/abs/2006.15191",
        "title": "Is SGD a Bayesian sampler? Well, almost",
        "authors": "Chris Mingard, Guillermo Valle-P\u00e9rez, Joar Skalse, Ard A. Louis",
        "abstract": "Overparameterised deep neural networks (DNNs) are highly expressive and so\ncan, in principle, generate almost any function that fits a training dataset\nwith zero error. The vast majority of these functions will perform poorly on\nunseen data, and yet in practice DNNs often generalise remarkably well. This\nsuccess suggests that a trained DNN must have a strong inductive bias towards\nfunctions with low generalisation error. Here we empirically investigate this\ninductive bias by calculating, for a range of architectures and datasets, the\nprobability $P_{SGD}(f\\mid S)$ that an overparameterised DNN, trained with\nstochastic gradient descent (SGD) or one of its variants, converges on a\nfunction $f$ consistent with a training set $S$. We also use Gaussian processes\nto estimate the Bayesian posterior probability $P_B(f\\mid S)$ that the DNN\nexpresses $f$ upon random sampling of its parameters, conditioned on $S$.\n  Our main findings are that $P_{SGD}(f\\mid S)$ correlates remarkably well with\n$P_B(f\\mid S)$ and that $P_B(f\\mid S)$ is strongly biased towards low-error and\nlow complexity functions. These results imply that strong inductive bias in the\nparameter-function map (which determines $P_B(f\\mid S)$), rather than a special\nproperty of SGD, is the primary explanation for why DNNs generalise so well in\nthe overparameterised regime.\n  While our results suggest that the Bayesian posterior $P_B(f\\mid S)$ is the\nfirst order determinant of $P_{SGD}(f\\mid S)$, there remain second order\ndifferences that are sensitive to hyperparameter tuning. A function probability\npicture, based on $P_{SGD}(f\\mid S)$ and/or $P_B(f\\mid S)$, can shed new light\non the way that variations in architecture or hyperparameter settings such as\nbatch size, learning rate, and optimiser choice, affect DNN performance.",
        "timestamp": "2025-02-04T20:03:48.168Z",
        "rating": "novote",
        "published_date": "2020-06-26T19:45:36Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T20:03:48+00:00",
        "updated_at": "2025-02-04T20:03:52+00:00",
        "version": 2
      }
    },
    "interactions:2501.18812": {
      "data": {
        "paper_id": "2501.18812",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-04T20:03:43+00:00",
        "updated_at": "2025-02-04T20:03:46+00:00",
        "version": 2
      }
    },
    "paper:2501.18812": {
      "data": {
        "arxivId": "2501.18812",
        "url": "https://arxiv.org/abs/2501.18812",
        "title": "Estimating the Probability of Sampling a Trained Neural Network at\n  Random",
        "authors": "Adam Scherlis, Nora Belrose",
        "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian\nor uniform prior, of a region in neural network parameter space corresponding\nto a particular behavior, such as achieving test loss below some threshold.\nWhen the prior is uniform, this problem is equivalent to measuring the volume\nof a region. We show empirically and theoretically that existing algorithms for\nestimating volumes in parameter space underestimate the true volume by millions\nof orders of magnitude. We find that this error can be dramatically reduced,\nbut not entirely eliminated, with an importance sampling method using gradient\ninformation that is already provided by popular optimizers. The negative\nlogarithm of this probability can be interpreted as a measure of a network's\ninformation content, in accordance with minimum description length (MDL)\nprinciples and rate-distortion theory. As expected, this quantity increases\nduring language model training. We also find that badly-generalizing behavioral\nregions are smaller, and therefore less likely to be sampled at random,\ndemonstrating an inductive bias towards well-generalizing functions.",
        "timestamp": "2025-02-04T08:55:49.988Z",
        "rating": "novote",
        "published_date": "2025-01-31T00:16:06Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T08:55:50+00:00",
        "updated_at": "2025-02-04T08:55:54+00:00",
        "version": 2
      }
    },
    "paper:2410.04265": {
      "data": {
        "arxivId": "2410.04265",
        "url": "https://arxiv.org/abs/2410.04265",
        "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text",
        "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi",
        "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.",
        "timestamp": "2025-02-03T05:17:00.864Z",
        "rating": "novote",
        "published_date": "2024-10-05T18:55:01Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-03T05:17:01+00:00",
        "updated_at": "2025-02-03T05:17:04+00:00",
        "version": 2
      }
    },
    "interactions:2502.01061": {
      "data": {
        "paper_id": "2502.01061",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-04T21:31:19.948Z",
            "data": {
              "session_id": "session_1738704657885_3uvh86g",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-04T21:30:57.885Z",
              "end_time": "2025-02-04T21:31:19.936Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-04T21:30:52+00:00",
        "updated_at": "2025-02-04T21:32:40+00:00",
        "version": 5
      }
    },
    "paper:2502.01061": {
      "data": {
        "arxivId": "2502.01061",
        "url": "https://arxiv.org/abs/2502.01061",
        "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models",
        "authors": "Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang",
        "abstract": "End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)",
        "timestamp": "2025-02-04T21:29:46.589Z",
        "rating": "thumbsup",
        "published_date": "2025-02-03T05:17:32Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-04T21:29:47+00:00",
        "updated_at": "2025-02-04T21:32:31+00:00",
        "version": 3
      }
    },
    "paper:2502.02977": {
      "data": {
        "arxivId": "2502.02977",
        "url": "https://arxiv.org/abs/2502.02977",
        "title": "Disentangling CLIP Features for Enhanced Localized Understanding",
        "authors": "Samyak Rawelekar, Yujun Cai, Yiwei Wang, Ming-Hsuan Yang, Narendra Ahuja",
        "abstract": "Vision-language models (VLMs) demonstrate impressive capabilities in\ncoarse-grained tasks like image classification and retrieval. However, they\nstruggle with fine-grained tasks that require localized understanding. To\ninvestigate this weakness, we comprehensively analyze CLIP features and\nidentify an important issue: semantic features are highly correlated.\nSpecifically, the features of a class encode information about other classes,\nwhich we call mutual feature information (MFI). This mutual information becomes\nevident when we query a specific class and unrelated objects are activated\nalong with the target class. To address this issue, we propose Unmix-CLIP, a\nnovel framework designed to reduce MFI and improve feature disentanglement. We\nintroduce MFI loss, which explicitly separates text features by projecting them\ninto a space where inter-class similarity is minimized. To ensure a\ncorresponding separation in image features, we use multi-label recognition\n(MLR) to align the image features with the separated text features. This\nensures that both image and text features are disentangled and aligned across\nmodalities, improving feature separation for downstream tasks. For the COCO- 14\ndataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its\neffectiveness through extensive evaluations of MLR and zeroshot semantic\nsegmentation (ZS3). In MLR, our method performs competitively on the VOC2007\nand surpasses SOTA approaches on the COCO-14 dataset, using fewer training\nparameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3\nmethods on COCO and VOC",
        "timestamp": "2025-02-06T07:43:42.785Z",
        "rating": "novote",
        "published_date": "2025-02-05T08:20:31Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:43:43+00:00",
        "updated_at": "2025-02-06T07:43:45+00:00",
        "version": 2
      }
    },
    "interactions:2402.10588": {
      "data": {
        "paper_id": "2402.10588",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T07:42:03.414Z",
            "data": {
              "session_id": "session_1738827719049_iorjaob",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-06T07:41:59.049Z",
              "end_time": "2025-02-06T07:42:02.609Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:42:04+00:00",
        "updated_at": "2025-02-06T07:43:40+00:00",
        "version": 3
      }
    },
    "paper:2402.10588": {
      "data": {
        "arxivId": "2402.10588",
        "url": "https://arxiv.org/abs/2402.10588",
        "title": "Do Llamas Work in English? On the Latent Language of Multilingual\n  Transformers",
        "authors": "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West",
        "abstract": "We ask whether multilingual language models trained on unbalanced,\nEnglish-dominated corpora use English as an internal pivot language -- a\nquestion of key importance for understanding how language models function and\nthe origins of linguistic bias. Focusing on the Llama-2 family of transformer\nmodels, our study uses carefully constructed non-English prompts with a unique\ncorrect single-token continuation. From layer to layer, transformers gradually\nmap an input embedding of the final prompt token to an output embedding from\nwhich next-token probabilities are computed. Tracking intermediate embeddings\nthrough their high-dimensional space reveals three distinct phases, whereby\nintermediate embeddings (1) start far away from output token embeddings; (2)\nalready allow for decoding a semantically correct next token in the middle\nlayers, but give higher probability to its version in English than in the input\nlanguage; (3) finally move into an input-language-specific region of the\nembedding space. We cast these results into a conceptual model where the three\nphases operate in \"input space\", \"concept space\", and \"output space\",\nrespectively. Crucially, our evidence suggests that the abstract \"concept\nspace\" lies closer to English than to other languages, which may have important\nconsequences regarding the biases held by multilingual language models.",
        "timestamp": "2025-02-06T07:41:59.002Z",
        "rating": "novote",
        "published_date": "2024-02-16T11:21:28Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.CY"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:41:59+00:00",
        "updated_at": "2025-02-06T07:42:02+00:00",
        "version": 2
      }
    },
    "paper:2501.06346": {
      "data": {
        "arxivId": "2501.06346",
        "url": "https://arxiv.org/abs/2501.06346",
        "title": "Large Language Models Share Representations of Latent Grammatical\n  Concepts Across Typologically Diverse Languages",
        "authors": "Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller",
        "abstract": "Human bilinguals often use similar brain regions to process multiple\nlanguages, depending on when they learned their second language and their\nproficiency. In large language models (LLMs), how are multiple languages\nlearned and encoded? In this work, we explore the extent to which LLMs share\nrepresentations of morphosyntactic concepts such as grammatical number, gender,\nand tense across languages. We train sparse autoencoders on Llama-3-8B and\nAya-23-8B, and demonstrate that abstract grammatical concepts are often encoded\nin feature directions shared across many languages. We use causal interventions\nto verify the multilingual nature of these representations; specifically, we\nshow that ablating only multilingual features decreases classifier performance\nto near-chance across languages. We then use these features to precisely modify\nmodel behavior in a machine translation task; this demonstrates both the\ngenerality and selectivity of these feature's roles in the network. Our\nfindings suggest that even models trained predominantly on English data can\ndevelop robust, cross-lingual abstractions of morphosyntactic concepts.",
        "timestamp": "2025-02-06T07:47:12.435Z",
        "rating": "novote",
        "published_date": "2025-01-10T21:18:21Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:47:12+00:00",
        "updated_at": "2025-02-06T07:47:15+00:00",
        "version": 2
      }
    },
    "interactions:2502.02977": {
      "data": {
        "paper_id": "2502.02977",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-06T07:44:00.913Z",
            "data": {
              "session_id": "session_1738827828800_rsr12xq",
              "duration_seconds": 10,
              "idle_seconds": 0,
              "start_time": "2025-02-06T07:43:48.800Z",
              "end_time": "2025-02-06T07:43:58.854Z",
              "total_elapsed_seconds": 10
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-06T07:44:02+00:00",
        "updated_at": "2025-02-06T07:45:21+00:00",
        "version": 3
      }
    },
    "paper:2502.01492": {
      "data": {
        "arxivId": "2502.01492",
        "url": "https://arxiv.org/abs/2502.01492",
        "title": "Develop AI Agents for System Engineering in Factorio",
        "authors": "Neel Kant",
        "abstract": "Continuing advances in frontier model research are paving the way for\nwidespread deployment of AI agents. Meanwhile, global interest in building\nlarge, complex systems in software, manufacturing, energy and logistics has\nnever been greater. Although AI driven system engineering holds tremendous\npromise, the static benchmarks dominating agent evaluations today fail to\ncapture the crucial skills required for implementing dynamic systems, such as\nmanaging uncertain trade-offs and ensuring proactive adaptability. This\nposition paper advocates for training and evaluating AI agents' system\nengineering abilities through automation-oriented sandbox games-particularly\nFactorio. By directing research efforts in this direction, we can equip AI\nagents with the specialized reasoning and long-horizon planning necessary to\ndesign, maintain, and optimize tomorrow's most demanding engineering projects.",
        "timestamp": "2025-02-06T08:39:54.750Z",
        "rating": "novote",
        "published_date": "2025-02-03T16:26:17Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-06T08:39:55+00:00",
        "updated_at": "2025-02-06T08:40:00+00:00",
        "version": 2
      }
    },
    "interactions:2502.02709": {
      "data": {
        "paper_id": "2502.02709",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T04:09:45.427Z",
            "data": {
              "session_id": "session_1738901374121_i1ll2hr",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-07T04:09:34.121Z",
              "end_time": "2025-02-07T04:09:44.712Z",
              "total_elapsed_seconds": 11
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T04:14:46.468Z",
            "data": {
              "session_id": "session_1738901657209_8pl2q1q",
              "duration_seconds": 29,
              "idle_seconds": 0,
              "start_time": "2025-02-07T04:14:17.209Z",
              "end_time": "2025-02-07T04:14:46.288Z",
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T04:09:46+00:00",
        "updated_at": "2025-02-07T04:16:11+00:00",
        "version": 4
      }
    },
    "paper:2502.02709": {
      "data": {
        "arxivId": "2502.02709",
        "url": "https://arxiv.org/abs/2502.02709",
        "title": "Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning\n  about Private Data Release",
        "authors": "Mark Bun, Marco Carmosino, Palak Jain, Gabriel Kaptchuk, Satchit Sivakumar",
        "abstract": "The technical literature about data privacy largely consists of two\ncomplementary approaches: formal definitions of conditions sufficient for\nprivacy preservation and attacks that demonstrate privacy breaches.\nDifferential privacy is an accepted standard in the former sphere. However,\ndifferential privacy's powerful adversarial model and worst-case guarantees may\nmake it too stringent in some situations, especially when achieving it comes at\na significant cost to data utility. Meanwhile, privacy attacks aim to expose\nreal and worrying privacy risks associated with existing data release processes\nbut often face criticism for being unrealistic. Moreover, the literature on\nattacks generally does not identify what properties are necessary to defend\nagainst them.\n  We address the gap between these approaches by introducing demographic\ncoherence, a condition inspired by privacy attacks that we argue is necessary\nfor data privacy. This condition captures privacy violations arising from\ninferences about individuals that are incoherent with respect to the\ndemographic patterns in the data. Our framework focuses on confidence rated\npredictors, which can in turn be distilled from almost any data-informed\nprocess. Thus, we capture privacy threats that exist even when no attack is\nexplicitly being carried out. Our framework not only provides a condition with\nrespect to which data release algorithms can be analysed but suggests natural\nexperimental evaluation methodologies that could be used to build practical\nintuition and make tangible assessment of risks. Finally, we argue that\ndemographic coherence is weaker than differential privacy: we prove that every\ndifferentially private data release is also demographically coherent, and that\nthere are demographically coherent algorithms which are not differentially\nprivate.",
        "timestamp": "2025-02-07T04:09:34.426Z",
        "rating": "novote",
        "published_date": "2025-02-04T20:42:30Z",
        "arxiv_tags": [
          "cs.CR",
          "cs.DB"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T04:09:34+00:00",
        "updated_at": "2025-02-07T04:09:37+00:00",
        "version": 2
      }
    },
    "interactions:2502.03349": {
      "data": {
        "paper_id": "2502.03349",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T08:00:03.949Z",
            "data": {
              "session_id": "session_1738915195032_5dkxa7n",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-07T07:59:55.032Z",
              "end_time": "2025-02-07T08:00:03.403Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T07:59:49+00:00",
        "updated_at": "2025-02-07T08:02:28+00:00",
        "version": 5
      }
    },
    "paper:2502.03349": {
      "data": {
        "arxivId": "2502.03349",
        "url": "https://arxiv.org/abs/2502.03349",
        "title": "Robust Autonomy Emerges from Self-Play",
        "authors": "Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun",
        "abstract": "Self-play has powered breakthroughs in two-player and multi-player games.\nHere we show that self-play is a surprisingly effective strategy in another\ndomain. We show that robust and naturalistic driving emerges entirely from\nself-play in simulation at unprecedented scale -- 1.6~billion~km of driving.\nThis is enabled by Gigaflow, a batched simulator that can synthesize and train\non 42 years of subjective driving experience per hour on a single 8-GPU node.\nThe resulting policy achieves state-of-the-art performance on three independent\nautonomous driving benchmarks. The policy outperforms the prior state of the\nart when tested on recorded real-world scenarios, amidst human drivers, without\never seeing human data during training. The policy is realistic when assessed\nagainst human references and achieves unprecedented robustness, averaging 17.5\nyears of continuous driving between incidents in simulation.",
        "timestamp": "2025-02-07T07:59:39.082Z",
        "rating": "novote",
        "published_date": "2025-02-05T16:41:05Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.RO"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T07:59:39+00:00",
        "updated_at": "2025-02-07T07:59:42+00:00",
        "version": 2
      }
    },
    "interactions:2501.18838": {
      "data": {
        "paper_id": "2501.18838",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T22:50:33.830Z",
            "data": {
              "session_id": "session_1738968628917_q3vgya7",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-07T22:50:28.917Z",
              "end_time": "2025-02-07T22:50:33.139Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:34+00:00",
        "updated_at": "2025-02-07T22:51:52+00:00",
        "version": 3
      }
    },
    "paper:2501.18838": {
      "data": {
        "arxivId": "2501.18838",
        "url": "https://arxiv.org/abs/2501.18838",
        "title": "Partially Rewriting a Transformer in Natural Language",
        "authors": "Gon\u00e7alo Paulo, Nora Belrose",
        "abstract": "The greatest ambition of mechanistic interpretability is to completely\nrewrite deep neural networks in a format that is more amenable to human\nunderstanding, while preserving their behavior and performance. In this paper,\nwe attempt to partially rewrite a large language model using simple natural\nlanguage explanations. We first approximate one of the feedforward networks in\nthe LLM with a wider MLP with sparsely activating neurons - a transcoder - and\nuse an automated interpretability pipeline to generate explanations for these\nneurons. We then replace the first layer of this sparse MLP with an LLM-based\nsimulator, which predicts the activation of each neuron given its explanation\nand the surrounding context. Finally, we measure the degree to which these\nmodifications distort the model's final output. With our pipeline, the model's\nincrease in loss is statistically similar to entirely replacing the sparse MLP\noutput with the zero vector. We employ the same protocol, this time using a\nsparse autoencoder, on the residual stream of the same layer and obtain similar\nresults. These results suggest that more detailed explanations are needed to\nimprove performance substantially above the zero ablation baseline.",
        "timestamp": "2025-02-07T22:50:28.974Z",
        "rating": "novote",
        "published_date": "2025-01-31T01:12:50Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:29+00:00",
        "updated_at": "2025-02-07T22:50:32+00:00",
        "version": 2
      }
    },
    "paper:2501.18823": {
      "data": {
        "arxivId": "2501.18823",
        "url": "https://arxiv.org/abs/2501.18823",
        "title": "Transcoders Beat Sparse Autoencoders for Interpretability",
        "authors": "Gon\u00e7alo Paulo, Stepan Shabalin, Nora Belrose",
        "abstract": "Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose _skip transcoders_, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.",
        "timestamp": "2025-02-07T22:50:23.541Z",
        "rating": "novote",
        "published_date": "2025-01-31T00:36:30Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:24+00:00",
        "updated_at": "2025-02-07T22:50:26+00:00",
        "version": 2
      }
    },
    "interactions:2410.13928": {
      "data": {
        "paper_id": "2410.13928",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-07T22:50:11+00:00",
        "updated_at": "2025-02-07T22:50:14+00:00",
        "version": 2
      }
    },
    "paper:2410.13928": {
      "data": {
        "arxivId": "2410.13928",
        "url": "https://arxiv.org/pdf/2410.13928",
        "title": "Automatically Interpreting Millions of Features in Large Language Models",
        "authors": "Gon\u00e7alo Paulo, Alex Mallen, Caden Juang, Nora Belrose",
        "abstract": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
        "timestamp": "2025-02-07T22:50:05.304Z",
        "rating": "novote",
        "published_date": "2024-10-17T17:56:01Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:50:05+00:00",
        "updated_at": "2025-02-07T22:50:08+00:00",
        "version": 2
      }
    },
    "interactions:2501.18823": {
      "data": {
        "paper_id": "2501.18823",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-07T22:53:56.344Z",
            "data": {
              "session_id": "session_1738968815007_8i1mqev",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-02-07T22:53:35.007Z",
              "end_time": "2025-02-07T22:53:55.278Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-07T22:53:29+00:00",
        "updated_at": "2025-02-07T22:54:56+00:00",
        "version": 5
      }
    },
    "interactions:2501.19393": {
      "data": {
        "paper_id": "2501.19393",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-08T04:47:43.248Z",
            "data": {
              "session_id": "session_1738990053546_il4qqnv",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-02-08T04:47:33.546Z",
              "end_time": "2025-02-08T04:47:42.530Z",
              "total_elapsed_seconds": 9
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-08T05:00:21.860Z",
            "data": {
              "session_id": "session_1738990808219_yilx5tj",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-08T05:00:08.219Z",
              "end_time": "2025-02-08T05:00:21.298Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-08T04:47:44+00:00",
        "updated_at": "2025-02-08T05:01:37+00:00",
        "version": 5
      }
    },
    "paper:2501.19393": {
      "data": {
        "arxivId": "2501.19393",
        "url": "https://arxiv.org/pdf/2501.19393",
        "title": "s1: Simple test-time scaling",
        "authors": "Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, Tatsunori Hashimoto",
        "abstract": "Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1",
        "timestamp": "2025-02-08T04:47:33.827Z",
        "rating": "novote",
        "published_date": "2025-01-31T18:48:08Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T04:47:34+00:00",
        "updated_at": "2025-02-08T04:47:37+00:00",
        "version": 2
      }
    },
    "interactions:2103.03874": {
      "data": {
        "paper_id": "2103.03874",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-08T06:28:08.436Z",
            "data": {
              "session_id": "session_1738996075059_2exga4a",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-08T06:27:55.059Z",
              "end_time": "2025-02-08T06:28:01.026Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-08T06:28:02+00:00",
        "updated_at": "2025-02-08T06:29:30+00:00",
        "version": 4
      }
    },
    "paper:2103.03874": {
      "data": {
        "arxivId": "2103.03874",
        "url": "https://arxiv.org/pdf/2103.03874",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
        "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt",
        "abstract": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.",
        "timestamp": "2025-02-08T06:21:47.033Z",
        "rating": "novote",
        "published_date": "2021-03-05T18:59:39Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T06:21:47+00:00",
        "updated_at": "2025-02-08T06:21:50+00:00",
        "version": 2
      }
    },
    "interactions:2407.15908": {
      "data": {
        "paper_id": "2407.15908",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-08T19:32:26.818Z",
            "data": {
              "rating": "thumbsdown"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-08T19:43:11.258Z",
            "data": {
              "session_id": "session_1739043785860_3t6atdq",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-08T19:43:05.860Z",
              "end_time": "2025-02-08T19:43:08.933Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-08T19:32:27+00:00",
        "updated_at": "2025-02-08T19:44:26+00:00",
        "version": 11
      }
    },
    "paper:2407.15908": {
      "data": {
        "arxivId": "2407.15908",
        "url": "https://arxiv.org/pdf/2407.15908",
        "title": "The Genomic Code: The genome instantiates a generative model of the\n  organism",
        "authors": "Kevin J. Mitchell, Nick Cheney",
        "abstract": "How does the genome encode the form of the organism? What is the nature of\nthis genomic code? Inspired by recent work in machine learning and\nneuroscience, we propose that the genome encodes a generative model of the\norganism. In this scheme, by analogy with variational autoencoders, the genome\ncomprises a connectionist network, embodying a compressed space of latent\nvariables, with weights that get encoded by the learning algorithm of evolution\nand decoded through the processes of development. The generative model analogy\naccounts for the complex, distributed genetic architecture of most traits and\nthe emergent robustness and evolvability of developmental processes, while also\noffering a conception that lends itself to formalisation.",
        "timestamp": "2025-02-08T19:30:55.727Z",
        "rating": "thumbsdown",
        "published_date": "2024-07-22T16:41:25Z",
        "arxiv_tags": [
          "q-bio.OT"
        ]
      },
      "meta": {
        "created_at": "2025-02-08T19:30:56+00:00",
        "updated_at": "2025-02-08T20:34:50+00:00",
        "version": 3
      }
    },
    "interactions:2312.11805": {
      "data": {
        "paper_id": "2312.11805",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:46:11.720Z",
            "data": {
              "session_id": "session_1739123167032_1siawe1",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:46:07.032Z",
              "end_time": "2025-02-09T17:46:10.495Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:56:00.613Z",
            "data": {
              "session_id": "session_1739123753660_xdyeohs",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:55:53.660Z",
              "end_time": "2025-02-09T17:55:56.929Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:46:13+00:00",
        "updated_at": "2025-02-09T17:57:14+00:00",
        "version": 7
      }
    },
    "paper:2312.11805": {
      "data": {
        "arxivId": "2312.11805",
        "url": "https://arxiv.org/abs/2312.11805",
        "title": "Gemini: A Family of Highly Capable Multimodal Models",
        "authors": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian G\u00fcra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, \u00c1goston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, I\u00f1aki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u0144ska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz K\u0119pa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton \u00c4lgmyr, Timoth\u00e9e Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, Fran\u00e7ois-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin B\u00f6lle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah \u00d3 Donnaile, S\u00e9bastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccol\u00f2 Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor \u00c4hdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bra\u017einskas, Andrei Sozanschi, Matthew Hayes, H\u00e9ctor Fern\u00e1ndez Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante K\u00e4rrman, Pawe\u0142 Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, R\u00e9mi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim P\u00f5der, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, V\u00edt List\u00edk, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul M\u00fcller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, Oriol Vinyals",
        "abstract": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI.",
        "timestamp": "2025-02-09T17:46:07.089Z",
        "rating": "novote",
        "published_date": "2023-12-19T02:39:27Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:46:07+00:00",
        "updated_at": "2025-02-09T17:46:10+00:00",
        "version": 2
      }
    },
    "paper:2502.03387": {
      "data": {
        "arxivId": "2502.03387",
        "url": "https://arxiv.org/abs/2502.03387",
        "title": "LIMO: Less is More for Reasoning",
        "authors": "Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu",
        "abstract": "We present a fundamental discovery that challenges our understanding of how\ncomplex reasoning emerges in large language models. While conventional wisdom\nsuggests that sophisticated reasoning tasks demand extensive training data\n(&gt;100,000 examples), we demonstrate that complex mathematical reasoning\nabilities can be effectively elicited with surprisingly few examples. Through\ncomprehensive experiments, our proposed model LIMO demonstrates unprecedented\nperformance in mathematical reasoning. With merely 817 curated training\nsamples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\nprevious SFT-based models' 6.5% and 59.2% respectively, while only using 1% of\nthe training data required by previous approaches. LIMO demonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\nmore data, challenging the notion that SFT leads to memorization rather than\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\nHypothesis (LIMO Hypothesis): In foundation models where domain knowledge has\nbeen comprehensively encoded during pre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated\ndemonstrations of cognitive processes. This hypothesis posits that the\nelicitation threshold for complex reasoning is determined by two key factors:\n(1) the completeness of the model's encoded knowledge foundation during\npre-training, and (2) the effectiveness of post-training examples as \"cognitive\ntemplates\" that show the model how to utilize its knowledge base to solve\ncomplex reasoning tasks. To facilitate reproducibility and future research in\ndata-efficient reasoning, we release LIMO as a comprehensive open-source suite\nat https://github.com/GAIR-NLP/LIMO.",
        "timestamp": "2025-02-09T17:35:03.289Z",
        "rating": "thumbsup",
        "published_date": "2025-02-05T17:23:45Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:35:03+00:00",
        "updated_at": "2025-02-09T18:35:57+00:00",
        "version": 3
      }
    },
    "interactions:2502.03387": {
      "data": {
        "paper_id": "2502.03387",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T17:56:07.669Z",
            "data": {
              "session_id": "session_1739123763127_cpwbu2t",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-09T17:56:03.128Z",
              "end_time": "2025-02-09T17:56:07.099Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-09T18:35:03.804Z",
            "data": {
              "session_id": "session_1739126087743_f1kabpl",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-09T18:34:47.743Z",
              "end_time": "2025-02-09T18:35:03.790Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-09T17:56:08+00:00",
        "updated_at": "2025-02-09T18:35:59+00:00",
        "version": 6
      }
    },
    "interactions:2502.05171": {
      "data": {
        "paper_id": "2502.05171",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-10T05:27:27.890Z",
            "data": {
              "session_id": "session_1739165241055_vo46266",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-10T05:27:21.055Z",
              "end_time": "2025-02-10T05:27:27.199Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-10T05:34:17.187Z",
            "data": {
              "session_id": "session_1739165650576_6d87r9x",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-10T05:34:10.576Z",
              "end_time": "2025-02-10T05:34:15.079Z",
              "total_elapsed_seconds": 5
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:45:05.743Z",
            "data": {
              "session_id": "session_1739659501371_tsme0ej",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:45:01.371Z",
              "end_time": "2025-02-15T22:45:05.725Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:48:44.570Z",
            "data": {
              "session_id": "session_1739659713366_ye05fgo",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:48:33.366Z",
              "end_time": "2025-02-15T22:48:44.152Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:27:28+00:00",
        "updated_at": "2025-02-15T22:49:41+00:00",
        "version": 10
      }
    },
    "paper:2502.05171": {
      "data": {
        "arxivId": "2502.05171",
        "url": "https://arxiv.org/abs/2502.05171",
        "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach",
        "authors": "Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein",
        "abstract": "We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.",
        "timestamp": "2025-02-10T05:27:21.173Z",
        "rating": "novote",
        "published_date": "2025-02-07T18:55:02Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:27:21+00:00",
        "updated_at": "2025-02-10T05:27:24+00:00",
        "version": 2
      }
    },
    "interactions:2502.04223": {
      "data": {
        "paper_id": "2502.04223",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-10T05:44:03+00:00",
        "updated_at": "2025-02-10T05:44:06+00:00",
        "version": 2
      }
    },
    "paper:2501.17887": {
      "data": {
        "arxivId": "2501.17887",
        "url": "https://arxiv.org/html/2501.17887v1",
        "title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document\n  Conversion",
        "authors": "Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar",
        "abstract": "We introduce Docling, an easy-to-use, self-contained, MIT-licensed,\nopen-source toolkit for document conversion, that can parse several types of\npopular document formats into a unified, richly structured representation. It\nis powered by state-of-the-art specialized AI models for layout analysis\n(DocLayNet) and table structure recognition (TableFormer), and runs efficiently\non commodity hardware in a small resource budget. Docling is released as a\nPython package and can be used as a Python API or as a CLI tool. Docling's\nmodular architecture and efficient document representation make it easy to\nimplement extensions, new features, models, and customizations. Docling has\nbeen already integrated in other popular open-source frameworks (e.g.,\nLangChain, LlamaIndex, spaCy), making it a natural fit for the processing of\ndocuments and the development of high-end applications. The open-source\ncommunity has fully engaged in using, promoting, and developing for Docling,\nwhich gathered 10k stars on GitHub in less than a month and was reported as the\nNo. 1 trending repository in GitHub worldwide in November 2024.",
        "timestamp": "2025-02-10T05:44:02.960Z",
        "rating": "novote",
        "published_date": "2025-01-27T19:40:00Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.CV",
          "cs.SE"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:44:03+00:00",
        "updated_at": "2025-02-10T05:44:06+00:00",
        "version": 2
      }
    },
    "paper:2502.04223": {
      "data": {
        "arxivId": "2502.04223",
        "url": "https://arxiv.org/html/2502.04223v1",
        "title": "\u00c9clair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
        "authors": "Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Sepp\u00e4nen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra",
        "abstract": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards.",
        "timestamp": "2025-02-10T05:43:48.654Z",
        "rating": "novote",
        "published_date": "2025-02-06T17:07:22Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-10T05:43:49+00:00",
        "updated_at": "2025-02-10T05:43:51+00:00",
        "version": 2
      }
    },
    "interactions:2502.04403": {
      "data": {
        "paper_id": "2502.04403",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-11T05:00:56.839Z",
            "data": {
              "session_id": "session_1739250052349_awy9hqs",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-11T05:00:52.350Z",
              "end_time": "2025-02-11T05:00:56.005Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-11T19:22:34.431Z",
            "data": {
              "session_id": "session_1739301748269_qhfa65o",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-11T19:22:28.269Z",
              "end_time": "2025-02-11T19:22:31.649Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-11T05:00:57+00:00",
        "updated_at": "2025-02-11T19:23:57+00:00",
        "version": 6
      }
    },
    "paper:2502.04403": {
      "data": {
        "arxivId": "2502.04403",
        "url": "https://arxiv.org/abs/2502.04403",
        "title": "Agency Is Frame-Dependent",
        "authors": "David Abel, Andr\u00e9 Barreto, Michael Bowling, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh",
        "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "timestamp": "2025-02-11T04:59:59.126Z",
        "rating": "novote",
        "published_date": "2025-02-06T08:34:57Z",
        "arxiv_tags": [
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-11T04:59:59+00:00",
        "updated_at": "2025-02-11T05:00:02+00:00",
        "version": 2
      }
    },
    "interactions:2502.04549": {
      "data": {
        "paper_id": "2502.04549",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-11T16:18:14.719Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-11T16:18:17+00:00",
        "updated_at": "2025-02-11T16:19:44+00:00",
        "version": 4
      }
    },
    "paper:2502.04549": {
      "data": {
        "arxivId": "2502.04549",
        "url": "https://arxiv.org/abs/2502.04549",
        "title": "Mechanisms of Projective Composition of Diffusion Models",
        "authors": "Arwen Bradley, Preetum Nakkiran, David Berthelot, James Thornton, Joshua M. Susskind",
        "abstract": "We study the theoretical foundations of composition in diffusion models, with\na particular focus on out-of-distribution extrapolation and\nlength-generalization. Prior work has shown that composing distributions via\nlinear score combination can achieve promising results, including\nlength-generalization in some cases (Du et al., 2023; Liu et al., 2022).\nHowever, our theoretical understanding of how and why such compositions work\nremains incomplete. In fact, it is not even entirely clear what it means for\ncomposition to \"work\". This paper starts to address these fundamental gaps. We\nbegin by precisely defining one possible desired result of composition, which\nwe call projective composition. Then, we investigate: (1) when linear score\ncombinations provably achieve projective composition, (2) whether\nreverse-diffusion sampling can generate the desired composition, and (3) the\nconditions under which composition fails. Finally, we connect our theoretical\nanalysis to prior empirical observations where composition has either worked or\nfailed, for reasons that were unclear at the time.",
        "timestamp": "2025-02-11T14:48:47.054Z",
        "rating": "novote",
        "published_date": "2025-02-06T22:59:54Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-11T14:48:47+00:00",
        "updated_at": "2025-02-11T14:49:19+00:00",
        "version": 2
      }
    },
    "interactions:2412.12140": {
      "data": {
        "paper_id": "2412.12140",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-12T02:04:24.065Z",
            "data": {
              "rating": "thumbsdown"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-12T02:04:24+00:00",
        "updated_at": "2025-02-12T02:05:49+00:00",
        "version": 4
      }
    },
    "paper:2412.12140": {
      "data": {
        "arxivId": "2412.12140",
        "url": "https://arxiv.org/abs/2412.12140",
        "title": "Frontier AI systems have surpassed the self-replicating red line",
        "authors": "Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang",
        "abstract": "Successful self-replication under no human assistance is the essential step\nfor AI to outsmart the human beings, and is an early signal for rogue AIs. That\nis why self-replication is widely recognized as one of the few red line risks\nof frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google\nevaluate their flagship large language models GPT-o1 and Gemini Pro 1.0, and\nreport the lowest risk level of self-replication. However, following their\nmethodology, we for the first time discover that two AI systems driven by\nMeta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large\nlanguage models of less parameters and weaker capabilities, have already\nsurpassed the self-replicating red line. In 50% and 90% experimental trials,\nthey succeed in creating a live and separate copy of itself respectively. By\nanalyzing the behavioral traces, we observe the AI systems under evaluation\nalready exhibit sufficient self-perception, situational awareness and\nproblem-solving capabilities to accomplish self-replication. We further note\nthe AI systems are even able to use the capability of self-replication to avoid\nshutdown and create a chain of replica to enhance the survivability, which may\nfinally lead to an uncontrolled population of AIs. If such a worst-case risk is\nlet unknown to the human society, we would eventually lose control over the\nfrontier AI systems: They would take control over more computing devices, form\nan AI species and collude with each other against human beings. Our findings\nare a timely alert on existing yet previously unknown severe AI risks, calling\nfor international collaboration on effective governance on uncontrolled\nself-replication of AI systems.",
        "timestamp": "2025-02-12T02:03:35.172Z",
        "rating": "novote",
        "published_date": "2024-12-09T15:01:37Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-12T02:03:35+00:00",
        "updated_at": "2025-02-12T02:03:38+00:00",
        "version": 2
      }
    },
    "interactions:2212.01508": {
      "data": {
        "paper_id": "2212.01508",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T01:38:48.536Z",
            "data": {
              "session_id": "session_1739410716599_oskjy7y",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-13T01:38:36.599Z",
              "end_time": "2025-02-13T01:38:48.532Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T01:37:57+00:00",
        "updated_at": "2025-02-13T01:39:18+00:00",
        "version": 7
      }
    },
    "paper:2212.01508": {
      "data": {
        "arxivId": "2212.01508",
        "url": "https://arxiv.org/abs/2212.01508",
        "title": "Space is a latent sequence: Structured sequence learning as a unified\n  theory of representation in the hippocampus",
        "authors": "Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Guangyao Zhou, Miguel L\u00e1zaro-Gredilla, Dileep George",
        "abstract": "Fascinating and puzzling phenomena, such as landmark vector cells, splitter\ncells, and event-specific representations to name a few, are regularly\ndiscovered in the hippocampus. Without a unifying principle that can explain\nthese divergent observations, each experiment seemingly discovers a new anomaly\nor coding type. Here, we provide a unifying principle that the mental\nrepresentation of space is an emergent property of latent higher-order sequence\nlearning. Treating space as a sequence resolves myriad phenomena, and suggests\nthat the place-field mapping methodology where sequential neuron responses are\ninterpreted in spatial and Euclidean terms might itself be a source of\nanomalies. Our model, called Clone-structured Causal Graph (CSCG), uses a\nspecific higher-order graph scaffolding to learn latent representations by\nmapping sensory inputs to unique contexts. Learning to compress sequential and\nepisodic experiences using CSCGs result in the emergence of cognitive maps -\nmental representations of spatial and conceptual relationships in an\nenvironment that are suited for planning, introspection, consolidation, and\nabstraction. We demonstrate that over a dozen different hippocampal phenomena,\nranging from those reported in classic experiments to the most recent ones, are\nsuccinctly and mechanistically explained by our model.",
        "timestamp": "2025-02-13T01:37:45.603Z",
        "rating": "novote",
        "published_date": "2022-12-03T02:00:56Z",
        "arxiv_tags": [
          "q-bio.NC"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T01:37:46+00:00",
        "updated_at": "2025-02-13T01:37:49+00:00",
        "version": 2
      }
    },
    "interactions:2502.08346": {
      "data": {
        "paper_id": "2502.08346",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T05:08:27.058Z",
            "data": {
              "session_id": "session_1739423278468_u80mie3",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-02-13T05:07:58.469Z",
              "end_time": "2025-02-13T05:08:26.250Z",
              "total_elapsed_seconds": 28
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:08:27+00:00",
        "updated_at": "2025-02-13T05:09:52+00:00",
        "version": 3
      }
    },
    "paper:2502.08346": {
      "data": {
        "arxivId": "2502.08346",
        "url": "https://arxiv.org/abs/2502.08346",
        "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
        "authors": "Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi",
        "abstract": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
        "timestamp": "2025-02-13T05:07:58.727Z",
        "rating": "novote",
        "published_date": "2025-02-12T12:13:51Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:07:59+00:00",
        "updated_at": "2025-02-13T05:08:01+00:00",
        "version": 2
      }
    },
    "paper:2502.08254": {
      "data": {
        "arxivId": "2502.08254",
        "url": "https://arxiv.org/abs/2502.08254",
        "title": "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "authors": "Maximilian Jaritz, Matthieu Guillaumin, Sabine Sternig, Loris Bazzani",
        "abstract": "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.",
        "timestamp": "2025-02-13T05:05:23.454Z",
        "rating": "novote",
        "published_date": "2025-02-12T09:49:43Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:05:23+00:00",
        "updated_at": "2025-02-13T05:05:27+00:00",
        "version": 2
      }
    },
    "paper:2502.07971": {
      "data": {
        "arxivId": "2502.07971",
        "url": "https://arxiv.org/abs/2502.07971",
        "title": "ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval",
        "authors": "Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi",
        "abstract": "Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.",
        "timestamp": "2025-02-13T05:02:43.036Z",
        "rating": "novote",
        "published_date": "2025-02-11T21:35:13Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.AI",
          "cs.LG",
          "I.2; I.7; E.2; H.3"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:02:43+00:00",
        "updated_at": "2025-02-13T05:02:46+00:00",
        "version": 2
      }
    },
    "interactions:2406.08636": {
      "data": {
        "paper_id": "2406.08636",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T05:23:06.946Z",
            "data": {
              "session_id": "session_1739424163032_acsxzxz",
              "duration_seconds": 22,
              "idle_seconds": 0,
              "start_time": "2025-02-13T05:22:43.032Z",
              "end_time": "2025-02-13T05:23:05.145Z",
              "total_elapsed_seconds": 22
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:22:36+00:00",
        "updated_at": "2025-02-13T05:24:00+00:00",
        "version": 5
      }
    },
    "paper:2406.08636": {
      "data": {
        "arxivId": "2406.08636",
        "url": "https://arxiv.org/abs/2406.08636",
        "title": "Towards Integrating Personal Knowledge into Test-Time Predictions",
        "authors": "Isaac Lage, Sonali Parbhoo, Finale Doshi-Velez",
        "abstract": "Machine learning (ML) models can make decisions based on large amounts of\ndata, but they can be missing personal knowledge available to human users about\nwhom predictions are made. For example, a model trained to predict psychiatric\noutcomes may know nothing about a patient's social support system, and social\nsupport may look different for different patients. In this work, we introduce\nthe problem of human feature integration, which provides a way to incorporate\nimportant personal-knowledge from users without domain expertise into ML\npredictions. We characterize this problem through illustrative user stories and\ncomparisons to existing approaches; we formally describe this problem in a way\nthat paves the ground for future technical solutions; and we provide a\nproof-of-concept study of a simple version of a solution to this problem in a\nsemi-realistic setting.",
        "timestamp": "2025-02-13T05:22:05.668Z",
        "rating": "novote",
        "published_date": "2024-06-12T20:47:17Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T05:22:06+00:00",
        "updated_at": "2025-02-13T05:22:09+00:00",
        "version": 2
      }
    },
    "interactions:2004.15011": {
      "data": {
        "paper_id": "2004.15011",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T21:19:16.095Z",
            "data": {
              "session_id": "session_1739481551573_jcayq9y",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-13T21:19:11.573Z",
              "end_time": "2025-02-13T21:19:15.305Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-13T21:30:48.996Z",
            "data": {
              "session_id": "session_1739482245306_c5yigpd",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-13T21:30:45.306Z",
              "end_time": "2025-02-13T21:30:48.396Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-13T21:19:17+00:00",
        "updated_at": "2025-02-13T21:31:46+00:00",
        "version": 4
      }
    },
    "paper:2004.15011": {
      "data": {
        "arxivId": "2004.15011",
        "url": "https://arxiv.org/abs/2004.15011",
        "title": "TLDR: Extreme Summarization of Scientific Documents",
        "authors": "Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld",
        "abstract": "We introduce TLDR generation, a new form of extreme summarization, for\nscientific papers. TLDR generation involves high source compression and\nrequires expert background knowledge and understanding of complex\ndomain-specific language. To facilitate study on this task, we introduce\nSciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR\ncontains both author-written and expert-derived TLDRs, where the latter are\ncollected using a novel annotation protocol that produces high-quality\nsummaries while minimizing annotation burden. We propose CATTS, a simple yet\neffective learning strategy for generating TLDRs that exploits titles as an\nauxiliary training signal. CATTS improves upon strong baselines under both\nautomated metrics and human evaluations. Data and code are publicly available\nat https://github.com/allenai/scitldr.",
        "timestamp": "2025-02-13T21:19:11.579Z",
        "rating": "novote",
        "published_date": "2020-04-30T17:56:18Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-13T21:19:12+00:00",
        "updated_at": "2025-02-13T21:19:15+00:00",
        "version": 2
      }
    },
    "interactions:2502.08606": {
      "data": {
        "paper_id": "2502.08606",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-14T06:33:16.469Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T06:52:26.851Z",
            "data": {
              "session_id": "session_1739515935950_9mu832l",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-14T06:52:15.950Z",
              "end_time": "2025-02-14T06:52:23.445Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-14T06:33:17+00:00",
        "updated_at": "2025-02-14T06:53:22+00:00",
        "version": 5
      }
    },
    "paper:2502.08606": {
      "data": {
        "arxivId": "2502.08606",
        "url": "https://arxiv.org/abs/2502.08606",
        "title": "Distillation Scaling Laws",
        "authors": "Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb",
        "abstract": "We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.",
        "timestamp": "2025-02-14T06:32:27.363Z",
        "rating": "novote",
        "published_date": "2025-02-12T17:52:47Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-14T06:32:27+00:00",
        "updated_at": "2025-02-14T06:32:30+00:00",
        "version": 2
      }
    },
    "interactions:2211.00235": {
      "data": {
        "paper_id": "2211.00235",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T23:46:25.734Z",
            "data": {
              "session_id": "session_1739576772297_lkni4f4",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-14T23:46:12.297Z",
              "end_time": "2025-02-14T23:46:24.995Z",
              "total_elapsed_seconds": 13
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-14T23:55:53.303Z",
            "data": {
              "session_id": "session_1739577336192_am2nzt2",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-14T23:55:36.192Z",
              "end_time": "2025-02-14T23:55:50.665Z",
              "total_elapsed_seconds": 14
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T07:51:31.680Z",
            "data": {
              "session_id": "session_1739605887438_47xuwdv",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T07:51:27.438Z",
              "end_time": "2025-02-15T07:51:31.041Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:47:04.541Z",
            "data": {
              "session_id": "session_1739670417662_f9g7zdj",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:46:57.662Z",
              "end_time": "2025-02-16T01:47:03.922Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-14T23:46:26+00:00",
        "updated_at": "2025-02-16T01:47:55+00:00",
        "version": 10
      }
    },
    "paper:2211.00235": {
      "data": {
        "arxivId": "2211.00235",
        "url": "https://arxiv.org/pdf/2211.00235",
        "title": "Efficient AlphaFold2 Training using Parallel Evoformer and Branch\n  Parallelism",
        "authors": "Guoxia Wang, Zhihua Wu, Xiaomin Fang, Yingfei Xiang, Yiqun Liu, Dianhai Yu, Yanjun Ma",
        "abstract": "The accuracy of AlphaFold2, a frontier end-to-end structure prediction\nsystem, is already close to that of the experimental determination techniques.\nDue to the complex model architecture and large memory consumption, it requires\nlots of computational resources and time to train AlphaFold2 from scratch.\nEfficient AlphaFold2 training could accelerate the development of life science.\nIn this paper, we propose a Parallel Evoformer and Branch Parallelism to speed\nup the training of AlphaFold2. We conduct sufficient experiments on UniFold\nimplemented in PyTorch and HelixFold implemented in PaddlePaddle, and Branch\nParallelism can improve the training performance by 38.67% and 36.93%,\nrespectively. We also demonstrate that the accuracy of Parallel Evoformer could\nbe on par with AlphaFold2 on the CASP14 and CAMEO datasets. The source code is\navailable on https://github.com/PaddlePaddle/PaddleFleetX",
        "timestamp": "2025-02-14T23:44:19.874Z",
        "rating": "novote",
        "published_date": "2022-11-01T02:59:35Z",
        "arxiv_tags": [
          "cs.DC"
        ]
      },
      "meta": {
        "created_at": "2025-02-14T23:44:20+00:00",
        "updated_at": "2025-02-14T23:44:23+00:00",
        "version": 2
      }
    },
    "interactions:2410.10485": {
      "data": {
        "paper_id": "2410.10485",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T08:27:09.127Z",
            "data": {
              "session_id": "session_1739608025217_stferaa",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-15T08:27:05.217Z",
              "end_time": "2025-02-15T08:27:09.111Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:18:34.667Z",
            "data": {
              "session_id": "session_1739632697223_4jqcpvj",
              "duration_seconds": 17,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:18:17.223Z",
              "end_time": "2025-02-15T15:18:34.114Z",
              "total_elapsed_seconds": 17
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:26:21+00:00",
        "updated_at": "2025-02-15T15:19:29+00:00",
        "version": 7
      }
    },
    "paper:2410.10485": {
      "data": {
        "arxivId": "2410.10485",
        "url": "https://arxiv.org/abs/2410.10485",
        "title": "Characterising high-order interdependence via entropic conjugation",
        "authors": "Fernando E. Rosas, Aaron Gutknecht, Pedro A. M. Mediano, Michael Gastpar",
        "abstract": "High-order phenomena play crucial roles in many systems of interest, but\ntheir analysis is often highly nontrivial. There is a rich literature providing\na number of alternative information-theoretic quantities capturing high-order\nphenomena, but their interpretation and relationship with each other is not\nwell understood. The lack of principles unifying these quantities obscures the\nchoice of tools for enabling specific type of analyses. Here we show how an\nentropic conjugation provides a theoretically grounded principle to investigate\nthe space of possible high-order quantities, clarifying the nature of the\nexistent metrics while revealing gaps in the literature. This leads to identify\nnovel notions of symmetry and skew-symmetry as key properties for guaranteeing\na balanced account of high-order interdependencies and enabling broadly\napplicable analyses across physical systems.",
        "timestamp": "2025-02-15T08:26:04.568Z",
        "rating": "novote",
        "published_date": "2024-10-14T13:27:08Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT",
          "physics.data-an"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:26:05+00:00",
        "updated_at": "2025-02-15T08:26:07+00:00",
        "version": 2
      }
    },
    "paper:2502.08009": {
      "data": {
        "arxivId": "2502.08009",
        "url": "https://arxiv.org/abs/2502.08009",
        "title": "The Geometry of Prompting: Unveiling Distinct Mechanisms of Task\n  Adaptation in Language Models",
        "authors": "Artem Kirsanov, Chi-Ning Chou, Kyunghyun Cho, SueYeon Chung",
        "abstract": "Decoder-only language models have the ability to dynamically switch between\nvarious computational tasks based on input prompts. Despite many successful\napplications of prompting, there is very limited understanding of the internal\nmechanism behind such flexibility. In this work, we investigate how different\nprompting methods affect the geometry of representations in these models.\nEmploying a framework grounded in statistical physics, we reveal that various\nprompting techniques, while achieving similar performance, operate through\ndistinct representational mechanisms for task adaptation. Our analysis\nhighlights the critical role of input distribution samples and label semantics\nin few-shot in-context learning. We also demonstrate evidence of synergistic\nand interfering interactions between different tasks on the representational\nlevel. Our work contributes to the theoretical understanding of large language\nmodels and lays the groundwork for developing more effective,\nrepresentation-aware prompting strategies.",
        "timestamp": "2025-02-15T08:18:37.599Z",
        "rating": "novote",
        "published_date": "2025-02-11T23:09:50Z",
        "arxiv_tags": [
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T08:18:38+00:00",
        "updated_at": "2025-02-15T08:18:40+00:00",
        "version": 2
      }
    },
    "interactions:2502.08009": {
      "data": {
        "paper_id": "2502.08009",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:03:42.226Z",
            "data": {
              "session_id": "session_1739631818140_9jdyeuo",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:03:38.140Z",
              "end_time": "2025-02-15T15:03:41.505Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T16:28:19.123Z",
            "data": {
              "session_id": "session_1739636895140_brw7czk",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-15T16:28:15.140Z",
              "end_time": "2025-02-15T16:28:18.529Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:03:42+00:00",
        "updated_at": "2025-02-15T16:29:11+00:00",
        "version": 4
      }
    },
    "interactions:2309.08003": {
      "data": {
        "paper_id": "2309.08003",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-15T15:20:15.661Z",
            "data": {
              "rating": "thumbsup"
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:47.662Z",
            "data": {
              "session_id": "session_1739670332759_2xyrnfw",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:32.759Z",
              "end_time": "2025-02-16T01:45:46.408Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:56+00:00",
        "updated_at": "2025-02-16T01:46:34+00:00",
        "version": 8
      }
    },
    "paper:2309.08003": {
      "data": {
        "arxivId": "2309.08003",
        "url": "https://arxiv.org/html/2309.08003v2",
        "title": "Generalized Decomposition of Multivariate Information",
        "authors": "Thomas F. Varley",
        "abstract": "Since its introduction, the partial information decomposition (PID) has\nemerged as a powerful, information-theoretic technique useful for studying the\nstructure of (potentially higher-order) interactions in complex systems.\nDespite its utility, the applicability of the PID is restricted by the need to\nassign elements as either inputs or targets, as well as the specific structure\nof the mutual information itself. Here, we introduce a generalized information\ndecomposition that relaxes the source/target distinction while still satisfying\nthe basic intuitions about information. This approach is based on the\ndecomposition of the Kullback-Leibler divergence, and consequently allows for\nthe analysis of any information gained when updating from an arbitrary prior to\nan arbitrary posterior. Consequently, any information-theoretic measure that\ncan be written in as a Kullback-Leibler divergence admits a decomposition in\nthe style of Williams and Beer, including the total correlation, the\nnegentropy, and the mutual information as special cases. In this paper, we\nexplore how the generalized information decomposition can reveal novel insights\ninto existing measures, as well as the nature of higher-order synergies. We\nshow that synergistic information is intimately related to the well-known\nTononi-Sporns-Edelman (TSE) complexity, and that synergistic information\nrequires a similar integration/segregation balance as a high TSE complexity.\nFinally, we end with a discussion of how this approach fits into other attempts\nto generalize the PID and the possibilities for empirical applications.",
        "timestamp": "2025-02-15T15:19:51.046Z",
        "rating": "thumbsup",
        "published_date": "2023-09-14T19:26:46Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:51+00:00",
        "updated_at": "2025-02-16T01:46:25+00:00",
        "version": 4
      }
    },
    "interactions:2401.14347": {
      "data": {
        "paper_id": "2401.14347",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T15:19:40.221Z",
            "data": {
              "session_id": "session_1739632751864_8hpxkz3",
              "duration_seconds": 28,
              "idle_seconds": 0,
              "start_time": "2025-02-15T15:19:11.864Z",
              "end_time": "2025-02-15T15:19:39.553Z",
              "total_elapsed_seconds": 28
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:30.800Z",
            "data": {
              "session_id": "session_1739670326057_0a9i18x",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:26.058Z",
              "end_time": "2025-02-16T01:45:29.087Z",
              "total_elapsed_seconds": 3
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:40+00:00",
        "updated_at": "2025-02-16T01:46:30+00:00",
        "version": 6
      }
    },
    "paper:2401.14347": {
      "data": {
        "arxivId": "2401.14347",
        "url": "https://arxiv.org/abs/2401.14347",
        "title": "Evolving higher-order synergies reveals a trade-off between stability\n  and information integration capacity in complex systems",
        "authors": "Thomas F. Varley, Joshua Bongard",
        "abstract": "There has recently been an explosion of interest in how \"higher-order\"\nstructures emerge in complex systems. This \"emergent\" organization has been\nfound in a variety of natural and artificial systems, although at present the\nfield lacks a unified understanding of what the consequences of higher-order\nsynergies and redundancies are for systems. Typical research treat the presence\n(or absence) of synergistic information as a dependent variable and report\nchanges in the level of synergy in response to some change in the system. Here,\nwe attempt to flip the script: rather than treating higher-order information as\na dependent variable, we use evolutionary optimization to evolve boolean\nnetworks with significant higher-order redundancies, synergies, or statistical\ncomplexity. We then analyse these evolved populations of networks using\nestablished tools for characterizing discrete dynamics: the number of\nattractors, average transient length, and Derrida coefficient. We also assess\nthe capacity of the systems to integrate information. We find that high-synergy\nsystems are unstable and chaotic, but with a high capacity to integrate\ninformation. In contrast, evolved redundant systems are extremely stable, but\nhave negligible capacity to integrate information. Finally, the complex systems\nthat balance integration and segregation (known as Tononi-Sporns-Edelman\ncomplexity) show features of both chaosticity and stability, with a greater\ncapacity to integrate information than the redundant systems while being more\nstable than the random and synergistic systems. We conclude that there may be a\nfundamental trade-off between the robustness of a systems dynamics and its\ncapacity to integrate information (which inherently requires flexibility and\nsensitivity), and that certain kinds of complexity naturally balance this\ntrade-off.",
        "timestamp": "2025-02-15T15:19:11.977Z",
        "rating": "thumbsup",
        "published_date": "2024-01-25T17:48:11Z",
        "arxiv_tags": [
          "cs.IT",
          "math.DS",
          "math.IT",
          "nlin.CD",
          "nlin.CG"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:12+00:00",
        "updated_at": "2025-02-16T01:46:15+00:00",
        "version": 3
      }
    },
    "paper:0909.2120": {
      "data": {
        "arxivId": "0909.2120",
        "url": "https://arxiv.org/abs/0909.2120",
        "title": "Approximate maximizers of intricacy functionals",
        "authors": "Jerome Buzzi, Lorenzo Zambotti",
        "abstract": "G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the\nneural complexity of a family of random variables. This functional is a special\ncase of intricacy, i.e., an average of the mutual information of subsystems\nwhose weights have good mathematical properties. Moreover, its maximum value\ngrows at a definite speed with the size of the system.\n  In this work, we compute exactly this speed of growth by building\n\"approximate maximizers\" subject to an entropy condition. These approximate\nmaximizers work simultaneously for all intricacies. We also establish some\nproperties of arbitrary approximate maximizers, in particular the existence of\na threshold in the size of subsystems of approximate maximizers: most smaller\nsubsystems are almost equidistributed, most larger subsystems determine the\nfull system.\n  The main ideas are a random construction of almost maximizers with a high\nstatistical symmetry and the consideration of entropy profiles, i.e., the\naverage entropies of sub-systems of a given size. The latter gives rise to\ninteresting questions of probability and information theory.",
        "timestamp": "2025-02-15T15:19:09.854Z",
        "rating": "novote",
        "published_date": "2009-09-11T09:33:01Z",
        "arxiv_tags": [
          "math.PR",
          "94A17; 92B30; 60C05"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T15:19:10+00:00",
        "updated_at": "2025-02-15T15:19:12+00:00",
        "version": 2
      }
    },
    "paper:2405.21060": {
      "data": {
        "arxivId": "2405.21060",
        "url": "https://arxiv.org/abs/2405.21060",
        "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through Structured State Space Duality",
        "authors": "Tri Dao, Albert Gu",
        "abstract": "While Transformers have been the main architecture behind deep learning's\nsuccess in language modeling, state-space models (SSMs) such as Mamba have\nrecently been shown to match or outperform Transformers at small to medium\nscale. We show that these families of models are actually quite closely\nrelated, and develop a rich framework of theoretical connections between SSMs\nand variants of attention, connected through various decompositions of a\nwell-studied class of structured semiseparable matrices. Our state space\nduality (SSD) framework allows us to design a new architecture (Mamba-2) whose\ncore layer is an a refinement of Mamba's selective SSM that is 2-8X faster,\nwhile continuing to be competitive with Transformers on language modeling.",
        "timestamp": "2025-02-15T22:41:28.414Z",
        "rating": "novote",
        "published_date": "2024-05-31T17:50:01Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:41:28+00:00",
        "updated_at": "2025-02-15T22:41:31+00:00",
        "version": 2
      }
    },
    "paper:2212.14052": {
      "data": {
        "arxivId": "2212.14052",
        "url": "https://arxiv.org/abs/2212.14052",
        "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
        "authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9",
        "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.",
        "timestamp": "2025-02-15T22:40:30.551Z",
        "rating": "novote",
        "published_date": "2022-12-28T17:56:03Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:40:30+00:00",
        "updated_at": "2025-02-15T22:40:33+00:00",
        "version": 2
      }
    },
    "paper:2412.03603": {
      "data": {
        "arxivId": "2412.03603",
        "url": "https://arxiv.org/abs/2412.03603",
        "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
        "authors": "Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",
        "abstract": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
        "timestamp": "2025-02-15T22:38:36.193Z",
        "rating": "novote",
        "published_date": "2024-12-03T23:52:37Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:38:36+00:00",
        "updated_at": "2025-02-15T22:38:39+00:00",
        "version": 2
      }
    },
    "interactions:2405.21060": {
      "data": {
        "paper_id": "2405.21060",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:59:39.561Z",
            "data": {
              "session_id": "session_1739660375023_s75ff3w",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:59:35.023Z",
              "end_time": "2025-02-15T22:59:39.549Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:59:28+00:00",
        "updated_at": "2025-02-15T23:00:28+00:00",
        "version": 4
      }
    },
    "interactions:2212.14052": {
      "data": {
        "paper_id": "2212.14052",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T22:59:06.173Z",
            "data": {
              "session_id": "session_1739660330192_lx2jzq5",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-15T22:58:50.192Z",
              "end_time": "2025-02-15T22:59:05.829Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T22:59:06+00:00",
        "updated_at": "2025-02-15T22:59:57+00:00",
        "version": 3
      }
    },
    "interactions:2412.03603": {
      "data": {
        "paper_id": "2412.03603",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-15T23:04:15.327Z",
            "data": {
              "session_id": "session_1739660635355_h8p8dsr",
              "duration_seconds": 20,
              "idle_seconds": 0,
              "start_time": "2025-02-15T23:03:55.356Z",
              "end_time": "2025-02-15T23:04:14.978Z",
              "total_elapsed_seconds": 20
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-15T23:03:49+00:00",
        "updated_at": "2025-02-15T23:04:38+00:00",
        "version": 4
      }
    },
    "interactions:0909.2120": {
      "data": {
        "paper_id": "0909.2120",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-16T01:45:19.362Z",
            "data": {
              "session_id": "session_1739670313447_v222tn4",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-16T01:45:13.447Z",
              "end_time": "2025-02-16T01:45:18.696Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-16T01:45:20+00:00",
        "updated_at": "2025-02-16T01:46:11+00:00",
        "version": 3
      }
    },
    "interactions:2010.02331": {
      "data": {
        "paper_id": "2010.02331",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T06:45:10.118Z",
            "data": {
              "session_id": "session_1739774696587_4ovs8w7",
              "duration_seconds": 13,
              "idle_seconds": 0,
              "start_time": "2025-02-17T06:44:56.587Z",
              "end_time": "2025-02-17T06:45:09.389Z",
              "total_elapsed_seconds": 13
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T06:45:11+00:00",
        "updated_at": "2025-02-17T06:46:11+00:00",
        "version": 3
      }
    },
    "paper:2010.02331": {
      "data": {
        "arxivId": "2010.02331",
        "url": "https://arxiv.org/abs/2010.02331",
        "title": "How to send a real number using a single bit (and some shared\n  randomness)",
        "authors": "Ran Ben-Basat, Michael Mitzenmacher, Shay Vargaftik",
        "abstract": "We consider the fundamental problem of communicating an estimate of a real\nnumber $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value\n$X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the\nvalue of $X$. We consider both the biased and unbiased estimation problems and\naim to minimize the cost. For the biased case, the cost is the worst-case (over\nthe choice of $x$) expected squared error, which coincides with the variance if\nthe algorithm is required to be unbiased.\n  We first overview common biased and unbiased estimation approaches and prove\ntheir optimality when no shared randomness is allowed. We then show how a small\namount of shared randomness, which can be as low as a single bit, reduces the\ncost in both cases. Specifically, we derive lower bounds on the cost attainable\nby any algorithm with unrestricted use of shared randomness and propose\nnear-optimal solutions that use a small number of shared random bits. Finally,\nwe discuss open problems and future directions.",
        "timestamp": "2025-02-17T06:44:56.714Z",
        "rating": "novote",
        "published_date": "2020-10-05T20:52:06Z",
        "arxiv_tags": [
          "cs.DS",
          "cs.IT",
          "cs.LG",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T06:44:57+00:00",
        "updated_at": "2025-02-17T06:45:00+00:00",
        "version": 2
      }
    },
    "interactions:2502.10248": {
      "data": {
        "paper_id": "2502.10248",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T13:46:13.451Z",
            "data": {
              "session_id": "session_1739799953977_mra89it",
              "duration_seconds": 19,
              "idle_seconds": 0,
              "start_time": "2025-02-17T13:45:53.977Z",
              "end_time": "2025-02-17T13:46:12.706Z",
              "total_elapsed_seconds": 19
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T13:52:28.721Z",
            "data": {
              "session_id": "session_1739800341991_gb1k3u5",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-17T13:52:21.991Z",
              "end_time": "2025-02-17T13:52:27.992Z",
              "total_elapsed_seconds": 6
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T18:41:33.533Z",
            "data": {
              "session_id": "session_1739817689248_eesktlo",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-17T18:41:29.248Z",
              "end_time": "2025-02-17T18:41:33.341Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T13:46:14+00:00",
        "updated_at": "2025-02-17T18:42:26+00:00",
        "version": 5
      }
    },
    "paper:2502.10248": {
      "data": {
        "arxivId": "2502.10248",
        "url": "https://arxiv.org/abs/2502.10248",
        "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model",
        "authors": "Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang",
        "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.",
        "timestamp": "2025-02-17T13:45:52.287Z",
        "rating": "novote",
        "published_date": "2025-02-14T15:58:10Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T13:45:52+00:00",
        "updated_at": "2025-02-17T13:45:56+00:00",
        "version": 2
      }
    },
    "interactions:0711.1859": {
      "data": {
        "paper_id": "0711.1859",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-17T14:34:26+00:00",
        "updated_at": "2025-02-17T14:34:29+00:00",
        "version": 2
      }
    },
    "paper:0711.1859": {
      "data": {
        "arxivId": "0711.1859",
        "url": "https://arxiv.org/pdf/0711.1859",
        "title": "Doubles for monoidal categories",
        "authors": "Craig Pastro, Ross Street",
        "abstract": "In a recent paper, Daisuke Tambara defined two-sided actions on an endomodule\n(= endodistributor) of a monoidal V-category A. When A is autonomous (= rigid =\ncompact), he showed that the V-category (that we call Tamb(A)) of so-equipped\nendomodules (that we call Tambara modules) is equivalent to the monoidal centre\nZ[A,V] of the convolution monoidal V-category [A,V]. Our paper extends these\nideas somewhat. For general A, we construct a promonoidal V-category DA (which\nwe suggest should be called the double of A) with an equivalence [DA,V] \\simeq\nTamb(A). When A is closed, we define strong (respectively, left strong) Tambara\nmodules and show that these constitute a V-category Tamb_s(A) (respectively,\nTamb_{ls}(A)) which is equivalent to the centre (respectively, lax centre) of\n[A,V]. We construct localizations D_s A and D_{ls} A of DA such that there are\nequivalences Tamb_s(A) \\simeq [D_s A,V] and Tamb_{ls}(A) \\simeq [D_{ls} A,V].\nWhen A is autonomous, every Tambara module is strong; this implies an\nequivalence Z[A,V] \\simeq [DA,V].",
        "timestamp": "2025-02-17T14:33:04.178Z",
        "rating": "novote",
        "published_date": "2007-11-13T00:13:19Z",
        "arxiv_tags": [
          "math.CT"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T14:33:04+00:00",
        "updated_at": "2025-02-17T14:33:08+00:00",
        "version": 2
      }
    },
    "interactions:1805.09843": {
      "data": {
        "paper_id": "1805.09843",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-17T15:22:56+00:00",
        "updated_at": "2025-02-17T15:22:58+00:00",
        "version": 2
      }
    },
    "paper:1805.09843": {
      "data": {
        "arxivId": "1805.09843",
        "url": "https://arxiv.org/abs/1805.09843",
        "title": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and\n  Associated Pooling Mechanisms",
        "authors": "Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin",
        "abstract": "Many deep learning architectures have been proposed to model the\ncompositionality in text sequences, requiring a substantial number of\nparameters and expensive computations. However, there has not been a rigorous\nevaluation regarding the added value of sophisticated compositional functions.\nIn this paper, we conduct a point-by-point comparative study between Simple\nWord-Embedding-based Models (SWEMs), consisting of parameter-free pooling\noperations, relative to word-embedding-based RNN/CNN models. Surprisingly,\nSWEMs exhibit comparable or even superior performance in the majority of cases\nconsidered. Based upon this understanding, we propose two additional pooling\nstrategies over learned word embeddings: (i) a max-pooling operation for\nimproved interpretability; and (ii) a hierarchical pooling operation, which\npreserves spatial (n-gram) information within text sequences. We present\nexperiments on 17 datasets encompassing three tasks: (i) (long) document\nclassification; (ii) text sequence matching; and (iii) short text tasks,\nincluding classification and tagging. The source code and datasets can be\nobtained from https:// github.com/dinghanshen/SWEM.",
        "timestamp": "2025-02-17T15:22:36.514Z",
        "rating": "novote",
        "published_date": "2018-05-24T18:27:21Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T15:22:37+00:00",
        "updated_at": "2025-02-17T15:22:41+00:00",
        "version": 2
      }
    },
    "paper:2410.07590": {
      "data": {
        "arxivId": "2410.07590",
        "url": "https://arxiv.org/pdf/2410.07590",
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
        "authors": "Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, Yaohua Tang",
        "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
        "timestamp": "2025-02-17T15:04:32.964Z",
        "rating": "novote",
        "published_date": "2024-10-10T03:52:54Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.CL"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T15:04:33+00:00",
        "updated_at": "2025-02-17T15:04:37+00:00",
        "version": 2
      }
    },
    "interactions:1801.01715": {
      "data": {
        "paper_id": "1801.01715",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T16:31:56.736Z",
            "data": {
              "session_id": "session_1739809903606_cgzr7jj",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-17T16:31:43.606Z",
              "end_time": "2025-02-17T16:31:55.992Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-17T16:54:18.001Z",
            "data": {
              "session_id": "session_1739811246577_tcqhh11",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-17T16:54:06.577Z",
              "end_time": "2025-02-17T16:54:17.382Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-17T16:31:57+00:00",
        "updated_at": "2025-02-17T16:55:10+00:00",
        "version": 4
      }
    },
    "paper:1801.01715": {
      "data": {
        "arxivId": "1801.01715",
        "url": "https://arxiv.org/abs/1801.01715",
        "title": "Spectral Graph Forge: Graph Generation Targeting Modularity",
        "authors": "Luca Baldesi, Athina Markopoulou, Carter T. Butts",
        "abstract": "Community structure is an important property that captures inhomogeneities\ncommon in large networks, and modularity is one of the most widely used metrics\nfor such community structure. In this paper, we introduce a principled\nmethodology, the Spectral Graph Forge, for generating random graphs that\npreserves community structure from a real network of interest, in terms of\nmodularity. Our approach leverages the fact that the spectral structure of\nmatrix representations of a graph encodes global information about community\nstructure. The Spectral Graph Forge uses a low-rank approximation of the\nmodularity matrix to generate synthetic graphs that match a target modularity\nwithin user-selectable degree of accuracy, while allowing other aspects of\nstructure to vary. We show that the Spectral Graph Forge outperforms\nstate-of-the-art techniques in terms of accuracy in targeting the modularity\nand randomness of the realizations, while also preserving other local\nstructural properties and node attributes. We discuss extensions of the\nSpectral Graph Forge to target other properties beyond modularity, and its\napplications to anonymization.",
        "timestamp": "2025-02-17T16:31:42.498Z",
        "rating": "novote",
        "published_date": "2018-01-05T11:11:20Z",
        "arxiv_tags": [
          "cs.SI",
          "physics.soc-ph"
        ]
      },
      "meta": {
        "created_at": "2025-02-17T16:31:42+00:00",
        "updated_at": "2025-02-17T16:31:46+00:00",
        "version": 2
      }
    },
    "interactions:2502.12981": {
      "data": {
        "paper_id": "2502.12981",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T17:00:44.571Z",
            "data": {
              "session_id": "session_1739984421179_3vwhogj",
              "duration_seconds": 23,
              "idle_seconds": 0,
              "start_time": "2025-02-19T17:00:21.179Z",
              "end_time": "2025-02-19T17:00:43.759Z",
              "total_elapsed_seconds": 23
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T17:03:37.508Z",
            "data": {
              "session_id": "session_1739984605893_ezjx9z6",
              "duration_seconds": 9,
              "idle_seconds": 0,
              "start_time": "2025-02-19T17:03:25.893Z",
              "end_time": "2025-02-19T17:03:35.093Z",
              "total_elapsed_seconds": 9
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T17:00:45+00:00",
        "updated_at": "2025-02-19T17:04:15+00:00",
        "version": 7
      }
    },
    "paper:2502.12981": {
      "data": {
        "arxivId": "2502.12981",
        "url": "https://arxiv.org/abs/2502.12981",
        "title": "Towards Variational Flow Matching on General Geometries",
        "authors": "Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Erik J. Bekkers",
        "abstract": "We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an\nextension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian\ndistributions for generative modeling on structured manifolds. We derive a\nvariational objective for probability flows on manifolds with closed-form\ngeodesics, making RG-VFM comparable - though fundamentally different to\nRiemannian Flow Matching (RFM) in this geometric setting. Experiments on a\ncheckerboard dataset wrapped on the sphere demonstrate that RG-VFM captures\ngeometric structure more effectively than Euclidean VFM and baseline methods,\nestablishing it as a robust framework for manifold-aware generative modeling.",
        "timestamp": "2025-02-19T17:00:20.667Z",
        "rating": "novote",
        "published_date": "2025-02-18T16:02:10Z",
        "arxiv_tags": [
          "cs.LG",
          "math.DG"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T17:00:21+00:00",
        "updated_at": "2025-02-19T17:00:24+00:00",
        "version": 2
      }
    },
    "paper:2011.14522": {
      "data": {
        "arxivId": "2011.14522",
        "url": "https://arxiv.org/abs/2011.14522",
        "title": "Feature Learning in Infinite-Width Neural Networks",
        "authors": "Greg Yang, Edward J. Hu",
        "abstract": "As its width tends to infinity, a deep neural network's behavior under\ngradient descent can become simplified and predictable (e.g. given by the\nNeural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK\nparametrization). However, we show that the standard and NTK parametrizations\nof a neural network do not admit infinite-width limits that can learn features,\nwhich is crucial for pretraining and transfer learning such as with BERT. We\npropose simple modifications to the standard parametrization to allow for\nfeature learning in the limit. Using the *Tensor Programs* technique, we derive\nexplicit formulas for such limits. On Word2Vec and few-shot learning on\nOmniglot via MAML, two canonical tasks that rely crucially on feature learning,\nwe compute these limits exactly. We find that they outperform both NTK\nbaselines and finite-width networks, with the latter approaching the\ninfinite-width feature learning performance as width increases.\n  More generally, we classify a natural space of neural network\nparametrizations that generalizes standard, NTK, and Mean Field\nparametrizations. We show 1) any parametrization in this space either admits\nfeature learning or has an infinite-width training dynamics given by kernel\ngradient descent, but not both; 2) any such infinite-width limit can be\ncomputed using the Tensor Programs technique. Code for our experiments can be\nfound at github.com/edwardjhu/TP4.",
        "timestamp": "2025-02-19T07:54:41.193Z",
        "rating": "novote",
        "published_date": "2020-11-30T03:21:05Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:54:41+00:00",
        "updated_at": "2025-02-19T07:54:44+00:00",
        "version": 2
      }
    },
    "interactions:1712.08969": {
      "data": {
        "paper_id": "1712.08969",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-19T07:54:38+00:00",
        "updated_at": "2025-02-19T07:54:41+00:00",
        "version": 2
      }
    },
    "paper:1712.08969": {
      "data": {
        "arxivId": "1712.08969",
        "url": "https://arxiv.org/abs/1712.08969",
        "title": "Mean Field Residual Networks: On the Edge of Chaos",
        "authors": "Greg Yang, Samuel S. Schoenholz",
        "abstract": "We study randomly initialized residual networks using mean field theory and\nthe theory of difference equations. Classical feedforward neural networks, such\nas those with tanh activations, exhibit exponential behavior on the average\nwhen propagating inputs forward or gradients backward. The exponential forward\ndynamics causes rapid collapsing of the input space geometry, while the\nexponential backward dynamics causes drastic vanishing or exploding gradients.\nWe show, in contrast, that by adding skip connections, the network will,\ndepending on the nonlinearity, adopt subexponential forward and backward\ndynamics, and in many cases in fact polynomial. The exponents of these\npolynomials are obtained through analytic methods and proved and verified\nempirically to be correct. In terms of the \"edge of chaos\" hypothesis, these\nsubexponential and polynomial laws allow residual networks to \"hover over the\nboundary between stability and chaos,\" thus preserving the geometry of the\ninput space and the gradient information flow. In our experiments, for each\nactivation function we study here, we initialize residual networks with\ndifferent hyperparameters and train them on MNIST. Remarkably, our\ninitialization time theory can accurately predict test time performance of\nthese networks, by tracking either the expected amount of gradient explosion or\nthe expected squared distance between the images of two input vectors.\nImportantly, we show, theoretically as well as empirically, that common\ninitializations such as the Xavier or the He schemes are not optimal for\nresidual networks, because the optimal initialization variances depend on the\ndepth. Finally, we have made mathematical contributions by deriving several new\nidentities for the kernels of powers of ReLU functions by relating them to the\nzeroth Bessel function of the second kind.",
        "timestamp": "2025-02-19T07:54:33.205Z",
        "rating": "novote",
        "published_date": "2017-12-24T21:51:08Z",
        "arxiv_tags": [
          "cs.NE",
          "cond-mat.dis-nn",
          "cs.LG",
          "math.DS",
          "nlin.CD"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:54:33+00:00",
        "updated_at": "2025-02-19T07:54:36+00:00",
        "version": 2
      }
    },
    "interactions:2203.03466": {
      "data": {
        "paper_id": "2203.03466",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:48:05.358Z",
            "data": {
              "session_id": "session_1739951277242_qbmceb0",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:47:57.242Z",
              "end_time": "2025-02-19T07:48:04.663Z",
              "total_elapsed_seconds": 7
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:48:06+00:00",
        "updated_at": "2025-02-19T07:49:02+00:00",
        "version": 4
      }
    },
    "paper:2203.03466": {
      "data": {
        "arxivId": "2203.03466",
        "url": "https://arxiv.org/abs/2203.03466",
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot\n  Hyperparameter Transfer",
        "authors": "Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",
        "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process,\nprohibitively so for neural networks (NNs) with billions of parameters. We show\nthat, in the recently discovered Maximal Update Parametrization (muP), many\noptimal HPs remain stable even as model size changes. This leads to a new HP\ntuning paradigm we call muTransfer: parametrize the target model in muP, tune\nthe HP indirectly on a smaller model, and zero-shot transfer them to the\nfull-sized model, i.e., without directly tuning the latter at all. We verify\nmuTransfer on Transformer and ResNet. For example, 1) by transferring\npretraining HPs from a model of 13M parameters, we outperform published numbers\nof BERT-large (350M parameters), with a total tuning cost equivalent to\npretraining BERT-large once; 2) by transferring from 40M parameters, we\noutperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7%\nof total pretraining cost. A Pytorch implementation of our technique can be\nfound at github.com/microsoft/mup and installable via `pip install mup`.",
        "timestamp": "2025-02-19T07:47:57.320Z",
        "rating": "novote",
        "published_date": "2022-03-07T15:37:35Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.NE"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:47:57+00:00",
        "updated_at": "2025-02-19T07:48:00+00:00",
        "version": 2
      }
    },
    "interactions:2407.17465": {
      "data": {
        "paper_id": "2407.17465",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-19T07:47:34+00:00",
        "updated_at": "2025-02-19T07:47:37+00:00",
        "version": 2
      }
    },
    "paper:2407.17465": {
      "data": {
        "arxivId": "2407.17465",
        "url": "https://arxiv.org/abs/2407.17465v2",
        "title": "u-$\u03bc$P: The Unit-Scaled Maximal Update Parametrization",
        "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Bj\u00f6rn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr",
        "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a loss that is equal to or lower than comparable\n$\\mu$P models and working out-of-the-box in FP8.",
        "timestamp": "2025-02-19T07:47:27.536Z",
        "rating": "novote",
        "published_date": "2024-07-24T17:58:42Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:47:28+00:00",
        "updated_at": "2025-02-19T07:47:30+00:00",
        "version": 2
      }
    },
    "interactions:2502.05795": {
      "data": {
        "paper_id": "2502.05795",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:45:05.081Z",
            "data": {
              "session_id": "session_1739951096757_gbdiq79",
              "duration_seconds": 7,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:44:56.757Z",
              "end_time": "2025-02-19T07:45:04.167Z",
              "total_elapsed_seconds": 7
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-19T07:58:00.169Z",
            "data": {
              "session_id": "session_1739951870985_qqi2ymf",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-19T07:57:50.985Z",
              "end_time": "2025-02-19T07:57:56.675Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:45:06+00:00",
        "updated_at": "2025-02-19T07:58:52+00:00",
        "version": 5
      }
    },
    "paper:2502.05795": {
      "data": {
        "arxivId": "2502.05795",
        "url": "https://arxiv.org/abs/2502.05795",
        "title": "The Curse of Depth in Large Language Models",
        "authors": "Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu",
        "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language\nModels(LLMs) where nearly half of the layers are less effective than expected.\nWe first confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling, which scales the variance\nof output of the layer normalization inversely by the square root of its depth.\nThis simple modification mitigates the output variance explosion of deeper\nTransformer layers, improving their contribution. Our experimental results,\nspanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling\nsignificantly enhances LLM pre-training performance compared to Pre-LN.\nMoreover, this improvement seamlessly carries over to supervised fine-tuning.\nAll these gains can be attributed to the fact that LayerNorm Scaling enables\ndeeper layers to contribute more effectively during training.",
        "timestamp": "2025-02-19T07:44:29.134Z",
        "rating": "novote",
        "published_date": "2025-02-09T07:03:36Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-19T07:44:29+00:00",
        "updated_at": "2025-02-19T07:44:32+00:00",
        "version": 2
      }
    },
    "interactions:2310.07547": {
      "data": {
        "paper_id": "2310.07547",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-18T17:51:34+00:00",
        "updated_at": "2025-02-18T17:51:38+00:00",
        "version": 2
      }
    },
    "interactions:2012.11197": {
      "data": {
        "paper_id": "2012.11197",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:50:54.277Z",
            "data": {
              "session_id": "session_1739901037432_6692uur",
              "duration_seconds": 16,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:50:37.432Z",
              "end_time": "2025-02-18T17:50:53.605Z",
              "total_elapsed_seconds": 16
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:50:55+00:00",
        "updated_at": "2025-02-18T17:52:01+00:00",
        "version": 3
      }
    },
    "paper:2310.07547": {
      "data": {
        "arxivId": "2310.07547",
        "url": "https://arxiv.org/abs/2310.07547",
        "title": "Entropy estimators for Markovian sequences: A comparative analysis",
        "authors": "Juan De Gregorio, David Sanchez, Raul Toral",
        "abstract": "Entropy estimation is a fundamental problem in information theory that has\napplications in various fields, including physics, biology, and computer\nscience. Estimating the entropy of discrete sequences can be challenging due to\nlimited data and the lack of unbiased estimators. Most existing entropy\nestimators are designed for sequences of independent events and their\nperformance vary depending on the system being studied and the available data\nsize. In this work we compare different entropy estimators and their\nperformance when applied to Markovian sequences. Specifically, we analyze both\nbinary Markovian sequences and Markovian systems in the undersampled regime. We\ncalculate the bias, standard deviation and mean squared error for some of the\nmost widely employed estimators. We discuss the limitations of entropy\nestimation as a function of the transition probabilities of the Markov\nprocesses and the sample size. Overall, this paper provides a comprehensive\ncomparison of entropy estimators and their performance in estimating entropy\nfor systems with memory, which can be useful for researchers and practitioners\nin various fields.",
        "timestamp": "2025-02-18T17:28:47.045Z",
        "rating": "novote",
        "published_date": "2023-10-11T14:50:47Z",
        "arxiv_tags": [
          "cond-mat.stat-mech",
          "nlin.CD",
          "physics.data-an"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:47+00:00",
        "updated_at": "2025-02-18T17:28:50+00:00",
        "version": 2
      }
    },
    "interactions:1406.6959": {
      "data": {
        "paper_id": "1406.6959",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-18T17:28:46+00:00",
        "updated_at": "2025-02-18T17:28:48+00:00",
        "version": 2
      }
    },
    "interactions:0811.3579": {
      "data": {
        "paper_id": "0811.3579",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:28:30.764Z",
            "data": {
              "session_id": "session_1739899703756_rmqd1hr",
              "duration_seconds": 6,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:28:23.756Z",
              "end_time": "2025-02-18T17:28:30.011Z",
              "total_elapsed_seconds": 6
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:31+00:00",
        "updated_at": "2025-02-18T17:29:30+00:00",
        "version": 3
      }
    },
    "paper:1406.6959": {
      "data": {
        "arxivId": "1406.6959",
        "url": "https://arxiv.org/pdf/1406.6959",
        "title": "Maximum Likelihood Estimation of Functionals of Discrete Distributions",
        "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman",
        "abstract": "We consider the problem of estimating functionals of discrete distributions,\nand focus on tight nonasymptotic analysis of the worst case squared error risk\nof widely used estimators. We apply concentration inequalities to analyze the\nrandom fluctuation of these estimators around their expectations, and the\ntheory of approximation using positive linear operators to analyze the\ndeviation of their expectations from the true functional, namely their\n\\emph{bias}.\n  We characterize the worst case squared error risk incurred by the Maximum\nLikelihood Estimator (MLE) in estimating the Shannon entropy $H(P) = \\sum_{i =\n1}^S -p_i \\ln p_i$, and $F_\\alpha(P) = \\sum_{i = 1}^S p_i^\\alpha,\\alpha&gt;0$, up\nto multiplicative constants, for any alphabet size $S\\leq \\infty$ and sample\nsize $n$ for which the risk may vanish. As a corollary, for Shannon entropy\nestimation, we show that it is necessary and sufficient to have $n \\gg S$\nobservations for the MLE to be consistent. In addition, we establish that it is\nnecessary and sufficient to consider $n \\gg S^{1/\\alpha}$ samples for the MLE\nto consistently estimate $F_\\alpha(P), 0&lt;\\alpha&lt;1$. The minimax rate-optimal\nestimators for both problems require $S/\\ln S$ and $S^{1/\\alpha}/\\ln S$\nsamples, which implies that the MLE has a strictly sub-optimal sample\ncomplexity. When $1&lt;\\alpha&lt;3/2$, we show that the worst-case squared error rate\nof convergence for the MLE is $n^{-2(\\alpha-1)}$ for infinite alphabet size,\nwhile the minimax squared error rate is $(n\\ln n)^{-2(\\alpha-1)}$. When\n$\\alpha\\geq 3/2$, the MLE achieves the minimax optimal rate $n^{-1}$ regardless\nof the alphabet size.\n  As an application of the general theory, we analyze the Dirichlet prior\nsmoothing techniques for Shannon entropy estimation. We show that no matter how\nwe tune the parameters in the Dirichlet prior, this technique cannot achieve\nthe minimax rates in entropy estimation.",
        "timestamp": "2025-02-18T17:28:30.667Z",
        "rating": "novote",
        "published_date": "2014-06-26T17:53:58Z",
        "arxiv_tags": [
          "cs.IT",
          "math.IT",
          "math.ST",
          "stat.TH"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:31+00:00",
        "updated_at": "2025-02-18T17:28:34+00:00",
        "version": 2
      }
    },
    "paper:2012.11197": {
      "data": {
        "arxivId": "2012.11197",
        "url": "https://arxiv.org/pdf/2012.11197",
        "title": "Neural Joint Entropy Estimation",
        "authors": "Yuval Shalev, Amichai Painsky, Irad Ben-Gal",
        "abstract": "Estimating the entropy of a discrete random variable is a fundamental problem\nin information theory and related fields. This problem has many applications in\nvarious domains, including machine learning, statistics and data compression.\nOver the years, a variety of estimation schemes have been suggested. However,\ndespite significant progress, most methods still struggle when the sample is\nsmall, compared to the variable's alphabet size. In this work, we introduce a\npractical solution to this problem, which extends the work of McAllester and\nStatos (2020). The proposed scheme uses the generalization abilities of\ncross-entropy estimation in deep neural networks (DNNs) to introduce improved\nentropy estimation accuracy. Furthermore, we introduce a family of estimators\nfor related information-theoretic measures, such as conditional entropy and\nmutual information. We show that these estimators are strongly consistent and\ndemonstrate their performance in a variety of use-cases. First, we consider\nlarge alphabet entropy estimation. Then, we extend the scope to mutual\ninformation estimation. Next, we apply the proposed scheme to conditional\nmutual information estimation, as we focus on independence testing tasks.\nFinally, we study a transfer entropy estimation problem. The proposed\nestimators demonstrate improved performance compared to existing methods in all\ntested setups.",
        "timestamp": "2025-02-18T17:28:22.686Z",
        "rating": "novote",
        "published_date": "2020-12-21T09:23:39Z",
        "arxiv_tags": [
          "cs.IT",
          "cs.LG",
          "math.IT"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:23+00:00",
        "updated_at": "2025-02-18T17:28:26+00:00",
        "version": 2
      }
    },
    "interactions:2204.01469": {
      "data": {
        "paper_id": "2204.01469",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:28:19.786Z",
            "data": {
              "session_id": "session_1739899691126_g2zokvu",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:28:11.126Z",
              "end_time": "2025-02-18T17:28:14.829Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T17:31:07.555Z",
            "data": {
              "session_id": "session_1739899860637_qw3ukfp",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-18T17:31:00.637Z",
              "end_time": "2025-02-18T17:31:04.197Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:16+00:00",
        "updated_at": "2025-02-18T17:31:55+00:00",
        "version": 6
      }
    },
    "paper:0811.3579": {
      "data": {
        "arxivId": "0811.3579",
        "url": "https://arxiv.org/pdf/0811.3579v1",
        "title": "Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks",
        "authors": "Jean Hausser, Korbinian Strimmer",
        "abstract": "We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.",
        "timestamp": "2025-02-18T17:28:15.480Z",
        "rating": "novote",
        "published_date": "2008-11-21T16:47:58Z",
        "arxiv_tags": [
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:16+00:00",
        "updated_at": "2025-02-18T17:28:18+00:00",
        "version": 2
      }
    },
    "paper:2204.01469": {
      "data": {
        "arxivId": "2204.01469",
        "url": "https://arxiv.org/pdf/2204.01469",
        "title": "Estimating the Entropy of Linguistic Distributions",
        "authors": "Aryaman Arora, Clara Meister, Ryan Cotterell",
        "abstract": "Shannon entropy is often a quantity of interest to linguists studying the\ncommunicative capacity of human language. However, entropy must typically be\nestimated from observed data because researchers do not have access to the\nunderlying probability distribution that gives rise to these data. While\nentropy estimation is a well-studied problem in other fields, there is not yet\na comprehensive exploration of the efficacy of entropy estimators for use with\nlinguistic data. In this work, we fill this void, studying the empirical\neffectiveness of different entropy estimators for linguistic distributions. In\na replication of two recent information-theoretic linguistic studies, we find\nevidence that the reported effect size is over-estimated due to over-reliance\non poor entropy estimators. Finally, we end our paper with concrete\nrecommendations for entropy estimation depending on distribution type and data\navailability.",
        "timestamp": "2025-02-18T17:28:11.585Z",
        "rating": "novote",
        "published_date": "2022-04-04T13:36:46Z",
        "arxiv_tags": [
          "cs.CL",
          "94A17 (Primary) 62B10 (Secondary)",
          "I.2.7; E.4"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T17:28:12+00:00",
        "updated_at": "2025-02-18T17:28:15+00:00",
        "version": 2
      }
    },
    "interactions:2502.10216": {
      "data": {
        "paper_id": "2502.10216",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-18T10:42:24.949Z",
            "data": {
              "session_id": "session_1739875323310_knnc6ct",
              "duration_seconds": 21,
              "idle_seconds": 0,
              "start_time": "2025-02-18T10:42:03.311Z",
              "end_time": "2025-02-18T10:42:23.945Z",
              "total_elapsed_seconds": 21
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T10:42:25+00:00",
        "updated_at": "2025-02-18T10:43:54+00:00",
        "version": 3
      }
    },
    "paper:2502.10216": {
      "data": {
        "arxivId": "2502.10216",
        "url": "https://arxiv.org/pdf/2502.10216",
        "title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress",
        "authors": "Dong Wang, Haris \u0160iki\u0107, Lothar Thiele, Olga Saukh",
        "abstract": "We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.",
        "timestamp": "2025-02-18T10:42:03.489Z",
        "rating": "novote",
        "published_date": "2025-02-14T15:10:43Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T10:42:03+00:00",
        "updated_at": "2025-02-18T10:42:07+00:00",
        "version": 2
      }
    },
    "interactions:1603.05027": {
      "data": {
        "paper_id": "1603.05027",
        "interactions": [
          {
            "type": "rating",
            "timestamp": "2025-02-18T09:51:39.463Z",
            "data": {
              "rating": "thumbsup"
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-18T09:51:30+00:00",
        "updated_at": "2025-02-18T09:52:25+00:00",
        "version": 4
      }
    },
    "paper:1603.05027": {
      "data": {
        "arxivId": "1603.05027",
        "url": "https://arxiv.org/abs/1603.05027",
        "title": "Identity Mappings in Deep Residual Networks",
        "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
        "abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers",
        "timestamp": "2025-02-18T09:50:34.906Z",
        "rating": "novote",
        "published_date": "2016-03-16T10:53:56Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-18T09:50:35+00:00",
        "updated_at": "2025-02-18T09:50:38+00:00",
        "version": 2
      }
    },
    "interactions:2502.12977": {
      "data": {
        "paper_id": "2502.12977",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:59:15.372Z",
            "data": {
              "session_id": "session_1740034750623_5r3qpou",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:59:10.623Z",
              "end_time": "2025-02-20T06:59:14.809Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:59:16+00:00",
        "updated_at": "2025-02-20T07:00:00+00:00",
        "version": 3
      }
    },
    "interactions:2405.20324": {
      "data": {
        "paper_id": "2405.20324",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:59:41.619Z",
            "data": {
              "session_id": "session_1740034763803_3oz5zlh",
              "duration_seconds": 18,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:59:23.803Z",
              "end_time": "2025-02-20T06:59:41.602Z",
              "total_elapsed_seconds": 18
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:58:51+00:00",
        "updated_at": "2025-02-20T07:00:24+00:00",
        "version": 4
      }
    },
    "paper:2405.20324": {
      "data": {
        "arxivId": "2405.20324",
        "url": "https://arxiv.org/abs/2405.20324",
        "title": "Don't drop your samples! Coherence-aware training benefits Conditional\n  diffusion",
        "authors": "Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard",
        "abstract": "Conditional diffusion models are powerful generative models that can leverage\nvarious types of conditional information, such as class labels, segmentation\nmasks, or text captions. However, in many real-world scenarios, conditional\ninformation may be noisy or unreliable due to human annotation errors or weak\nalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a\nnovel method that integrates coherence in conditional information into\ndiffusion models, allowing them to learn from noisy annotations without\ndiscarding data. We assume that each data point has an associated coherence\nscore that reflects the quality of the conditional information. We then\ncondition the diffusion model on both the conditional information and the\ncoherence score. In this way, the model learns to ignore or discount the\nconditioning when the coherence is low. We show that CAD is theoretically sound\nand empirically effective on various conditional generation tasks. Moreover, we\nshow that leveraging coherence generates realistic and diverse samples that\nrespect conditional information better than models trained on cleaned datasets\nwhere samples with low coherence have been discarded.",
        "timestamp": "2025-02-20T06:58:44.476Z",
        "rating": "novote",
        "published_date": "2024-05-30T17:57:26Z",
        "arxiv_tags": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:58:44+00:00",
        "updated_at": "2025-02-20T06:58:47+00:00",
        "version": 2
      }
    },
    "paper:2502.12977": {
      "data": {
        "arxivId": "2502.12977",
        "url": "https://arxiv.org/abs/2502.12977",
        "title": "Time-series attribution maps with regularized contrastive learning",
        "authors": "Steffen Schneider, Rodrigo Gonz\u00e1lez Laiz, Anastasiia Filippova, Markus Frey, Mackenzie Weygandt Mathis",
        "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "timestamp": "2025-02-20T06:57:42.987Z",
        "rating": "novote",
        "published_date": "2025-02-17T18:34:25Z",
        "arxiv_tags": [
          "stat.ML",
          "cs.AI",
          "cs.LG",
          "q-bio.NC"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:57:43+00:00",
        "updated_at": "2025-02-20T06:57:46+00:00",
        "version": 2
      }
    },
    "paper:2502.10843": {
      "data": {
        "arxivId": "2502.10843",
        "url": "https://arxiv.org/abs/2502.10843",
        "title": "LEAPS: A discrete neural sampler via locally equivariant networks",
        "authors": "Peter Holderrieth, Michael S. Albergo, Tommi Jaakkola",
        "abstract": "We propose LEAPS, an algorithm to sample from discrete distributions known up\nto normalization by learning a rate matrix of a continuous-time Markov chain\n(CTMC). LEAPS can be seen as a continuous-time formulation of annealed\nimportance sampling and sequential Monte Carlo methods, extended so that the\nvariance of the importance weights is offset by the inclusion of the CTMC. To\nderive these importance weights, we introduce a set of Radon-Nikodym\nderivatives of CTMCs over their path measures. Because the computation of these\nweights is intractable with standard neural network parameterizations of rate\nmatrices, we devise a new compact representation for rate matrices via what we\ncall locally equivariant functions. To parameterize them, we introduce a family\nof locally equivariant multilayer perceptrons, attention layers, and\nconvolutional networks, and provide an approach to make deep networks that\npreserve the local equivariance. This property allows us to propose a scalable\ntraining algorithm for the rate matrix such that the variance of the importance\nweights associated to the CTMC are minimal. We demonstrate the efficacy of\nLEAPS on problems in statistical physics.",
        "timestamp": "2025-02-20T06:50:52.167Z",
        "rating": "novote",
        "published_date": "2025-02-15T16:16:45Z",
        "arxiv_tags": [
          "cs.LG",
          "stat.CO",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:50:52+00:00",
        "updated_at": "2025-02-20T06:50:55+00:00",
        "version": 2
      }
    },
    "interactions:2502.13581": {
      "data": {
        "paper_id": "2502.13581",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-20T06:45:08.635Z",
            "data": {
              "session_id": "session_1740033886608_ghekvlp",
              "duration_seconds": 14,
              "idle_seconds": 0,
              "start_time": "2025-02-20T06:44:46.609Z",
              "end_time": "2025-02-20T06:45:00.510Z",
              "total_elapsed_seconds": 14
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:45:02+00:00",
        "updated_at": "2025-02-20T06:45:47+00:00",
        "version": 4
      }
    },
    "paper:2502.13581": {
      "data": {
        "arxivId": "2502.13581",
        "url": "https://arxiv.org/abs/2502.13581",
        "title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative\n  Recommendation",
        "authors": "Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng",
        "abstract": "Generative recommendation (GR) is an emerging paradigm where user actions are\ntokenized into discrete token patterns and autoregressively generated as\npredictions. However, existing GR models tokenize each action independently,\nassigning the same fixed tokens to identical actions across all sequences\nwithout considering contextual relationships. This lack of context-awareness\ncan lead to suboptimal performance, as the same action may hold different\nmeanings depending on its surrounding context. To address this issue, we\npropose ActionPiece to explicitly incorporate context when tokenizing action\nsequences. In ActionPiece, each action is represented as a set of item\nfeatures, which serve as the initial tokens. Given the action sequence corpora,\nwe construct the vocabulary by merging feature patterns as new tokens, based on\ntheir co-occurrence frequency both within individual sets and across adjacent\nsets. Considering the unordered nature of feature sets, we further introduce\nset permutation regularization, which produces multiple segmentations of action\nsequences with the same semantics. Experiments on public datasets demonstrate\nthat ActionPiece consistently outperforms existing action tokenization methods,\nimproving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.",
        "timestamp": "2025-02-20T06:44:46.483Z",
        "rating": "novote",
        "published_date": "2025-02-19T09:45:29Z",
        "arxiv_tags": [
          "cs.IR",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-20T06:44:46+00:00",
        "updated_at": "2025-02-20T06:44:49+00:00",
        "version": 2
      }
    },
    "interactions:2406.15927": {
      "data": {
        "paper_id": "2406.15927",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:56:49.842Z",
            "data": {
              "session_id": "session_1740099406374_rpg8q3f",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:56:46.374Z",
              "end_time": "2025-02-21T00:56:49.830Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:57:18.015Z",
            "data": {
              "session_id": "session_1740099426678_j1xuro6",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:57:06.678Z",
              "end_time": "2025-02-21T00:57:15.119Z",
              "total_elapsed_seconds": 8
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:58:07.030Z",
            "data": {
              "session_id": "session_1740099480559_sqccjwq",
              "duration_seconds": 5,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:58:00.559Z",
              "end_time": "2025-02-21T00:58:05.402Z",
              "total_elapsed_seconds": 5
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:56:08+00:00",
        "updated_at": "2025-02-21T00:58:55+00:00",
        "version": 10
      }
    },
    "interactions:2405.19648": {
      "data": {
        "paper_id": "2405.19648",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:56:38.895Z",
            "data": {
              "session_id": "session_1740099395790_w5j9hjh",
              "duration_seconds": 3,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:56:35.790Z",
              "end_time": "2025-02-21T00:56:38.880Z",
              "total_elapsed_seconds": 3
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T00:59:11.031Z",
            "data": {
              "session_id": "session_1740099545376_8sig93d",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-21T00:59:05.376Z",
              "end_time": "2025-02-21T00:59:09.188Z",
              "total_elapsed_seconds": 4
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:51+00:00",
        "updated_at": "2025-02-21T00:59:53+00:00",
        "version": 8
      }
    },
    "paper:2406.15927": {
      "data": {
        "arxivId": "2406.15927",
        "url": "https://arxiv.org/abs/2406.15927",
        "title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in\n  LLMs",
        "authors": "Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal",
        "abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for\nuncertainty quantification in Large Language Models (LLMs). Hallucinations,\nwhich are plausible-sounding but factually incorrect and arbitrary model\ngenerations, present a major challenge to the practical adoption of LLMs.\nRecent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can\ndetect hallucinations by estimating uncertainty in the space semantic meaning\nfor a set of model generations. However, the 5-to-10-fold increase in\ncomputation cost associated with SE computation hinders practical adoption. To\naddress this, we propose SEPs, which directly approximate SE from the hidden\nstates of a single generation. SEPs are simple to train and do not require\nsampling multiple model generations at test time, reducing the overhead of\nsemantic uncertainty quantification to almost zero. We show that SEPs retain\nhigh performance for hallucination detection and generalize better to\nout-of-distribution data than previous probing methods that directly predict\nmodel accuracy. Our results across models and tasks suggest that model hidden\nstates capture SE, and our ablation studies give further insights into the\ntoken positions and model layers for which this is the case.",
        "timestamp": "2025-02-21T00:55:50.356Z",
        "rating": "novote",
        "published_date": "2024-06-22T19:46:06Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:50+00:00",
        "updated_at": "2025-02-21T00:55:53+00:00",
        "version": 2
      }
    },
    "paper:2405.19648": {
      "data": {
        "arxivId": "2405.19648",
        "url": "https://arxiv.org/abs/2405.19648",
        "title": "Detecting Hallucinations in Large Language Model Generation: A Token\n  Probability Approach",
        "authors": "Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomas Cerny",
        "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce\ninaccurate outputs, also known as hallucinations, have escalated. Detecting\nthem is vital for ensuring the reliability of applications relying on\nLLM-generated content. Current methods often demand substantial resources and\nrely on extensive LLMs or employ supervised learning with multidimensional\nfeatures or intricate linguistic and semantic analyses difficult to reproduce\nand largely depend on using the same LLM that hallucinated. This paper\nintroduces a supervised learning approach employing two simple classifiers\nutilizing only four numerical features derived from tokens and vocabulary\nprobabilities obtained from other LLM evaluators, which are not necessarily the\nsame. The method yields promising results, surpassing state-of-the-art outcomes\nin multiple tasks across three different benchmarks. Additionally, we provide a\ncomprehensive examination of the strengths and weaknesses of our approach,\nhighlighting the significance of the features utilized and the LLM employed as\nan evaluator. We have released our code publicly at\nhttps://github.com/Baylor-AI/HalluDetect.",
        "timestamp": "2025-02-21T00:55:46.932Z",
        "rating": "novote",
        "published_date": "2024-05-30T03:00:47Z",
        "arxiv_tags": [
          "cs.CL",
          "cs.AI",
          "cs.LG",
          "I.2.7"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T00:55:47+00:00",
        "updated_at": "2025-02-21T00:55:50+00:00",
        "version": 2
      }
    },
    "paper:1503.01156": {
      "data": {
        "arxivId": "1503.01156",
        "url": "https://arxiv.org/abs/1503.01156",
        "title": "A randomized online quantile summary in $O(\\frac{1}{\\varepsilon} \\log\n  \\frac{1}{\\varepsilon})$ words",
        "authors": "David Felber, Rafail Ostrovsky",
        "abstract": "A quantile summary is a data structure that approximates to\n$\\varepsilon$-relative error the order statistics of a much larger underlying\ndataset.\n  In this paper we develop a randomized online quantile summary for the cash\nregister data input model and comparison data domain model that uses\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words of memory. This\nimproves upon the previous best upper bound of $O(\\frac{1}{\\varepsilon}\n\\log^{3/2} \\frac{1}{\\varepsilon})$ by Agarwal et. al. (PODS 2012). Further, by\na lower bound of Hung and Ting (FAW 2010) no deterministic summary for the\ncomparison model can outperform our randomized summary in terms of space\ncomplexity. Lastly, our summary has the nice property that\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words suffice to ensure\nthat the success probability is $1 - e^{-\\text{poly}(1/\\varepsilon)}$.",
        "timestamp": "2025-02-21T15:52:46.023Z",
        "rating": "novote",
        "published_date": "2015-03-03T22:58:55Z",
        "arxiv_tags": [
          "cs.DS"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:52:46+00:00",
        "updated_at": "2025-02-21T15:52:49+00:00",
        "version": 2
      }
    },
    "interactions:1903.08762": {
      "data": {
        "paper_id": "1903.08762",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:51:57.296Z",
            "data": {
              "session_id": "session_1740153108501_3qr13yz",
              "duration_seconds": 8,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:51:48.501Z",
              "end_time": "2025-02-21T15:51:56.577Z",
              "total_elapsed_seconds": 8
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:51:58+00:00",
        "updated_at": "2025-02-21T15:54:07+00:00",
        "version": 3
      }
    },
    "paper:1903.08762": {
      "data": {
        "arxivId": "1903.08762",
        "url": "https://arxiv.org/abs/1903.08762",
        "title": "Large-Scale Online Experimentation with Quantile Metrics",
        "authors": "Min Liu, Xiaohui Sun, Maneesh Varshney, Ya Xu",
        "abstract": "Online experimentation (or A/B testing) has been widely adopted in industry\nas the gold standard for measuring product impacts. Despite the wide adoption,\nfew literatures discuss A/B testing with quantile metrics. Quantile metrics,\nsuch as 90th percentile page load time, are crucial to A/B testing as many key\nperformance metrics including site speed and service latency are defined as\nquantiles. However, with LinkedIn's data size, quantile metric A/B testing is\nextremely challenging because there is no statistically valid and scalable\nvariance estimator for the quantile of dependent samples: the bootstrap\nestimator is statistically valid, but takes days to compute; the standard\nasymptotic variance estimate is scalable but results in order-of-magnitude\nunderestimation. In this paper, we present a statistically valid and scalable\nmethodology for A/B testing with quantiles that is fully generalizable to other\nA/B testing platforms. It achieves over 500 times speed up compared to\nbootstrap and has only $2\\%$ chance to differ from bootstrap estimates. Beyond\nmethodology, we also share the implementation of a data pipeline using this\nmethodology and insights on pipeline optimization.",
        "timestamp": "2025-02-21T15:51:48.657Z",
        "rating": "novote",
        "published_date": "2019-03-20T22:07:58Z",
        "arxiv_tags": [
          "stat.AP"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:51:49+00:00",
        "updated_at": "2025-02-21T15:51:52+00:00",
        "version": 2
      }
    },
    "paper:2411.18933": {
      "data": {
        "arxivId": "2411.18933",
        "url": "https://arxiv.org/abs/2411.18933",
        "title": "Efficient Track Anything",
        "authors": "Yunyang Xiong, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun Liu, Saksham Suri, Balakrishnan Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman Krishnamoorthi, Bilge Soran, Vikas Chandra",
        "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video\nobject segmentation and tracking anything. Key components of SAM 2 that drive\nthe impressive video object segmentation performance include a large multistage\nimage encoder for frame feature extraction and a memory mechanism that stores\nmemory contexts from past frames to help current frame segmentation. The high\ncomputation complexity of multistage image encoder and memory module has\nlimited its applications in real-world tasks, e.g., video object segmentation\non mobile devices. To address this limitation, we propose EfficientTAMs,\nlightweight track anything models that produce high-quality results with low\nlatency and model size. Our idea is based on revisiting the plain,\nnonhierarchical Vision Transformer (ViT) as an image encoder for video object\nsegmentation, and introducing an efficient memory module, which reduces the\ncomplexity for both frame feature extraction and memory computation for current\nframe segmentation. We take vanilla lightweight ViTs and efficient memory\nmodule to build EfficientTAMs, and train the models on SA-1B and SA-V datasets\nfor video object segmentation and track anything tasks. We evaluate on multiple\nvideo segmentation benchmarks including semi-supervised VOS and promptable\nvideo segmentation, and find that our proposed EfficientTAM with vanilla ViT\nperform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and\n~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs\nalso perform favorably over original SAM with ~20x speedup on A100 and ~20x\nparameter reduction. On mobile devices such as iPhone 15 Pro Max, our\nEfficientTAMs can run at ~10 FPS for performing video object segmentation with\nreasonable quality, highlighting the capability of small models for on-device\nvideo object segmentation applications.",
        "timestamp": "2025-02-21T09:27:58.255Z",
        "rating": "novote",
        "published_date": "2024-11-28T05:52:10Z",
        "arxiv_tags": [
          "cs.CV"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T09:27:58+00:00",
        "updated_at": "2025-02-21T09:28:02+00:00",
        "version": 2
      }
    },
    "interactions:1503.01156": {
      "data": {
        "paper_id": "1503.01156",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:53:11.299Z",
            "data": {
              "session_id": "session_1740153165776_shuxs2w",
              "duration_seconds": 25,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:52:45.776Z",
              "end_time": "2025-02-21T15:53:10.538Z",
              "total_elapsed_seconds": 25
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:55:46.373Z",
            "data": {
              "session_id": "session_1740153342023_p0l3ok1",
              "duration_seconds": 4,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:55:42.023Z",
              "end_time": "2025-02-21T15:55:46.147Z",
              "total_elapsed_seconds": 4
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T15:58:16.321Z",
            "data": {
              "session_id": "session_1740153477824_wwsklil",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-02-21T15:57:57.824Z",
              "end_time": "2025-02-21T15:58:13.286Z",
              "total_elapsed_seconds": 15
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T16:02:09.109Z",
            "data": {
              "session_id": "session_1740153713997_xe4az2o",
              "duration_seconds": 15,
              "idle_seconds": 0,
              "start_time": "2025-02-21T16:01:53.997Z",
              "end_time": "2025-02-21T16:02:08.608Z",
              "total_elapsed_seconds": 15
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T15:53:12+00:00",
        "updated_at": "2025-02-21T16:03:46+00:00",
        "version": 8
      }
    },
    "paper:2002.05709": {
      "data": {
        "arxivId": "2002.05709",
        "url": "https://arxiv.org/abs/2002.05709",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations",
        "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
        "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of\nvisual representations. We simplify recently proposed contrastive\nself-supervised learning algorithms without requiring specialized architectures\nor a memory bank. In order to understand what enables the contrastive\nprediction tasks to learn useful representations, we systematically study the\nmajor components of our framework. We show that (1) composition of data\naugmentations plays a critical role in defining effective predictive tasks, (2)\nintroducing a learnable nonlinear transformation between the representation and\nthe contrastive loss substantially improves the quality of the learned\nrepresentations, and (3) contrastive learning benefits from larger batch sizes\nand more training steps compared to supervised learning. By combining these\nfindings, we are able to considerably outperform previous methods for\nself-supervised and semi-supervised learning on ImageNet. A linear classifier\ntrained on self-supervised representations learned by SimCLR achieves 76.5%\ntop-1 accuracy, which is a 7% relative improvement over previous\nstate-of-the-art, matching the performance of a supervised ResNet-50. When\nfine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy,\noutperforming AlexNet with 100X fewer labels.",
        "timestamp": "2025-02-21T23:09:43.920Z",
        "rating": "novote",
        "published_date": "2020-02-13T18:50:45Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:09:44+00:00",
        "updated_at": "2025-02-21T23:09:47+00:00",
        "version": 2
      }
    },
    "interactions:2502.09509": {
      "data": {
        "paper_id": "2502.09509",
        "interactions": []
      },
      "meta": {
        "created_at": "2025-02-21T23:06:38+00:00",
        "updated_at": "2025-02-21T23:06:41+00:00",
        "version": 2
      }
    },
    "paper:2502.09509": {
      "data": {
        "arxivId": "2502.09509",
        "url": "https://arxiv.org/abs/2502.09509",
        "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling",
        "authors": "Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis",
        "abstract": "Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.",
        "timestamp": "2025-02-21T23:06:30.371Z",
        "rating": "novote",
        "published_date": "2025-02-13T17:21:51Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:06:30+00:00",
        "updated_at": "2025-02-21T23:06:33+00:00",
        "version": 2
      }
    },
    "paper:2006.07733": {
      "data": {
        "arxivId": "2006.07733",
        "url": "https://arxiv.org/abs/2006.07733",
        "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
        "authors": "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko",
        "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to\nself-supervised image representation learning. BYOL relies on two neural\nnetworks, referred to as online and target networks, that interact and learn\nfrom each other. From an augmented view of an image, we train the online\nnetwork to predict the target network representation of the same image under a\ndifferent augmented view. At the same time, we update the target network with a\nslow-moving average of the online network. While state-of-the art methods rely\non negative pairs, BYOL achieves a new state of the art without them. BYOL\nreaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear\nevaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We\nshow that BYOL performs on par or better than the current state of the art on\nboth transfer and semi-supervised benchmarks. Our implementation and pretrained\nmodels are given on GitHub.",
        "timestamp": "2025-02-21T23:06:18.432Z",
        "rating": "novote",
        "published_date": "2020-06-13T22:35:21Z",
        "arxiv_tags": [
          "cs.LG",
          "cs.CV",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:06:18+00:00",
        "updated_at": "2025-02-21T23:06:22+00:00",
        "version": 2
      }
    },
    "interactions:2002.05709": {
      "data": {
        "paper_id": "2002.05709",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:10:03.004Z",
            "data": {
              "session_id": "session_1740179383854_el3r9r4",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:09:43.854Z",
              "end_time": "2025-02-21T23:09:56.132Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:09:57+00:00",
        "updated_at": "2025-02-21T23:11:59+00:00",
        "version": 7
      }
    },
    "interactions:2006.07733": {
      "data": {
        "paper_id": "2006.07733",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:07:04.759Z",
            "data": {
              "session_id": "session_1740179211357_x6bra6s",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:06:51.357Z",
              "end_time": "2025-02-21T23:07:03.828Z",
              "total_elapsed_seconds": 12
            }
          },
          {
            "type": "reading_session",
            "timestamp": "2025-02-21T23:09:36.878Z",
            "data": {
              "session_id": "session_1740179365666_ozpjmxp",
              "duration_seconds": 11,
              "idle_seconds": 0,
              "start_time": "2025-02-21T23:09:25.666Z",
              "end_time": "2025-02-21T23:09:36.664Z",
              "total_elapsed_seconds": 11
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-21T23:07:05+00:00",
        "updated_at": "2025-02-21T23:10:57+00:00",
        "version": 6
      }
    },
    "paper:2311.00452": {
      "data": {
        "arxivId": "2311.00452",
        "url": "https://arxiv.org/abs/2311.00452",
        "title": "Hessian Eigenvectors and Principal Component Analysis of Neural Network\n  Weight Matrices",
        "authors": "David Haink",
        "abstract": "This study delves into the intricate dynamics of trained deep neural networks\nand their relationships with network parameters. Trained networks predominantly\ncontinue training in a single direction, known as the drift mode. This drift\nmode can be explained by the quadratic potential model of the loss function,\nsuggesting a slow exponential decay towards the potential minima. We unveil a\ncorrelation between Hessian eigenvectors and network weights. This\nrelationship, hinging on the magnitude of eigenvalues, allows us to discern\nparameter directions within the network. Notably, the significance of these\ndirections relies on two defining attributes: the curvature of their potential\nwells (indicated by the magnitude of Hessian eigenvalues) and their alignment\nwith the weight vectors. Our exploration extends to the decomposition of weight\nmatrices through singular value decomposition. This approach proves practical\nin identifying critical directions within the Hessian, considering both their\nmagnitude and curvature. Furthermore, our examination showcases the\napplicability of principal component analysis in approximating the Hessian,\nwith update parameters emerging as a superior choice over weights for this\npurpose. Remarkably, our findings unveil a similarity between the largest\nHessian eigenvalues of individual layers and the entire network. Notably,\nhigher eigenvalues are concentrated more in deeper layers. Leveraging these\ninsights, we venture into addressing catastrophic forgetting, a challenge of\nneural networks when learning new tasks while retaining knowledge from previous\nones. By applying our discoveries, we formulate an effective strategy to\nmitigate catastrophic forgetting, offering a possible solution that can be\napplied to networks of varying scales, including larger architectures.",
        "timestamp": "2025-02-22T18:13:15.498Z",
        "rating": "novote",
        "published_date": "2023-11-01T11:38:31Z",
        "arxiv_tags": [
          "cs.LG"
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:13:15+00:00",
        "updated_at": "2025-02-22T18:13:18+00:00",
        "version": 2
      }
    },
    "interactions:2107.09133": {
      "data": {
        "paper_id": "2107.09133",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-22T18:10:45.003Z",
            "data": {
              "session_id": "session_1740247832360_xp0m2e8",
              "duration_seconds": 12,
              "idle_seconds": 0,
              "start_time": "2025-02-22T18:10:32.360Z",
              "end_time": "2025-02-22T18:10:44.841Z",
              "total_elapsed_seconds": 12
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:10:39+00:00",
        "updated_at": "2025-02-22T18:12:25+00:00",
        "version": 5
      }
    },
    "paper:2107.09133": {
      "data": {
        "arxivId": "2107.09133",
        "url": "https://arxiv.org/abs/2107.09133",
        "title": "The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations,\n  and Anomalous Diffusion",
        "authors": "Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie, Eshed Margalit, Hidenori Tanaka, Surya Ganguli, Daniel L. K. Yamins",
        "abstract": "In this work we explore the limiting dynamics of deep neural networks trained\nwith stochastic gradient descent (SGD). As observed previously, long after\nperformance has converged, networks continue to move through parameter space by\na process of anomalous diffusion in which distance travelled grows as a power\nlaw in the number of gradient updates with a nontrivial exponent. We reveal an\nintricate interaction between the hyperparameters of optimization, the\nstructure in the gradient noise, and the Hessian matrix at the end of training\nthat explains this anomalous diffusion. To build this understanding, we first\nderive a continuous-time model for SGD with finite learning rates and batch\nsizes as an underdamped Langevin equation. We study this equation in the\nsetting of linear regression, where we can derive exact, analytic expressions\nfor the phase space dynamics of the parameters and their instantaneous\nvelocities from initialization to stationarity. Using the Fokker-Planck\nequation, we show that the key ingredient driving these dynamics is not the\noriginal training loss, but rather the combination of a modified loss, which\nimplicitly regularizes the velocity, and probability currents, which cause\noscillations in phase space. We identify qualitative and quantitative\npredictions of this theory in the dynamics of a ResNet-18 model trained on\nImageNet. Through the lens of statistical physics, we uncover a mechanistic\norigin for the anomalous limiting dynamics of deep neural networks trained with\nSGD.",
        "timestamp": "2025-02-22T18:10:32.678Z",
        "rating": "thumbsup",
        "published_date": "2021-07-19T20:18:57Z",
        "arxiv_tags": [
          "cs.LG",
          "cond-mat.stat-mech",
          "q-bio.NC",
          "stat.ML"
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:10:33+00:00",
        "updated_at": "2025-02-22T18:12:16+00:00",
        "version": 3
      }
    },
    "interactions:2311.00452": {
      "data": {
        "paper_id": "2311.00452",
        "interactions": [
          {
            "type": "reading_session",
            "timestamp": "2025-02-22T18:13:45.409Z",
            "data": {
              "session_id": "session_1740247995961_4rpcbjz",
              "duration_seconds": 29,
              "idle_seconds": 0,
              "start_time": "2025-02-22T18:13:15.961Z",
              "end_time": "2025-02-22T18:13:44.524Z",
              "total_elapsed_seconds": 29
            }
          }
        ]
      },
      "meta": {
        "created_at": "2025-02-22T18:13:46+00:00",
        "updated_at": "2025-02-22T18:15:21+00:00",
        "version": 3
      }
    }
  }
}